galaxy.jobs.runners DEBUG 2025-01-13 01:04:22,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 36 finished
galaxy.model.metadata DEBUG 2025-01-13 01:04:22,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 36
galaxy.util WARNING 2025-01-13 01:04:22,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/2/e/dataset_62e270cd-ba01-414c-b712-fe919b1507f9.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/2/e/dataset_62e270cd-ba01-414c-b712-fe919b1507f9.dat'
galaxy.jobs INFO 2025-01-13 01:04:22,400 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 36 in /galaxy/server/database/jobs_directory/000/36
galaxy.jobs DEBUG 2025-01-13 01:04:22,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 36 executed (187.199 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:04:23,563 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 37
tpv.core.entities DEBUG 2025-01-13 01:04:23,600 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:04:23,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:04:23,606 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:04:23,624 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:04:23,646 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Working directory for job is: /galaxy/server/database/jobs_directory/000/37
galaxy.jobs.runners DEBUG 2025-01-13 01:04:23,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [37] queued (47.296 ms)
galaxy.jobs.handler INFO 2025-01-13 01:04:23,657 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:04:23,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 37
galaxy.jobs DEBUG 2025-01-13 01:04:23,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [37] prepared (102.738 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:04:23,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/37/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/37/registry.xml' '/galaxy/server/database/jobs_directory/000/37/upload_params.json' '37:/galaxy/server/database/objects/8/5/8/dataset_85834219-a8d5-4c92-9514-542556cfcf9b_files:/galaxy/server/database/objects/8/5/8/dataset_85834219-a8d5-4c92-9514-542556cfcf9b.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:04:23,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (37) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/37/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/37/galaxy_37.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:04:31,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 37: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:04:36,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 37 finished
galaxy.model.metadata DEBUG 2025-01-13 01:04:36,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 37
galaxy.jobs INFO 2025-01-13 01:04:36,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 37 in /galaxy/server/database/jobs_directory/000/37
galaxy.jobs DEBUG 2025-01-13 01:04:36,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 37 executed (138.402 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:04:36,952 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 38
tpv.core.entities DEBUG 2025-01-13 01:04:36,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:04:36,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:04:36,996 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:04:37,012 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:04:37,034 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Working directory for job is: /galaxy/server/database/jobs_directory/000/38
galaxy.jobs.runners DEBUG 2025-01-13 01:04:37,042 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [38] queued (45.850 ms)
galaxy.jobs.handler INFO 2025-01-13 01:04:37,046 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:04:37,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 38
galaxy.jobs DEBUG 2025-01-13 01:04:37,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [38] prepared (56.207 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:04:37,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:04:37,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:04:37,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:04:37,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/38/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/38/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/8/5/8/dataset_85834219-a8d5-4c92-9514-542556cfcf9b.dat' infile &&        samtools view -@ $addthreads -h      -o outfile      infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:04:37,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (38) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/38/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/38/galaxy_38.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/38/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/38/working/outfile" "/galaxy/server/database/objects/b/7/e/dataset_b7e34717-891b-4871-8c71-364d3dde4dc2.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:04:37,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:04:37,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:04:37,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:04:42,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 38: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:04:47,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 38 finished
galaxy.model.metadata DEBUG 2025-01-13 01:04:47,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 38
galaxy.util WARNING 2025-01-13 01:04:47,829 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/7/e/dataset_b7e34717-891b-4871-8c71-364d3dde4dc2.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/7/e/dataset_b7e34717-891b-4871-8c71-364d3dde4dc2.dat'
galaxy.jobs INFO 2025-01-13 01:04:47,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 38 in /galaxy/server/database/jobs_directory/000/38
galaxy.jobs DEBUG 2025-01-13 01:04:47,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 38 executed (134.188 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:04:49,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 39
tpv.core.entities DEBUG 2025-01-13 01:04:49,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:04:49,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:04:49,358 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:04:49,375 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:04:49,396 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Working directory for job is: /galaxy/server/database/jobs_directory/000/39
galaxy.jobs.runners DEBUG 2025-01-13 01:04:49,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [39] queued (46.239 ms)
galaxy.jobs.handler INFO 2025-01-13 01:04:49,409 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:04:49,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 39
galaxy.jobs DEBUG 2025-01-13 01:04:49,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [39] prepared (98.774 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:04:49,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/39/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/39/registry.xml' '/galaxy/server/database/jobs_directory/000/39/upload_params.json' '39:/galaxy/server/database/objects/5/6/6/dataset_566ab9b8-81bb-416f-87e2-a370da199ee2_files:/galaxy/server/database/objects/5/6/6/dataset_566ab9b8-81bb-416f-87e2-a370da199ee2.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:04:49,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (39) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/39/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/39/galaxy_39.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:04:58,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 39: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:05:02,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 39 finished
galaxy.model.metadata DEBUG 2025-01-13 01:05:02,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 39
galaxy.jobs INFO 2025-01-13 01:05:03,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 39 in /galaxy/server/database/jobs_directory/000/39
galaxy.jobs DEBUG 2025-01-13 01:05:03,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 39 executed (221.252 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:05:03,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 40
tpv.core.entities DEBUG 2025-01-13 01:05:03,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:05:03,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:05:03,770 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:05:03,788 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:05:03,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Working directory for job is: /galaxy/server/database/jobs_directory/000/40
galaxy.jobs.runners DEBUG 2025-01-13 01:05:03,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [40] queued (44.182 ms)
galaxy.jobs.handler INFO 2025-01-13 01:05:03,818 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:05:03,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 40
galaxy.jobs DEBUG 2025-01-13 01:05:03,910 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [40] prepared (67.498 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:05:03,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:05:03,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:05:03,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:05:03,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/40/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/40/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/5/6/6/dataset_566ab9b8-81bb-416f-87e2-a370da199ee2.dat' infile && ln -s '/galaxy/server/database/objects/_metadata_files/8/9/7/metadata_8972465e-2484-4e41-935f-583089973f0b.dat' infile.bai &&        samtools view -@ $addthreads      -h  -o outfile      infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:05:03,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (40) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/40/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/40/galaxy_40.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/40/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/40/working/outfile" "/galaxy/server/database/objects/8/d/c/dataset_8dcfb0f7-2e27-4ebc-9b71-95f22336dba9.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:05:03,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:05:03,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:05:04,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:05:08,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 40: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:05:12,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 40 finished
galaxy.model.metadata DEBUG 2025-01-13 01:05:13,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 40
galaxy.util WARNING 2025-01-13 01:05:13,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/d/c/dataset_8dcfb0f7-2e27-4ebc-9b71-95f22336dba9.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/d/c/dataset_8dcfb0f7-2e27-4ebc-9b71-95f22336dba9.dat'
galaxy.jobs INFO 2025-01-13 01:05:13,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 40 in /galaxy/server/database/jobs_directory/000/40
galaxy.jobs DEBUG 2025-01-13 01:05:13,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 40 executed (153.885 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:05:14,044 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 41
tpv.core.entities DEBUG 2025-01-13 01:05:14,085 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:05:14,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:05:14,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:05:14,113 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:05:14,137 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Working directory for job is: /galaxy/server/database/jobs_directory/000/41
galaxy.jobs.runners DEBUG 2025-01-13 01:05:14,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [41] queued (52.776 ms)
galaxy.jobs.handler INFO 2025-01-13 01:05:14,149 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:05:14,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 41
galaxy.jobs DEBUG 2025-01-13 01:05:14,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [41] prepared (113.910 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:05:14,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/41/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/41/registry.xml' '/galaxy/server/database/jobs_directory/000/41/upload_params.json' '41:/galaxy/server/database/objects/b/5/0/dataset_b502cd3d-b9ef-4741-88e1-631cd3efac45_files:/galaxy/server/database/objects/b/5/0/dataset_b502cd3d-b9ef-4741-88e1-631cd3efac45.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:05:14,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (41) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/41/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/41/galaxy_41.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:05:23,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 41: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:05:28,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 41 finished
galaxy.model.metadata DEBUG 2025-01-13 01:05:28,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 41
galaxy.jobs INFO 2025-01-13 01:05:28,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 41 in /galaxy/server/database/jobs_directory/000/41
galaxy.jobs DEBUG 2025-01-13 01:05:28,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 41 executed (199.260 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:05:29,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 42
tpv.core.entities DEBUG 2025-01-13 01:05:29,529 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:05:29,529 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:05:29,534 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:05:29,551 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:05:29,572 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Working directory for job is: /galaxy/server/database/jobs_directory/000/42
galaxy.jobs.runners DEBUG 2025-01-13 01:05:29,580 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [42] queued (46.318 ms)
galaxy.jobs.handler INFO 2025-01-13 01:05:29,585 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:05:29,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 42
galaxy.jobs DEBUG 2025-01-13 01:05:29,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [42] prepared (58.206 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:05:29,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:05:29,669 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:05:29,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:05:29,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/42/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/42/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/b/5/0/dataset_b502cd3d-b9ef-4741-88e1-631cd3efac45.dat' infile && ln -s '/galaxy/server/database/objects/_metadata_files/f/7/2/metadata_f7246753-b0f0-49b7-bb89-819f9a9dab9a.dat' infile.bai &&    samtools view -H  -o outfile   infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:05:29,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (42) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/42/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/42/galaxy_42.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/42/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/42/working/outfile" "/galaxy/server/database/objects/5/d/b/dataset_5db29196-9bb7-4cfa-b827-78abf6da55a9.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:05:29,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:05:29,751 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:05:29,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:05:34,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 42: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:05:39,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 42 finished
galaxy.model.metadata DEBUG 2025-01-13 01:05:39,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 42
galaxy.util WARNING 2025-01-13 01:05:39,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/d/b/dataset_5db29196-9bb7-4cfa-b827-78abf6da55a9.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/d/b/dataset_5db29196-9bb7-4cfa-b827-78abf6da55a9.dat'
galaxy.jobs INFO 2025-01-13 01:05:39,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 42 in /galaxy/server/database/jobs_directory/000/42
galaxy.jobs DEBUG 2025-01-13 01:05:39,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 42 executed (144.353 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:05:40,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 43
tpv.core.entities DEBUG 2025-01-13 01:05:40,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:05:40,870 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:05:40,876 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:05:40,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:05:40,912 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Working directory for job is: /galaxy/server/database/jobs_directory/000/43
galaxy.jobs.runners DEBUG 2025-01-13 01:05:40,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [43] queued (42.803 ms)
galaxy.jobs.handler INFO 2025-01-13 01:05:40,923 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:05:40,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 43
galaxy.jobs DEBUG 2025-01-13 01:05:41,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [43] prepared (99.293 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:05:41,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/43/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/43/registry.xml' '/galaxy/server/database/jobs_directory/000/43/upload_params.json' '43:/galaxy/server/database/objects/7/5/1/dataset_751dcdb8-7345-4dbd-8634-a862e9c3e6d3_files:/galaxy/server/database/objects/7/5/1/dataset_751dcdb8-7345-4dbd-8634-a862e9c3e6d3.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:05:41,083 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (43) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/43/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/43/galaxy_43.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:05:50,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 43: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:05:55,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 43 finished
galaxy.model.metadata DEBUG 2025-01-13 01:05:55,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 43
galaxy.jobs INFO 2025-01-13 01:05:55,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 43 in /galaxy/server/database/jobs_directory/000/43
galaxy.jobs DEBUG 2025-01-13 01:05:55,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 43 executed (198.658 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:05:56,257 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 44
tpv.core.entities DEBUG 2025-01-13 01:05:56,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:05:56,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:05:56,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:05:56,328 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:05:56,350 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Working directory for job is: /galaxy/server/database/jobs_directory/000/44
galaxy.jobs.runners DEBUG 2025-01-13 01:05:56,360 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [44] queued (51.119 ms)
galaxy.jobs.handler INFO 2025-01-13 01:05:56,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:05:56,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 44
galaxy.jobs DEBUG 2025-01-13 01:05:56,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [44] prepared (65.743 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:05:56,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:05:56,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:05:56,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:05:56,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/44/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/44/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/7/5/1/dataset_751dcdb8-7345-4dbd-8634-a862e9c3e6d3.dat' infile && ln -s '/galaxy/server/database/objects/_metadata_files/4/a/5/metadata_4a518d29-2ec9-4b0f-acfd-79a39c56b30b.dat' infile.bai &&        samtools view -@ $addthreads        -o outfile      infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:05:56,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (44) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/44/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/44/galaxy_44.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/44/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/44/working/outfile" "/galaxy/server/database/objects/a/2/8/dataset_a28de694-85ec-482c-8047-8b96096fe0a7.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:05:56,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:05:56,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:05:56,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:06:01,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 44: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:06:06,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 44 finished
galaxy.model.metadata DEBUG 2025-01-13 01:06:07,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 44
galaxy.util WARNING 2025-01-13 01:06:07,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/2/8/dataset_a28de694-85ec-482c-8047-8b96096fe0a7.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/2/8/dataset_a28de694-85ec-482c-8047-8b96096fe0a7.dat'
galaxy.jobs INFO 2025-01-13 01:06:07,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 44 in /galaxy/server/database/jobs_directory/000/44
galaxy.jobs DEBUG 2025-01-13 01:06:07,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 44 executed (147.878 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:06:08,635 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 45
tpv.core.entities DEBUG 2025-01-13 01:06:08,673 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:06:08,674 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:06:08,680 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:06:08,698 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:06:08,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Working directory for job is: /galaxy/server/database/jobs_directory/000/45
galaxy.jobs.runners DEBUG 2025-01-13 01:06:08,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [45] queued (44.265 ms)
galaxy.jobs.handler INFO 2025-01-13 01:06:08,729 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:06:08,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 45
galaxy.jobs DEBUG 2025-01-13 01:06:08,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [45] prepared (101.382 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:06:08,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/45/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/45/registry.xml' '/galaxy/server/database/jobs_directory/000/45/upload_params.json' '45:/galaxy/server/database/objects/d/b/4/dataset_db44df63-9f00-4a39-b7fe-32bf7e4ed24a_files:/galaxy/server/database/objects/d/b/4/dataset_db44df63-9f00-4a39-b7fe-32bf7e4ed24a.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:06:08,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (45) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/45/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/45/galaxy_45.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:06:18,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 45: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:06:23,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 45 finished
galaxy.model.metadata DEBUG 2025-01-13 01:06:23,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 45
galaxy.jobs INFO 2025-01-13 01:06:23,167 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 45 in /galaxy/server/database/jobs_directory/000/45
galaxy.jobs DEBUG 2025-01-13 01:06:23,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 45 executed (215.132 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:06:24,073 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 46
tpv.core.entities DEBUG 2025-01-13 01:06:24,116 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:06:24,117 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:06:24,124 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:06:24,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:06:24,165 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Working directory for job is: /galaxy/server/database/jobs_directory/000/46
galaxy.jobs.runners DEBUG 2025-01-13 01:06:24,174 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [46] queued (50.369 ms)
galaxy.jobs.handler INFO 2025-01-13 01:06:24,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:06:24,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 46
galaxy.jobs DEBUG 2025-01-13 01:06:24,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [46] prepared (71.273 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:06:24,278 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:06:24,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:06:24,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:06:24,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/46/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/46/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/d/b/4/dataset_db44df63-9f00-4a39-b7fe-32bf7e4ed24a.dat' infile && ln -s '/galaxy/server/database/objects/_metadata_files/c/5/a/metadata_c5aa0886-0a0f-4021-89de-3ae275b211e8.dat' infile.bai &&        samtools view -@ $addthreads -c     -o outfile     infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:06:24,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (46) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/46/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/46/galaxy_46.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/46/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/46/working/outfile" "/galaxy/server/database/objects/f/8/f/dataset_f8fcc184-783b-4e22-8a25-9b81f105b46c.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:06:24,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:06:24,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:06:24,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:06:30,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 46: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:06:35,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 46 finished
galaxy.model.metadata DEBUG 2025-01-13 01:06:35,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 46
galaxy.util WARNING 2025-01-13 01:06:35,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/8/f/dataset_f8fcc184-783b-4e22-8a25-9b81f105b46c.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/8/f/dataset_f8fcc184-783b-4e22-8a25-9b81f105b46c.dat'
galaxy.jobs INFO 2025-01-13 01:06:35,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 46 in /galaxy/server/database/jobs_directory/000/46
galaxy.jobs DEBUG 2025-01-13 01:06:35,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 46 executed (153.161 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:06:36,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 47, 48
tpv.core.entities DEBUG 2025-01-13 01:06:36,597 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:06:36,598 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:06:36,603 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:06:36,622 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:06:36,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Working directory for job is: /galaxy/server/database/jobs_directory/000/47
galaxy.jobs.runners DEBUG 2025-01-13 01:06:36,653 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [47] queued (49.605 ms)
galaxy.jobs.handler INFO 2025-01-13 01:06:36,658 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:06:36,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 47
tpv.core.entities DEBUG 2025-01-13 01:06:36,680 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:06:36,681 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:06:36,688 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:06:36,709 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:06:36,768 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Working directory for job is: /galaxy/server/database/jobs_directory/000/48
galaxy.jobs.runners DEBUG 2025-01-13 01:06:36,777 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [48] queued (88.112 ms)
galaxy.jobs.handler INFO 2025-01-13 01:06:36,782 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:06:36,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 48
galaxy.jobs DEBUG 2025-01-13 01:06:36,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [47] prepared (180.140 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:06:36,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/47/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/47/registry.xml' '/galaxy/server/database/jobs_directory/000/47/upload_params.json' '47:/galaxy/server/database/objects/1/6/4/dataset_1640c82d-e83a-4a92-9f8e-577505bb55da_files:/galaxy/server/database/objects/1/6/4/dataset_1640c82d-e83a-4a92-9f8e-577505bb55da.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:06:36,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (47) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/47/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/47/galaxy_47.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:06:36,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [48] prepared (127.245 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:06:37,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/48/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/48/registry.xml' '/galaxy/server/database/jobs_directory/000/48/upload_params.json' '48:/galaxy/server/database/objects/a/7/1/dataset_a71cf1f5-f848-46b1-8981-64d2f13c66ab_files:/galaxy/server/database/objects/a/7/1/dataset_a71cf1f5-f848-46b1-8981-64d2f13c66ab.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:06:37,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (48) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/48/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/48/galaxy_48.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:06:45,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 47: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:06:45,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 48: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:06:51,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 47 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:06:51,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 48 finished
galaxy.model.metadata DEBUG 2025-01-13 01:06:51,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 47
galaxy.model.metadata DEBUG 2025-01-13 01:06:51,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 48
galaxy.jobs INFO 2025-01-13 01:06:51,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 47 in /galaxy/server/database/jobs_directory/000/47
galaxy.jobs INFO 2025-01-13 01:06:51,411 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 48 in /galaxy/server/database/jobs_directory/000/48
galaxy.jobs DEBUG 2025-01-13 01:06:51,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 47 executed (168.102 ms)
galaxy.jobs DEBUG 2025-01-13 01:06:51,490 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 48 executed (170.599 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:06:52,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 49
tpv.core.entities DEBUG 2025-01-13 01:06:52,203 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:06:52,203 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:06:52,209 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:06:52,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:06:52,251 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Working directory for job is: /galaxy/server/database/jobs_directory/000/49
galaxy.jobs.runners DEBUG 2025-01-13 01:06:52,261 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [49] queued (51.919 ms)
galaxy.jobs.handler INFO 2025-01-13 01:06:52,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:06:52,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 49
galaxy.jobs DEBUG 2025-01-13 01:06:52,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [49] prepared (76.143 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:06:52,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:06:52,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:06:52,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:06:52,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/49/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/49/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&   ln -s '/galaxy/server/database/objects/a/7/1/dataset_a71cf1f5-f848-46b1-8981-64d2f13c66ab.dat' reference.fa && samtools faidx reference.fa &&   ln -s '/galaxy/server/database/objects/1/6/4/dataset_1640c82d-e83a-4a92-9f8e-577505bb55da.dat' infile &&         samtools view -@ $addthreads -C      -o outfile  --output-fmt-option no_ref   -T 'reference.fa' -t 'reference.fa.fai'   infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:06:52,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (49) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/49/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/49/galaxy_49.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/49/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/49/working/outfile" "/galaxy/server/database/objects/8/e/c/dataset_8ec3e1a8-a898-44dc-99dd-7b52a174c920.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:06:52,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:06:52,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:06:52,462 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:06:58,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 49: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:07:02,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 49 finished
galaxy.model.metadata DEBUG 2025-01-13 01:07:02,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 49
galaxy.util WARNING 2025-01-13 01:07:03,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/e/c/dataset_8ec3e1a8-a898-44dc-99dd-7b52a174c920.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/e/c/dataset_8ec3e1a8-a898-44dc-99dd-7b52a174c920.dat'
galaxy.jobs INFO 2025-01-13 01:07:03,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 49 in /galaxy/server/database/jobs_directory/000/49
galaxy.jobs DEBUG 2025-01-13 01:07:03,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 49 executed (191.817 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:07:04,527 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 50, 51
tpv.core.entities DEBUG 2025-01-13 01:07:04,565 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:07:04,566 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:07:04,573 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:07:04,591 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:07:04,611 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Working directory for job is: /galaxy/server/database/jobs_directory/000/50
galaxy.jobs.runners DEBUG 2025-01-13 01:07:04,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [50] queued (44.897 ms)
galaxy.jobs.handler INFO 2025-01-13 01:07:04,621 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:07:04,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 50
tpv.core.entities DEBUG 2025-01-13 01:07:04,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:07:04,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:07:04,650 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:07:04,669 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:07:04,702 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Working directory for job is: /galaxy/server/database/jobs_directory/000/51
galaxy.jobs.runners DEBUG 2025-01-13 01:07:04,710 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [51] queued (60.191 ms)
galaxy.jobs.handler INFO 2025-01-13 01:07:04,713 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:07:04,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 51
galaxy.jobs DEBUG 2025-01-13 01:07:04,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [50] prepared (113.030 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:07:04,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/50/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/50/registry.xml' '/galaxy/server/database/jobs_directory/000/50/upload_params.json' '50:/galaxy/server/database/objects/1/7/0/dataset_170fe117-fc4b-4b23-af3c-ba0e20331fd4_files:/galaxy/server/database/objects/1/7/0/dataset_170fe117-fc4b-4b23-af3c-ba0e20331fd4.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:07:04,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (50) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/50/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/50/galaxy_50.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:07:04,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [51] prepared (110.773 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:07:04,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/51/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/51/registry.xml' '/galaxy/server/database/jobs_directory/000/51/upload_params.json' '51:/galaxy/server/database/objects/b/3/b/dataset_b3b47a9d-0754-496f-a8ae-effbfc96e674_files:/galaxy/server/database/objects/b/3/b/dataset_b3b47a9d-0754-496f-a8ae-effbfc96e674.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:07:04,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (51) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/51/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/51/galaxy_51.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:07:13,713 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 50: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:07:13,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 51: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:07:19,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 50 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:07:19,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 51 finished
galaxy.model.metadata DEBUG 2025-01-13 01:07:19,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 50
galaxy.model.metadata DEBUG 2025-01-13 01:07:19,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 51
galaxy.jobs INFO 2025-01-13 01:07:19,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 51 in /galaxy/server/database/jobs_directory/000/51
galaxy.jobs INFO 2025-01-13 01:07:19,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 50 in /galaxy/server/database/jobs_directory/000/50
galaxy.jobs DEBUG 2025-01-13 01:07:19,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 51 executed (165.071 ms)
galaxy.jobs DEBUG 2025-01-13 01:07:19,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 50 executed (213.778 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:07:20,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 52
tpv.core.entities DEBUG 2025-01-13 01:07:20,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:07:20,158 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:07:20,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:07:20,182 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:07:20,202 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Working directory for job is: /galaxy/server/database/jobs_directory/000/52
galaxy.jobs.runners DEBUG 2025-01-13 01:07:20,212 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [52] queued (47.910 ms)
galaxy.jobs.handler INFO 2025-01-13 01:07:20,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:07:20,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 52
galaxy.jobs DEBUG 2025-01-13 01:07:20,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [52] prepared (86.533 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:07:20,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:07:20,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:07:20,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:07:20,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/52/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/52/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&   ln -s '/galaxy/server/database/objects/b/3/b/dataset_b3b47a9d-0754-496f-a8ae-effbfc96e674.dat' reference.fa && samtools faidx reference.fa &&   ln -s '/galaxy/server/database/objects/1/7/0/dataset_170fe117-fc4b-4b23-af3c-ba0e20331fd4.dat' infile && ln -s '/galaxy/server/database/objects/_metadata_files/f/8/c/metadata_f8cc4b02-2594-4335-88a5-f0870df61593.dat' infile.bai &&         samtools view -@ $addthreads -C      -o outfile  --output-fmt-option no_ref   -T 'reference.fa' -t 'reference.fa.fai'   infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:07:20,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (52) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/52/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/52/galaxy_52.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/52/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/52/working/outfile" "/galaxy/server/database/objects/b/5/8/dataset_b58b45ec-7708-4606-8364-2b9cf1ea6397.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:07:20,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:07:20,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:07:20,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:07:25,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 52: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:07:30,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 52 finished
galaxy.model.metadata DEBUG 2025-01-13 01:07:30,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 52
galaxy.util WARNING 2025-01-13 01:07:30,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/5/8/dataset_b58b45ec-7708-4606-8364-2b9cf1ea6397.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/5/8/dataset_b58b45ec-7708-4606-8364-2b9cf1ea6397.dat'
galaxy.jobs INFO 2025-01-13 01:07:30,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 52 in /galaxy/server/database/jobs_directory/000/52
galaxy.jobs DEBUG 2025-01-13 01:07:30,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 52 executed (172.389 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:07:32,455 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 54, 53
tpv.core.entities DEBUG 2025-01-13 01:07:32,492 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:07:32,492 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:07:32,498 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:07:32,517 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:07:32,535 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Working directory for job is: /galaxy/server/database/jobs_directory/000/53
galaxy.jobs.runners DEBUG 2025-01-13 01:07:32,542 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [53] queued (43.272 ms)
galaxy.jobs.handler INFO 2025-01-13 01:07:32,546 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:07:32,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 53
tpv.core.entities DEBUG 2025-01-13 01:07:32,566 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:07:32,567 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:07:32,573 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:07:32,590 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:07:32,624 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Working directory for job is: /galaxy/server/database/jobs_directory/000/54
galaxy.jobs.runners DEBUG 2025-01-13 01:07:32,631 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [54] queued (57.821 ms)
galaxy.jobs.handler INFO 2025-01-13 01:07:32,634 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:07:32,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 54
galaxy.jobs DEBUG 2025-01-13 01:07:32,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [53] prepared (112.241 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:07:32,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/53/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/53/registry.xml' '/galaxy/server/database/jobs_directory/000/53/upload_params.json' '53:/galaxy/server/database/objects/c/4/0/dataset_c4026e9b-ce0b-4b7b-a8c7-0b1c66e72c6b_files:/galaxy/server/database/objects/c/4/0/dataset_c4026e9b-ce0b-4b7b-a8c7-0b1c66e72c6b.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:07:32,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (53) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/53/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/53/galaxy_53.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:07:32,778 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [54] prepared (115.386 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:07:32,803 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/54/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/54/registry.xml' '/galaxy/server/database/jobs_directory/000/54/upload_params.json' '54:/galaxy/server/database/objects/2/a/8/dataset_2a8d47ad-a05e-43c6-bb34-114f36ae38d8_files:/galaxy/server/database/objects/2/a/8/dataset_2a8d47ad-a05e-43c6-bb34-114f36ae38d8.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:07:32,817 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (54) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/54/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/54/galaxy_54.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:07:42,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 53: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:07:42,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 54: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:07:47,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 53 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:07:47,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 54 finished
galaxy.model.metadata DEBUG 2025-01-13 01:07:47,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 53
galaxy.model.metadata DEBUG 2025-01-13 01:07:47,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 54
galaxy.jobs INFO 2025-01-13 01:07:47,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 54 in /galaxy/server/database/jobs_directory/000/54
galaxy.jobs INFO 2025-01-13 01:07:47,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 53 in /galaxy/server/database/jobs_directory/000/53
galaxy.jobs DEBUG 2025-01-13 01:07:47,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 54 executed (162.967 ms)
galaxy.jobs DEBUG 2025-01-13 01:07:47,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 53 executed (220.098 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:07:48,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 55
tpv.core.entities DEBUG 2025-01-13 01:07:49,045 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:07:49,046 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:07:49,052 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:07:49,071 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:07:49,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Working directory for job is: /galaxy/server/database/jobs_directory/000/55
galaxy.jobs.runners DEBUG 2025-01-13 01:07:49,098 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [55] queued (45.559 ms)
galaxy.jobs.handler INFO 2025-01-13 01:07:49,101 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:07:49,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 55
galaxy.jobs DEBUG 2025-01-13 01:07:49,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [55] prepared (75.050 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:07:49,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:07:49,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:07:49,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:07:49,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/55/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/55/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&   ln -s '/galaxy/server/database/objects/2/a/8/dataset_2a8d47ad-a05e-43c6-bb34-114f36ae38d8.dat' reference.fa && samtools faidx reference.fa &&   ln -s '/galaxy/server/database/objects/c/4/0/dataset_c4026e9b-ce0b-4b7b-a8c7-0b1c66e72c6b.dat' infile && ln -s '/galaxy/server/database/objects/_metadata_files/5/1/b/metadata_51b3ce8f-d342-4652-8736-8b499c0e36b6.dat' infile.bai &&         samtools view -@ $addthreads -C      -o outfile  --output-fmt-option no_ref   -T 'reference.fa' -t 'reference.fa.fai'   infile  'CHROMOSOME_I']
galaxy.jobs.runners DEBUG 2025-01-13 01:07:49,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (55) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/55/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/55/galaxy_55.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/55/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/55/working/outfile" "/galaxy/server/database/objects/d/e/e/dataset_deeca321-4970-4407-8e11-f58278f9709f.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:07:49,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:07:49,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:07:49,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:07:54,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 55: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:07:59,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 55 finished
galaxy.model.metadata DEBUG 2025-01-13 01:07:59,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 55
galaxy.util WARNING 2025-01-13 01:07:59,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/e/e/dataset_deeca321-4970-4407-8e11-f58278f9709f.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/e/e/dataset_deeca321-4970-4407-8e11-f58278f9709f.dat'
galaxy.jobs INFO 2025-01-13 01:07:59,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 55 in /galaxy/server/database/jobs_directory/000/55
galaxy.jobs DEBUG 2025-01-13 01:07:59,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 55 executed (164.310 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:08:00,459 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 56
tpv.core.entities DEBUG 2025-01-13 01:08:00,500 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:08:00,501 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:08:00,507 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:08:00,526 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:08:00,545 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Working directory for job is: /galaxy/server/database/jobs_directory/000/56
galaxy.jobs.runners DEBUG 2025-01-13 01:08:00,553 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [56] queued (45.589 ms)
galaxy.jobs.handler INFO 2025-01-13 01:08:00,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:08:00,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 56
galaxy.jobs DEBUG 2025-01-13 01:08:00,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [56] prepared (92.265 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:08:00,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/56/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/56/registry.xml' '/galaxy/server/database/jobs_directory/000/56/upload_params.json' '56:/galaxy/server/database/objects/1/7/5/dataset_175bbcf5-d6ea-4003-a3c5-9cee2c612982_files:/galaxy/server/database/objects/1/7/5/dataset_175bbcf5-d6ea-4003-a3c5-9cee2c612982.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:08:00,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (56) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/56/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/56/galaxy_56.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:08:01,559 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 57
tpv.core.entities DEBUG 2025-01-13 01:08:01,596 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:08:01,596 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:08:01,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:08:01,619 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:08:01,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Working directory for job is: /galaxy/server/database/jobs_directory/000/57
galaxy.jobs.runners DEBUG 2025-01-13 01:08:01,646 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [57] queued (44.497 ms)
galaxy.jobs.handler INFO 2025-01-13 01:08:01,650 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:08:01,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 57
galaxy.jobs DEBUG 2025-01-13 01:08:01,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [57] prepared (104.272 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:08:01,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/57/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/57/registry.xml' '/galaxy/server/database/jobs_directory/000/57/upload_params.json' '57:/galaxy/server/database/objects/1/1/0/dataset_1103a8fb-ad78-4453-9b3b-ee09226f3986_files:/galaxy/server/database/objects/1/1/0/dataset_1103a8fb-ad78-4453-9b3b-ee09226f3986.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:08:01,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (57) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/57/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/57/galaxy_57.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:08:10,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 56: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:08:11,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 57: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:08:15,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 56 finished
galaxy.model.metadata DEBUG 2025-01-13 01:08:15,776 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 56
galaxy.jobs INFO 2025-01-13 01:08:15,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 56 in /galaxy/server/database/jobs_directory/000/56
galaxy.jobs DEBUG 2025-01-13 01:08:15,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 56 executed (176.737 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:08:16,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 57 finished
galaxy.model.metadata DEBUG 2025-01-13 01:08:16,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 57
galaxy.jobs INFO 2025-01-13 01:08:16,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 57 in /galaxy/server/database/jobs_directory/000/57
galaxy.jobs DEBUG 2025-01-13 01:08:17,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 57 executed (136.998 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:08:18,070 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 58
tpv.core.entities DEBUG 2025-01-13 01:08:18,119 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:08:18,119 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:08:18,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:08:18,144 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:08:18,167 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Working directory for job is: /galaxy/server/database/jobs_directory/000/58
galaxy.jobs.runners DEBUG 2025-01-13 01:08:18,176 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [58] queued (50.872 ms)
galaxy.jobs.handler INFO 2025-01-13 01:08:18,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:08:18,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 58
galaxy.jobs DEBUG 2025-01-13 01:08:18,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [58] prepared (83.257 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:08:18,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:08:18,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:08:18,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:08:18,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/58/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/58/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&   ln -s '/galaxy/server/database/objects/1/1/0/dataset_1103a8fb-ad78-4453-9b3b-ee09226f3986.dat' reference.fa && samtools faidx reference.fa &&   ln -s '/galaxy/server/database/objects/1/7/5/dataset_175bbcf5-d6ea-4003-a3c5-9cee2c612982.dat' infile && ln -s '/galaxy/server/database/objects/_metadata_files/9/0/f/metadata_90f7ad67-14fc-4d15-bf5a-a65ba1cb4f4e.dat' infile.crai &&         samtools view -@ $addthreads -b      -o outfile    -T 'reference.fa' -t 'reference.fa.fai'   infile  'CHROMOSOME_I'  && samtools sort -@ $addthreads -m $addmemory"M" -T "${TMPDIR:-.}" -O bam -o tmpsam outfile && mv tmpsam outfile]
galaxy.jobs.runners DEBUG 2025-01-13 01:08:18,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (58) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/58/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/58/galaxy_58.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/58/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/58/working/outfile" "/galaxy/server/database/objects/1/f/9/dataset_1f99d3bd-4ae1-4492-b9f7-ac4d852f5816.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:08:18,375 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:08:18,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:08:18,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:08:23,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 58: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:08:28,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 58 finished
galaxy.model.metadata DEBUG 2025-01-13 01:08:28,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 58
galaxy.util WARNING 2025-01-13 01:08:29,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/f/9/dataset_1f99d3bd-4ae1-4492-b9f7-ac4d852f5816.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/f/9/dataset_1f99d3bd-4ae1-4492-b9f7-ac4d852f5816.dat'
galaxy.jobs INFO 2025-01-13 01:08:29,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 58 in /galaxy/server/database/jobs_directory/000/58
galaxy.jobs DEBUG 2025-01-13 01:08:29,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 58 executed (185.440 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:08:30,452 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 60, 59
tpv.core.entities DEBUG 2025-01-13 01:08:30,492 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:08:30,492 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:08:30,498 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:08:30,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:08:30,536 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Working directory for job is: /galaxy/server/database/jobs_directory/000/59
galaxy.jobs.runners DEBUG 2025-01-13 01:08:30,544 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [59] queued (46.328 ms)
galaxy.jobs.handler INFO 2025-01-13 01:08:30,549 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:08:30,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 59
tpv.core.entities DEBUG 2025-01-13 01:08:30,570 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:08:30,570 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:08:30,579 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:08:30,600 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:08:30,634 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Working directory for job is: /galaxy/server/database/jobs_directory/000/60
galaxy.jobs.runners DEBUG 2025-01-13 01:08:30,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [60] queued (62.209 ms)
galaxy.jobs.handler INFO 2025-01-13 01:08:30,649 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:08:30,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 60
galaxy.jobs DEBUG 2025-01-13 01:08:30,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [59] prepared (115.035 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:08:30,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/59/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/59/registry.xml' '/galaxy/server/database/jobs_directory/000/59/upload_params.json' '59:/galaxy/server/database/objects/8/5/d/dataset_85d82008-f702-4cdc-a46e-b518ac400f0c_files:/galaxy/server/database/objects/8/5/d/dataset_85d82008-f702-4cdc-a46e-b518ac400f0c.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:08:30,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (59) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/59/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/59/galaxy_59.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:08:30,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [60] prepared (107.649 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:08:30,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/60/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/60/registry.xml' '/galaxy/server/database/jobs_directory/000/60/upload_params.json' '60:/galaxy/server/database/objects/6/4/e/dataset_64e80e01-db15-46a7-92d6-205e39142147_files:/galaxy/server/database/objects/6/4/e/dataset_64e80e01-db15-46a7-92d6-205e39142147.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:08:30,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (60) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/60/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/60/galaxy_60.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:08:31,651 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 61
tpv.core.entities DEBUG 2025-01-13 01:08:31,691 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:08:31,692 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:08:31,697 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:08:31,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:08:31,738 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Working directory for job is: /galaxy/server/database/jobs_directory/000/61
galaxy.jobs.runners DEBUG 2025-01-13 01:08:31,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [61] queued (47.855 ms)
galaxy.jobs.handler INFO 2025-01-13 01:08:31,749 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:08:31,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 61
galaxy.jobs DEBUG 2025-01-13 01:08:31,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [61] prepared (112.907 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:08:31,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/61/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/61/registry.xml' '/galaxy/server/database/jobs_directory/000/61/upload_params.json' '61:/galaxy/server/database/objects/d/2/7/dataset_d27c6c24-7783-42f5-9363-fbe8d3afde1a_files:/galaxy/server/database/objects/d/2/7/dataset_d27c6c24-7783-42f5-9363-fbe8d3afde1a.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:08:31,935 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (61) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/61/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/61/galaxy_61.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:08:39,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 60: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:08:40,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 61: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:08:40,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 59: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:08:46,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 60 finished
galaxy.model.metadata DEBUG 2025-01-13 01:08:46,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 60
galaxy.jobs INFO 2025-01-13 01:08:46,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 60 in /galaxy/server/database/jobs_directory/000/60
galaxy.jobs DEBUG 2025-01-13 01:08:46,684 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 60 executed (196.151 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:08:47,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 59 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:08:47,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 61 finished
galaxy.model.metadata DEBUG 2025-01-13 01:08:47,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 59
galaxy.model.metadata DEBUG 2025-01-13 01:08:47,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 61
galaxy.jobs INFO 2025-01-13 01:08:47,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 61 in /galaxy/server/database/jobs_directory/000/61
galaxy.jobs INFO 2025-01-13 01:08:47,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 59 in /galaxy/server/database/jobs_directory/000/59
galaxy.jobs DEBUG 2025-01-13 01:08:47,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 61 executed (163.409 ms)
galaxy.jobs DEBUG 2025-01-13 01:08:48,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 59 executed (210.719 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:08:49,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 62
tpv.core.entities DEBUG 2025-01-13 01:08:49,484 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:08:49,485 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:08:49,491 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:08:49,510 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:08:49,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Working directory for job is: /galaxy/server/database/jobs_directory/000/62
galaxy.jobs.runners DEBUG 2025-01-13 01:08:49,543 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [62] queued (51.335 ms)
galaxy.jobs.handler INFO 2025-01-13 01:08:49,546 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:08:49,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 62
galaxy.jobs DEBUG 2025-01-13 01:08:49,659 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [62] prepared (85.912 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:08:49,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:08:49,660 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:08:49,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:08:49,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/62/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/62/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&   ln -s '/galaxy/server/database/objects/d/2/7/dataset_d27c6c24-7783-42f5-9363-fbe8d3afde1a.dat' reference.fa && samtools faidx reference.fa &&   ln -s '/galaxy/server/database/objects/8/5/d/dataset_85d82008-f702-4cdc-a46e-b518ac400f0c.dat' infile && ln -s '/galaxy/server/database/objects/_metadata_files/b/0/a/metadata_b0a2c75e-ecf8-4ecf-b451-53de76508a57.dat' infile.crai &&         samtools view -@ $addthreads -b  -L '/galaxy/server/database/objects/6/4/e/dataset_64e80e01-db15-46a7-92d6-205e39142147.dat'    -o outfile    -T 'reference.fa' -t 'reference.fa.fai'   infile    && samtools sort -@ $addthreads -m $addmemory"M" -T "${TMPDIR:-.}" -O bam -o tmpsam outfile && mv tmpsam outfile]
galaxy.jobs.runners DEBUG 2025-01-13 01:08:49,722 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (62) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/62/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/62/galaxy_62.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/62/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/62/working/outfile" "/galaxy/server/database/objects/9/9/6/dataset_996acab2-aab3-409f-aac6-ce875555145b.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:08:49,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:08:49,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:08:49,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:08:55,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 62: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:09:00,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 62 finished
galaxy.model.metadata DEBUG 2025-01-13 01:09:00,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 62
galaxy.util WARNING 2025-01-13 01:09:00,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/9/6/dataset_996acab2-aab3-409f-aac6-ce875555145b.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/9/6/dataset_996acab2-aab3-409f-aac6-ce875555145b.dat'
galaxy.jobs INFO 2025-01-13 01:09:00,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 62 in /galaxy/server/database/jobs_directory/000/62
galaxy.jobs DEBUG 2025-01-13 01:09:01,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 62 executed (195.085 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:09:01,953 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 63
tpv.core.entities DEBUG 2025-01-13 01:09:01,989 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:09:01,990 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:09:01,995 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:09:02,014 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:09:02,037 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Working directory for job is: /galaxy/server/database/jobs_directory/000/63
galaxy.jobs.runners DEBUG 2025-01-13 01:09:02,045 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [63] queued (49.502 ms)
galaxy.jobs.handler INFO 2025-01-13 01:09:02,050 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:09:02,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 63
galaxy.jobs DEBUG 2025-01-13 01:09:02,178 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [63] prepared (102.345 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:09:02,202 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/63/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/63/registry.xml' '/galaxy/server/database/jobs_directory/000/63/upload_params.json' '63:/galaxy/server/database/objects/3/2/3/dataset_323f5065-038a-494c-baeb-3bad866d0c6e_files:/galaxy/server/database/objects/3/2/3/dataset_323f5065-038a-494c-baeb-3bad866d0c6e.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:09:02,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (63) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/63/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/63/galaxy_63.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:09:11,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 63: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:09:16,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 63 finished
galaxy.model.metadata DEBUG 2025-01-13 01:09:16,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 63
galaxy.jobs INFO 2025-01-13 01:09:17,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 63 in /galaxy/server/database/jobs_directory/000/63
galaxy.jobs DEBUG 2025-01-13 01:09:17,066 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 63 executed (145.819 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:09:17,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 64
tpv.core.entities DEBUG 2025-01-13 01:09:17,420 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:09:17,420 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:09:17,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:09:17,443 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:09:17,463 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Working directory for job is: /galaxy/server/database/jobs_directory/000/64
galaxy.jobs.runners DEBUG 2025-01-13 01:09:17,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [64] queued (45.354 ms)
galaxy.jobs.handler INFO 2025-01-13 01:09:17,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:09:17,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 64
galaxy.jobs DEBUG 2025-01-13 01:09:17,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [64] prepared (71.033 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:09:17,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:09:17,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:09:17,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:09:17,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/64/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/64/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/3/2/3/dataset_323f5065-038a-494c-baeb-3bad866d0c6e.dat' infile &&         sample_fragment=`samtools view -c  infile  | awk '{s=$1} END {frac=s/2; print(frac > 1 ? 9316+1/frac : ".0")}'` &&  samtools view -@ $addthreads -h    -s ${sample_fragment}   -o outfile      infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:09:17,639 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (64) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/64/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/64/galaxy_64.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/64/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/64/working/outfile" "/galaxy/server/database/objects/0/a/0/dataset_0a050c69-8537-42e7-ac12-e5dd32ad4fc7.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:09:17,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:09:17,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:09:17,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:09:23,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 64: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:09:27,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 64 finished
galaxy.model.metadata DEBUG 2025-01-13 01:09:28,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 64
galaxy.util WARNING 2025-01-13 01:09:28,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/a/0/dataset_0a050c69-8537-42e7-ac12-e5dd32ad4fc7.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/a/0/dataset_0a050c69-8537-42e7-ac12-e5dd32ad4fc7.dat'
galaxy.jobs INFO 2025-01-13 01:09:28,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 64 in /galaxy/server/database/jobs_directory/000/64
galaxy.jobs DEBUG 2025-01-13 01:09:28,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 64 executed (141.644 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:09:29,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 65
tpv.core.entities DEBUG 2025-01-13 01:09:29,768 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:09:29,768 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:09:29,774 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:09:29,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:09:29,811 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Working directory for job is: /galaxy/server/database/jobs_directory/000/65
galaxy.jobs.runners DEBUG 2025-01-13 01:09:29,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [65] queued (45.393 ms)
galaxy.jobs.handler INFO 2025-01-13 01:09:29,823 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:09:29,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 65
galaxy.jobs DEBUG 2025-01-13 01:09:29,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [65] prepared (92.350 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:09:29,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/65/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/65/registry.xml' '/galaxy/server/database/jobs_directory/000/65/upload_params.json' '65:/galaxy/server/database/objects/0/f/d/dataset_0fd41ead-3a0b-408d-8b00-aabdc02051f3_files:/galaxy/server/database/objects/0/f/d/dataset_0fd41ead-3a0b-408d-8b00-aabdc02051f3.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:09:29,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (65) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/65/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/65/galaxy_65.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:09:37,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 65: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:09:42,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 65 finished
galaxy.model.metadata DEBUG 2025-01-13 01:09:42,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 65
galaxy.jobs INFO 2025-01-13 01:09:42,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 65 in /galaxy/server/database/jobs_directory/000/65
galaxy.jobs DEBUG 2025-01-13 01:09:42,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 65 executed (129.270 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:09:43,103 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 66
tpv.core.entities DEBUG 2025-01-13 01:09:43,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:09:43,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:09:43,149 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:09:43,166 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:09:43,182 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Working directory for job is: /galaxy/server/database/jobs_directory/000/66
galaxy.jobs.runners DEBUG 2025-01-13 01:09:43,191 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [66] queued (41.746 ms)
galaxy.jobs.handler INFO 2025-01-13 01:09:43,194 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:09:43,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 66
galaxy.jobs DEBUG 2025-01-13 01:09:43,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [66] prepared (63.312 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:09:43,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:09:43,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:09:43,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:09:43,333 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/66/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/66/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/0/f/d/dataset_0fd41ead-3a0b-408d-8b00-aabdc02051f3.dat' infile &&         sample_fragment=`samtools view -c  infile  | awk '{s=$1} END {frac=s/20; print(frac > 1 ? 27976+1/frac : ".0")}'` &&  samtools view -@ $addthreads -h    -s ${sample_fragment}   -o outfile      infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:09:43,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (66) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/66/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/66/galaxy_66.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/66/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/66/working/outfile" "/galaxy/server/database/objects/e/1/e/dataset_e1e4604f-b216-4b92-8aa5-56cd7e471bd9.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:09:43,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:09:43,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:09:43,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:09:48,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 66: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:09:53,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 66 finished
galaxy.model.metadata DEBUG 2025-01-13 01:09:53,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 66
galaxy.util WARNING 2025-01-13 01:09:53,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/1/e/dataset_e1e4604f-b216-4b92-8aa5-56cd7e471bd9.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/1/e/dataset_e1e4604f-b216-4b92-8aa5-56cd7e471bd9.dat'
galaxy.jobs INFO 2025-01-13 01:09:53,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 66 in /galaxy/server/database/jobs_directory/000/66
galaxy.jobs DEBUG 2025-01-13 01:09:53,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 66 executed (141.953 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:09:54,549 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 67
tpv.core.entities DEBUG 2025-01-13 01:09:54,592 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:09:54,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:09:54,597 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:09:54,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:09:54,632 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Working directory for job is: /galaxy/server/database/jobs_directory/000/67
galaxy.jobs.runners DEBUG 2025-01-13 01:09:54,639 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [67] queued (41.828 ms)
galaxy.jobs.handler INFO 2025-01-13 01:09:54,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:09:54,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 67
galaxy.jobs DEBUG 2025-01-13 01:09:54,758 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [67] prepared (90.615 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:09:54,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/67/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/67/registry.xml' '/galaxy/server/database/jobs_directory/000/67/upload_params.json' '67:/galaxy/server/database/objects/c/b/4/dataset_cb4f1c94-e397-4e8c-85e9-5641d5d99350_files:/galaxy/server/database/objects/c/b/4/dataset_cb4f1c94-e397-4e8c-85e9-5641d5d99350.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:09:54,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (67) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/67/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/67/galaxy_67.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:10:02,595 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 67: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:10:07,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 67 finished
galaxy.model.metadata DEBUG 2025-01-13 01:10:07,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 67
galaxy.jobs INFO 2025-01-13 01:10:07,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 67 in /galaxy/server/database/jobs_directory/000/67
galaxy.jobs DEBUG 2025-01-13 01:10:07,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 67 executed (131.597 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:10:08,947 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 68
tpv.core.entities DEBUG 2025-01-13 01:10:08,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:10:08,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:10:08,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:10:09,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:10:09,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Working directory for job is: /galaxy/server/database/jobs_directory/000/68
galaxy.jobs.runners DEBUG 2025-01-13 01:10:09,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [68] queued (47.521 ms)
galaxy.jobs.handler INFO 2025-01-13 01:10:09,043 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:10:09,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 68
galaxy.jobs DEBUG 2025-01-13 01:10:09,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [68] prepared (55.194 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:10:09,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:10:09,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:10:09,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:10:09,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/68/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/68/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/c/b/4/dataset_cb4f1c94-e397-4e8c-85e9-5641d5d99350.dat' infile &&         sample_fragment=`samtools view -c  infile  | awk '{s=$1} END {frac=s/20; print(frac > 1 ? 3244+1/frac : ".0")}'` &&  samtools view -@ $addthreads -h    -s ${sample_fragment}   -o outfile      infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:10:09,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (68) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/68/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/68/galaxy_68.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/68/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/68/working/outfile" "/galaxy/server/database/objects/c/c/5/dataset_cc5cd965-152a-42e3-bd61-bc5f0216ff96.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:10:09,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:10:09,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:10:09,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:10:13,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 68: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:10:18,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 68 finished
galaxy.model.metadata DEBUG 2025-01-13 01:10:18,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 68
galaxy.util WARNING 2025-01-13 01:10:18,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/c/5/dataset_cc5cd965-152a-42e3-bd61-bc5f0216ff96.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/c/5/dataset_cc5cd965-152a-42e3-bd61-bc5f0216ff96.dat'
galaxy.jobs INFO 2025-01-13 01:10:18,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 68 in /galaxy/server/database/jobs_directory/000/68
galaxy.jobs DEBUG 2025-01-13 01:10:18,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 68 executed (133.630 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:10:20,281 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 69
tpv.core.entities DEBUG 2025-01-13 01:10:20,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:10:20,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:10:20,326 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:10:20,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:10:20,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Working directory for job is: /galaxy/server/database/jobs_directory/000/69
galaxy.jobs.runners DEBUG 2025-01-13 01:10:20,374 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [69] queued (48.174 ms)
galaxy.jobs.handler INFO 2025-01-13 01:10:20,378 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:10:20,384 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 69
galaxy.jobs DEBUG 2025-01-13 01:10:20,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [69] prepared (91.563 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:10:20,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/69/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/69/registry.xml' '/galaxy/server/database/jobs_directory/000/69/upload_params.json' '69:/galaxy/server/database/objects/9/7/8/dataset_978e8660-b166-41b0-afbd-fed9c55e1a86_files:/galaxy/server/database/objects/9/7/8/dataset_978e8660-b166-41b0-afbd-fed9c55e1a86.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:10:20,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (69) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/69/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/69/galaxy_69.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:10:29,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 69: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:10:33,875 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 69 finished
galaxy.model.metadata DEBUG 2025-01-13 01:10:33,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 69
galaxy.jobs INFO 2025-01-13 01:10:34,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 69 in /galaxy/server/database/jobs_directory/000/69
galaxy.jobs DEBUG 2025-01-13 01:10:34,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 69 executed (160.430 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:10:34,688 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 70
tpv.core.entities DEBUG 2025-01-13 01:10:34,727 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:10:34,728 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:10:34,734 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:10:34,749 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:10:34,767 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Working directory for job is: /galaxy/server/database/jobs_directory/000/70
galaxy.jobs.runners DEBUG 2025-01-13 01:10:34,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [70] queued (40.905 ms)
galaxy.jobs.handler INFO 2025-01-13 01:10:34,778 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:10:34,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 70
galaxy.jobs DEBUG 2025-01-13 01:10:34,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [70] prepared (59.537 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:10:34,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:10:34,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:10:34,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:10:34,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/70/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/70/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/9/7/8/dataset_978e8660-b166-41b0-afbd-fed9c55e1a86.dat' infile && ln -s '/galaxy/server/database/objects/_metadata_files/0/0/8/metadata_00804c91-0e7c-4391-9229-851d8b2b6c41.dat' infile.bai &&         sample_fragment=`samtools idxstats infile | awk '{s+=$4+$3} END {frac=s/20; print(frac > 1 ? 15998+1/frac : ".0")}'` &&  samtools view -@ $addthreads -b    -s ${sample_fragment}   -o outfile      infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:10:34,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (70) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/70/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/70/galaxy_70.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/70/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/70/working/outfile" "/galaxy/server/database/objects/0/f/3/dataset_0f379ee9-ce8e-4cec-b2d0-0538f2b68b87.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:10:34,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:10:34,933 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:10:34,954 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:10:40,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 70: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:10:45,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 70 finished
galaxy.model.metadata DEBUG 2025-01-13 01:10:45,080 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 70
galaxy.util WARNING 2025-01-13 01:10:45,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/f/3/dataset_0f379ee9-ce8e-4cec-b2d0-0538f2b68b87.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/f/3/dataset_0f379ee9-ce8e-4cec-b2d0-0538f2b68b87.dat'
galaxy.jobs INFO 2025-01-13 01:10:45,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 70 in /galaxy/server/database/jobs_directory/000/70
galaxy.jobs DEBUG 2025-01-13 01:10:45,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 70 executed (176.353 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:10:46,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 71
tpv.core.entities DEBUG 2025-01-13 01:10:46,057 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:10:46,058 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:10:46,064 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:10:46,081 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:10:46,103 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Working directory for job is: /galaxy/server/database/jobs_directory/000/71
galaxy.jobs.runners DEBUG 2025-01-13 01:10:46,110 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [71] queued (45.636 ms)
galaxy.jobs.handler INFO 2025-01-13 01:10:46,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:10:46,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 71
galaxy.jobs DEBUG 2025-01-13 01:10:46,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [71] prepared (102.286 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:10:46,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/71/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/71/registry.xml' '/galaxy/server/database/jobs_directory/000/71/upload_params.json' '71:/galaxy/server/database/objects/3/b/f/dataset_3bf59ac9-d9bc-45ae-9458-807469733bf9_files:/galaxy/server/database/objects/3/b/f/dataset_3bf59ac9-d9bc-45ae-9458-807469733bf9.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:10:46,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (71) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/71/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/71/galaxy_71.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:10:54,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 71: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:10:59,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 71 finished
galaxy.model.metadata DEBUG 2025-01-13 01:10:59,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 71
galaxy.jobs INFO 2025-01-13 01:10:59,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 71 in /galaxy/server/database/jobs_directory/000/71
galaxy.jobs DEBUG 2025-01-13 01:10:59,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 71 executed (131.364 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:11:00,462 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 72
tpv.core.entities DEBUG 2025-01-13 01:11:00,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:11:00,505 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:11:00,511 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:11:00,530 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:11:00,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Working directory for job is: /galaxy/server/database/jobs_directory/000/72
galaxy.jobs.runners DEBUG 2025-01-13 01:11:00,558 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [72] queued (47.170 ms)
galaxy.jobs.handler INFO 2025-01-13 01:11:00,562 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:11:00,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 72
galaxy.jobs DEBUG 2025-01-13 01:11:00,652 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [72] prepared (65.322 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:11:00,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:11:00,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:11:00,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:11:00,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/72/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/72/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/3/b/f/dataset_3bf59ac9-d9bc-45ae-9458-807469733bf9.dat' infile &&         sample_fragment=`samtools view -c  infile  | awk '{s=$1} END {frac=s/2; print(frac > 1 ? 7+1/frac : ".0")}'` &&  samtools view -@ $addthreads -h    -s ${sample_fragment}   -o outfile      infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:11:00,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (72) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/72/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/72/galaxy_72.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/72/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/72/working/outfile" "/galaxy/server/database/objects/d/e/e/dataset_dee6ba7b-770a-44ee-a772-eb5e8add5c76.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:11:00,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:11:00,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:11:00,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:11:06,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 72: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:11:11,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 72 finished
galaxy.model.metadata DEBUG 2025-01-13 01:11:11,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 72
galaxy.util WARNING 2025-01-13 01:11:11,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/e/e/dataset_dee6ba7b-770a-44ee-a772-eb5e8add5c76.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/e/e/dataset_dee6ba7b-770a-44ee-a772-eb5e8add5c76.dat'
galaxy.jobs INFO 2025-01-13 01:11:11,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 72 in /galaxy/server/database/jobs_directory/000/72
galaxy.jobs DEBUG 2025-01-13 01:11:11,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 72 executed (135.649 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:11:12,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 73
tpv.core.entities DEBUG 2025-01-13 01:11:12,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:11:12,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:11:12,898 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:11:12,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:11:12,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Working directory for job is: /galaxy/server/database/jobs_directory/000/73
galaxy.jobs.runners DEBUG 2025-01-13 01:11:12,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [73] queued (43.153 ms)
galaxy.jobs.handler INFO 2025-01-13 01:11:12,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:11:12,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 73
galaxy.jobs DEBUG 2025-01-13 01:11:13,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [73] prepared (99.826 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:11:13,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/73/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/73/registry.xml' '/galaxy/server/database/jobs_directory/000/73/upload_params.json' '73:/galaxy/server/database/objects/2/9/f/dataset_29fb6d42-0dc0-4ca7-8548-ba1e63cd0059_files:/galaxy/server/database/objects/2/9/f/dataset_29fb6d42-0dc0-4ca7-8548-ba1e63cd0059.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:11:13,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (73) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/73/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/73/galaxy_73.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:11:21,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 73: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:11:26,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 73 finished
galaxy.model.metadata DEBUG 2025-01-13 01:11:26,554 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 73
galaxy.jobs INFO 2025-01-13 01:11:26,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 73 in /galaxy/server/database/jobs_directory/000/73
galaxy.jobs DEBUG 2025-01-13 01:11:26,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 73 executed (176.859 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:11:27,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 74
tpv.core.entities DEBUG 2025-01-13 01:11:27,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:11:27,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:11:27,311 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:11:27,330 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:11:27,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Working directory for job is: /galaxy/server/database/jobs_directory/000/74
galaxy.jobs.runners DEBUG 2025-01-13 01:11:27,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [74] queued (46.015 ms)
galaxy.jobs.handler INFO 2025-01-13 01:11:27,362 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:11:27,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 74
galaxy.jobs DEBUG 2025-01-13 01:11:27,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [74] prepared (68.869 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:11:27,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:11:27,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:11:27,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:11:27,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/74/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/74/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/2/9/f/dataset_29fb6d42-0dc0-4ca7-8548-ba1e63cd0059.dat' infile && ln -s '/galaxy/server/database/objects/_metadata_files/3/9/d/metadata_39dd2908-9072-4b46-8f7d-af1850b11018.dat' infile.bai &&         sample_fragment=`samtools idxstats infile | awk '{s+=$4+$3} END {frac=s/2; print(frac > 1 ? 7+1/frac : ".0")}'` &&  samtools view -@ $addthreads -b    -s ${sample_fragment}   -o outfile      infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:11:27,517 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (74) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/74/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/74/galaxy_74.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/74/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/74/working/outfile" "/galaxy/server/database/objects/f/3/3/dataset_f33e694f-4026-4d62-a919-3c9e37e47c97.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:11:27,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:11:27,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:11:27,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:11:32,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 74: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:11:37,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 74 finished
galaxy.model.metadata DEBUG 2025-01-13 01:11:37,773 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 74
galaxy.util WARNING 2025-01-13 01:11:37,821 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/3/3/dataset_f33e694f-4026-4d62-a919-3c9e37e47c97.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/3/3/dataset_f33e694f-4026-4d62-a919-3c9e37e47c97.dat'
galaxy.jobs INFO 2025-01-13 01:11:37,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 74 in /galaxy/server/database/jobs_directory/000/74
galaxy.jobs DEBUG 2025-01-13 01:11:37,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 74 executed (176.618 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:11:39,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 75
tpv.core.entities DEBUG 2025-01-13 01:11:39,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:11:39,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:11:39,674 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:11:39,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:11:39,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Working directory for job is: /galaxy/server/database/jobs_directory/000/75
galaxy.jobs.runners DEBUG 2025-01-13 01:11:39,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [75] queued (50.102 ms)
galaxy.jobs.handler INFO 2025-01-13 01:11:39,728 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:11:39,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 75
galaxy.jobs DEBUG 2025-01-13 01:11:39,851 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [75] prepared (95.495 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:11:39,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/75/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/75/registry.xml' '/galaxy/server/database/jobs_directory/000/75/upload_params.json' '75:/galaxy/server/database/objects/0/d/7/dataset_0d70a3da-1257-4def-98ad-cc8b87bfbb6c_files:/galaxy/server/database/objects/0/d/7/dataset_0d70a3da-1257-4def-98ad-cc8b87bfbb6c.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:11:39,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (75) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/75/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/75/galaxy_75.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:11:49,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 75: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:11:53,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 75 finished
galaxy.model.metadata DEBUG 2025-01-13 01:11:53,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 75
galaxy.jobs INFO 2025-01-13 01:11:54,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 75 in /galaxy/server/database/jobs_directory/000/75
galaxy.jobs DEBUG 2025-01-13 01:11:54,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 75 executed (172.473 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:11:55,257 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 76
tpv.core.entities DEBUG 2025-01-13 01:11:55,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:11:55,299 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:11:55,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:11:55,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:11:55,345 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Working directory for job is: /galaxy/server/database/jobs_directory/000/76
galaxy.jobs.runners DEBUG 2025-01-13 01:11:55,354 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [76] queued (48.004 ms)
galaxy.jobs.handler INFO 2025-01-13 01:11:55,358 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:11:55,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 76
galaxy.jobs DEBUG 2025-01-13 01:11:55,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [76] prepared (68.205 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:11:55,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:11:55,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:11:55,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:11:55,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/76/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/76/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/0/d/7/dataset_0d70a3da-1257-4def-98ad-cc8b87bfbb6c.dat' infile && ln -s '/galaxy/server/database/objects/_metadata_files/2/d/3/metadata_2d3100ed-bed5-4aa3-b8d3-66bdc30c0485.dat' infile.bai &&         sample_fragment=`samtools idxstats infile | awk '{s+=$4+$3} END {frac=s/20; print(frac > 1 ? 7+1/frac : ".0")}'` &&  samtools view -@ $addthreads -b    -s ${sample_fragment}   -o outfile      infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:11:55,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (76) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/76/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/76/galaxy_76.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/76/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/76/working/outfile" "/galaxy/server/database/objects/2/5/8/dataset_258b0029-142d-4d89-bc8c-459225b9ff5a.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:11:55,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:11:55,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:11:55,549 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:12:01,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 76: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:12:06,165 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 76 finished
galaxy.model.metadata DEBUG 2025-01-13 01:12:06,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 76
galaxy.util WARNING 2025-01-13 01:12:06,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/5/8/dataset_258b0029-142d-4d89-bc8c-459225b9ff5a.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/5/8/dataset_258b0029-142d-4d89-bc8c-459225b9ff5a.dat'
galaxy.jobs INFO 2025-01-13 01:12:06,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 76 in /galaxy/server/database/jobs_directory/000/76
galaxy.jobs DEBUG 2025-01-13 01:12:06,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 76 executed (183.962 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:12:07,632 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 77
tpv.core.entities DEBUG 2025-01-13 01:12:07,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:12:07,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:12:07,678 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:12:07,697 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:12:07,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Working directory for job is: /galaxy/server/database/jobs_directory/000/77
galaxy.jobs.runners DEBUG 2025-01-13 01:12:07,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [77] queued (47.396 ms)
galaxy.jobs.handler INFO 2025-01-13 01:12:07,729 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:12:07,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 77
galaxy.jobs DEBUG 2025-01-13 01:12:07,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [77] prepared (100.754 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:12:07,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/77/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/77/registry.xml' '/galaxy/server/database/jobs_directory/000/77/upload_params.json' '77:/galaxy/server/database/objects/4/7/1/dataset_471dbd43-a84c-4071-9565-afc331234bbc_files:/galaxy/server/database/objects/4/7/1/dataset_471dbd43-a84c-4071-9565-afc331234bbc.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:12:07,897 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (77) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/77/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/77/galaxy_77.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:12:16,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 77: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:12:21,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 77 finished
galaxy.model.metadata DEBUG 2025-01-13 01:12:21,511 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 77
galaxy.jobs INFO 2025-01-13 01:12:21,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 77 in /galaxy/server/database/jobs_directory/000/77
galaxy.jobs DEBUG 2025-01-13 01:12:21,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 77 executed (174.292 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:12:22,044 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 78
tpv.core.entities DEBUG 2025-01-13 01:12:22,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:12:22,088 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:12:22,093 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:12:22,110 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:12:22,131 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Working directory for job is: /galaxy/server/database/jobs_directory/000/78
galaxy.jobs.runners DEBUG 2025-01-13 01:12:22,140 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [78] queued (46.632 ms)
galaxy.jobs.handler INFO 2025-01-13 01:12:22,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:12:22,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 78
galaxy.jobs DEBUG 2025-01-13 01:12:22,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [78] prepared (65.192 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:12:22,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:12:22,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:12:22,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:12:22,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/78/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/78/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/4/7/1/dataset_471dbd43-a84c-4071-9565-afc331234bbc.dat' infile && ln -s '/galaxy/server/database/objects/_metadata_files/e/8/c/metadata_e8c978ee-6674-47f9-816b-2e978fcc13eb.dat' infile.bai &&          samtools view -@ $addthreads -b    -s 7.2   -o outfile      infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:12:22,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (78) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/78/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/78/galaxy_78.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/78/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/78/working/outfile" "/galaxy/server/database/objects/7/8/d/dataset_78d60bce-793e-4bed-ab9c-31a57e6ea5a2.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:12:22,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:12:22,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:12:22,522 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:12:27,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 78: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:12:32,679 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 78 finished
galaxy.model.metadata DEBUG 2025-01-13 01:12:32,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 78
galaxy.util WARNING 2025-01-13 01:12:32,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/8/d/dataset_78d60bce-793e-4bed-ab9c-31a57e6ea5a2.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/8/d/dataset_78d60bce-793e-4bed-ab9c-31a57e6ea5a2.dat'
galaxy.jobs INFO 2025-01-13 01:12:32,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 78 in /galaxy/server/database/jobs_directory/000/78
galaxy.jobs DEBUG 2025-01-13 01:12:32,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 78 executed (177.376 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:12:34,389 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 79
tpv.core.entities DEBUG 2025-01-13 01:12:34,422 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:12:34,422 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:12:34,427 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:12:34,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:12:34,460 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Working directory for job is: /galaxy/server/database/jobs_directory/000/79
galaxy.jobs.runners DEBUG 2025-01-13 01:12:34,466 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [79] queued (39.431 ms)
galaxy.jobs.handler INFO 2025-01-13 01:12:34,469 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:12:34,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 79
galaxy.jobs DEBUG 2025-01-13 01:12:34,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [79] prepared (88.577 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:12:34,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/79/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/79/registry.xml' '/galaxy/server/database/jobs_directory/000/79/upload_params.json' '79:/galaxy/server/database/objects/3/a/0/dataset_3a0bc67a-dc6a-4e16-b1ca-ff31b26493c7_files:/galaxy/server/database/objects/3/a/0/dataset_3a0bc67a-dc6a-4e16-b1ca-ff31b26493c7.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:12:34,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (79) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/79/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/79/galaxy_79.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:12:43,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 79: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:12:48,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 79 finished
galaxy.model.metadata DEBUG 2025-01-13 01:12:48,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 79
galaxy.jobs INFO 2025-01-13 01:12:48,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 79 in /galaxy/server/database/jobs_directory/000/79
galaxy.jobs DEBUG 2025-01-13 01:12:48,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 79 executed (176.303 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:12:48,793 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 80
tpv.core.entities DEBUG 2025-01-13 01:12:48,828 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:12:48,829 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:12:48,835 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:12:48,851 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:12:48,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Working directory for job is: /galaxy/server/database/jobs_directory/000/80
galaxy.jobs.runners DEBUG 2025-01-13 01:12:48,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [80] queued (41.937 ms)
galaxy.jobs.handler INFO 2025-01-13 01:12:48,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:12:48,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 80
galaxy.jobs DEBUG 2025-01-13 01:12:48,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [80] prepared (59.289 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:12:48,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:12:48,965 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:12:48,989 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:12:49,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/80/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/80/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&   addmemory=${GALAXY_MEMORY_MB_PER_SLOT:-768} && ((addmemory=addmemory*75/100)) &&     ln -s '/galaxy/server/database/objects/3/a/0/dataset_3a0bc67a-dc6a-4e16-b1ca-ff31b26493c7.dat' infile && ln -s '/galaxy/server/database/objects/_metadata_files/2/5/b/metadata_25b6552a-3256-4db4-a6b6-d2dcf5db5bdb.dat' infile.bai &&          samtools view -@ $addthreads -b    -s 7.8   -U outfile -o /dev/null      infile]
galaxy.jobs.runners DEBUG 2025-01-13 01:12:49,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (80) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/80/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/80/galaxy_80.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/80/working/outfile" ] ; then cp "/galaxy/server/database/jobs_directory/000/80/working/outfile" "/galaxy/server/database/objects/b/a/d/dataset_bad1ef1e-ebc7-426a-9167-3a27eb9a6dbb.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:12:49,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:12:49,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy3: mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33
galaxy.tool_util.deps.containers INFO 2025-01-13 01:12:49,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-8dd8177cf5b9476288c149088f4340b576b866e3:e3f57919aec2b96cba49c23b81609837e1db9c33-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:12:54,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 80: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:12:59,118 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 80 finished
galaxy.model.metadata DEBUG 2025-01-13 01:12:59,179 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 80
galaxy.util WARNING 2025-01-13 01:12:59,223 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/a/d/dataset_bad1ef1e-ebc7-426a-9167-3a27eb9a6dbb.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/a/d/dataset_bad1ef1e-ebc7-426a-9167-3a27eb9a6dbb.dat'
galaxy.jobs INFO 2025-01-13 01:12:59,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 80 in /galaxy/server/database/jobs_directory/000/80
galaxy.jobs DEBUG 2025-01-13 01:12:59,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 80 executed (160.261 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:13:02,169 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 81
tpv.core.entities DEBUG 2025-01-13 01:13:02,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:13:02,207 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:13:02,212 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:13:02,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:13:02,253 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Working directory for job is: /galaxy/server/database/jobs_directory/000/81
galaxy.jobs.runners DEBUG 2025-01-13 01:13:02,260 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [81] queued (47.055 ms)
galaxy.jobs.handler INFO 2025-01-13 01:13:02,264 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:13:02,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 81
galaxy.jobs DEBUG 2025-01-13 01:13:02,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [81] prepared (94.781 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:13:02,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/81/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/81/registry.xml' '/galaxy/server/database/jobs_directory/000/81/upload_params.json' '81:/galaxy/server/database/objects/5/c/1/dataset_5c16068b-c54c-459c-9d9a-fdb708199c03_files:/galaxy/server/database/objects/5/c/1/dataset_5c16068b-c54c-459c-9d9a-fdb708199c03.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:13:02,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (81) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/81/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/81/galaxy_81.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:13:10,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 81: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:13:15,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 81 finished
galaxy.model.metadata DEBUG 2025-01-13 01:13:15,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 81
galaxy.jobs INFO 2025-01-13 01:13:15,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 81 in /galaxy/server/database/jobs_directory/000/81
galaxy.jobs DEBUG 2025-01-13 01:13:15,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 81 executed (176.325 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:13:16,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 82
tpv.core.entities DEBUG 2025-01-13 01:13:16,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:13:16,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:13:16,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:13:16,656 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:13:16,677 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Working directory for job is: /galaxy/server/database/jobs_directory/000/82
galaxy.jobs.runners DEBUG 2025-01-13 01:13:16,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [82] queued (45.690 ms)
galaxy.jobs.handler INFO 2025-01-13 01:13:16,691 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:13:16,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 82
galaxy.jobs DEBUG 2025-01-13 01:13:16,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [82] prepared (87.384 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:13:16,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:13:16,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_split/samtools_split/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-01-13 01:13:17,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:13:17,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/82/tool_script.sh] for tool command [samtools 2>&1 | grep Version > /galaxy/server/database/jobs_directory/000/82/outputs/COMMAND_VERSION 2>&1;
addthreads=${GALAXY_SLOTS:-1} && (( addthreads-- )) &&  samtools split -f 'Read_Group_%!.bam' -u '/galaxy/server/database/objects/1/d/3/dataset_1d3a0293-167c-4a5c-9ec9-4d6ae9246021.dat' -@ $addthreads '/galaxy/server/database/objects/5/c/1/dataset_5c16068b-c54c-459c-9d9a-fdb708199c03.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:13:17,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (82) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/82/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/82/galaxy_82.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:13:17,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:13:17,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/samtools_split/samtools_split/1.9: samtools:1.9
galaxy.tool_util.deps.containers INFO 2025-01-13 01:13:17,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/samtools:1.9--h10a08f8_12,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:13:25,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 82: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:13:30,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 82 finished
galaxy.model.metadata DEBUG 2025-01-13 01:13:30,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 82
galaxy.jobs INFO 2025-01-13 01:13:30,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 82 in /galaxy/server/database/jobs_directory/000/82
galaxy.jobs DEBUG 2025-01-13 01:13:30,923 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 82 executed (411.073 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:13:34,110 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 83, 84
tpv.core.entities DEBUG 2025-01-13 01:13:34,147 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:13:34,147 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:13:34,152 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:13:34,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:13:34,190 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Working directory for job is: /galaxy/server/database/jobs_directory/000/83
galaxy.jobs.runners DEBUG 2025-01-13 01:13:34,198 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [83] queued (45.923 ms)
galaxy.jobs.handler INFO 2025-01-13 01:13:34,203 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:13:34,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 83
tpv.core.entities DEBUG 2025-01-13 01:13:34,224 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:13:34,224 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:13:34,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:13:34,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:13:34,283 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Working directory for job is: /galaxy/server/database/jobs_directory/000/84
galaxy.jobs.runners DEBUG 2025-01-13 01:13:34,292 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [84] queued (60.681 ms)
galaxy.jobs.handler INFO 2025-01-13 01:13:34,297 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:13:34,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 84
galaxy.jobs DEBUG 2025-01-13 01:13:34,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [83] prepared (111.434 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:13:34,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/83/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/83/registry.xml' '/galaxy/server/database/jobs_directory/000/83/upload_params.json' '86:/galaxy/server/database/objects/5/a/7/dataset_5a7ac4a1-aa0f-47cd-ac31-365e5a56e85b_files:/galaxy/server/database/objects/5/a/7/dataset_5a7ac4a1-aa0f-47cd-ac31-365e5a56e85b.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:13:34,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (83) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/83/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/83/galaxy_83.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:13:34,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [84] prepared (109.403 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:13:34,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/84/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/84/registry.xml' '/galaxy/server/database/jobs_directory/000/84/upload_params.json' '87:/galaxy/server/database/objects/d/7/a/dataset_d7a6af0a-040e-49dc-9753-4b892e7512d3_files:/galaxy/server/database/objects/d/7/a/dataset_d7a6af0a-040e-49dc-9753-4b892e7512d3.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:13:34,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (84) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/84/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/84/galaxy_84.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:13:44,013 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 83: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:13:44,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 84: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:13:49,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 83 finished
galaxy.model.metadata DEBUG 2025-01-13 01:13:49,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 86
galaxy.jobs.runners DEBUG 2025-01-13 01:13:49,291 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 84 finished
galaxy.jobs INFO 2025-01-13 01:13:49,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 83 in /galaxy/server/database/jobs_directory/000/83
galaxy.model.metadata DEBUG 2025-01-13 01:13:49,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 87
galaxy.jobs DEBUG 2025-01-13 01:13:49,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 83 executed (162.470 ms)
galaxy.jobs INFO 2025-01-13 01:13:49,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 84 in /galaxy/server/database/jobs_directory/000/84
galaxy.jobs DEBUG 2025-01-13 01:13:49,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 84 executed (153.993 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:13:50,664 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 85
tpv.core.entities DEBUG 2025-01-13 01:13:50,706 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:13:50,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:13:50,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:13:50,729 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:13:50,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Working directory for job is: /galaxy/server/database/jobs_directory/000/85
galaxy.jobs.runners DEBUG 2025-01-13 01:13:50,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [85] queued (45.975 ms)
galaxy.jobs.handler INFO 2025-01-13 01:13:50,762 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:13:50,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 85
galaxy.jobs DEBUG 2025-01-13 01:13:50,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [85] prepared (80.677 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:13:50,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:13:50,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_concat/bcftools_concat/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:13:51,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:13:51,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/85/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/85/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/5/a/7/dataset_5a7ac4a1-aa0f-47cd-ac31-365e5a56e85b.dat' > input0.vcf.gz && bcftools index input0.vcf.gz && echo 'input0.vcf.gz' >> vcfs_list &&  bgzip -c '/galaxy/server/database/objects/d/7/a/dataset_d7a6af0a-040e-49dc-9753-4b892e7512d3.dat' > input1.vcf.gz && bcftools index input1.vcf.gz && echo 'input1.vcf.gz' >> vcfs_list &&         bcftools concat   --allow-overlaps   --min-PQ 30         --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input0.vcf.gz input1.vcf.gz  > '/galaxy/server/database/objects/4/2/f/dataset_42f301e8-64af-46ca-908d-cb1ce948b717.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:13:51,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (85) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/85/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/85/galaxy_85.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:13:51,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:13:51,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_concat/bcftools_concat/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:13:51,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:14:05,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 85: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:14:10,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 85 finished
galaxy.model.metadata DEBUG 2025-01-13 01:14:10,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 88
galaxy.jobs INFO 2025-01-13 01:14:10,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 85 in /galaxy/server/database/jobs_directory/000/85
galaxy.jobs DEBUG 2025-01-13 01:14:10,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 85 executed (146.296 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:14:12,252 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 87, 86
tpv.core.entities DEBUG 2025-01-13 01:14:12,290 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:14:12,291 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:14:12,296 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:14:12,314 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:14:12,333 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Working directory for job is: /galaxy/server/database/jobs_directory/000/86
galaxy.jobs.runners DEBUG 2025-01-13 01:14:12,341 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [86] queued (44.149 ms)
galaxy.jobs.handler INFO 2025-01-13 01:14:12,345 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:14:12,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 86
tpv.core.entities DEBUG 2025-01-13 01:14:12,362 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:14:12,362 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:14:12,369 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:14:12,389 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:14:12,423 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Working directory for job is: /galaxy/server/database/jobs_directory/000/87
galaxy.jobs.runners DEBUG 2025-01-13 01:14:12,431 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [87] queued (61.669 ms)
galaxy.jobs.handler INFO 2025-01-13 01:14:12,434 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:14:12,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 87
galaxy.jobs DEBUG 2025-01-13 01:14:12,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [86] prepared (110.864 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:14:12,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/86/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/86/registry.xml' '/galaxy/server/database/jobs_directory/000/86/upload_params.json' '89:/galaxy/server/database/objects/c/6/d/dataset_c6da7d2f-b2d4-4d36-a7a0-5415fa8813f9_files:/galaxy/server/database/objects/c/6/d/dataset_c6da7d2f-b2d4-4d36-a7a0-5415fa8813f9.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:14:12,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (86) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/86/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/86/galaxy_86.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:14:12,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [87] prepared (110.962 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:14:12,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/87/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/87/registry.xml' '/galaxy/server/database/jobs_directory/000/87/upload_params.json' '90:/galaxy/server/database/objects/f/2/5/dataset_f2544534-80b5-41f0-a949-0f49e0dd6704_files:/galaxy/server/database/objects/f/2/5/dataset_f2544534-80b5-41f0-a949-0f49e0dd6704.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:14:12,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (87) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/87/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/87/galaxy_87.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:14:21,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 86: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:14:21,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 87: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:14:26,777 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 87 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:14:26,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 86 finished
galaxy.model.metadata DEBUG 2025-01-13 01:14:26,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 90
galaxy.model.metadata DEBUG 2025-01-13 01:14:26,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 89
galaxy.jobs INFO 2025-01-13 01:14:26,988 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 87 in /galaxy/server/database/jobs_directory/000/87
galaxy.jobs INFO 2025-01-13 01:14:27,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 86 in /galaxy/server/database/jobs_directory/000/86
galaxy.jobs DEBUG 2025-01-13 01:14:27,061 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 87 executed (202.649 ms)
galaxy.jobs DEBUG 2025-01-13 01:14:27,077 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 86 executed (201.187 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:14:27,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 88
tpv.core.entities DEBUG 2025-01-13 01:14:28,023 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:14:28,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:14:28,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:14:28,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:14:28,069 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Working directory for job is: /galaxy/server/database/jobs_directory/000/88
galaxy.jobs.runners DEBUG 2025-01-13 01:14:28,080 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [88] queued (49.609 ms)
galaxy.jobs.handler INFO 2025-01-13 01:14:28,084 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:14:28,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 88
galaxy.jobs DEBUG 2025-01-13 01:14:28,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [88] prepared (65.429 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:14:28,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:14:28,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_concat/bcftools_concat/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:14:28,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:14:28,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/88/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/88/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/c/6/d/dataset_c6da7d2f-b2d4-4d36-a7a0-5415fa8813f9.dat' > input0.vcf.gz && bcftools index input0.vcf.gz && echo 'input0.vcf.gz' >> vcfs_list &&  bgzip -c '/galaxy/server/database/objects/f/2/5/dataset_f2544534-80b5-41f0-a949-0f49e0dd6704.dat' > input1.vcf.gz && bcftools index input1.vcf.gz && echo 'input1.vcf.gz' >> vcfs_list &&         bcftools concat   --allow-overlaps   --min-PQ 30         --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input0.vcf.gz input1.vcf.gz  > '/galaxy/server/database/objects/6/a/3/dataset_6a381af2-da9f-4125-af22-1e2c36de1006.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:14:28,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (88) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/88/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/88/galaxy_88.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:14:28,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:14:28,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_concat/bcftools_concat/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:14:28,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:14:33,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 88: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:14:38,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 88 finished
galaxy.model.metadata DEBUG 2025-01-13 01:14:38,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 91
galaxy.jobs INFO 2025-01-13 01:14:38,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 88 in /galaxy/server/database/jobs_directory/000/88
galaxy.jobs DEBUG 2025-01-13 01:14:38,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 88 executed (136.368 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:14:40,361 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 90, 89
tpv.core.entities DEBUG 2025-01-13 01:14:40,400 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:14:40,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:14:40,407 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:14:40,424 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:14:40,443 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Working directory for job is: /galaxy/server/database/jobs_directory/000/89
galaxy.jobs.runners DEBUG 2025-01-13 01:14:40,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [89] queued (41.734 ms)
galaxy.jobs.handler INFO 2025-01-13 01:14:40,453 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:14:40,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 89
tpv.core.entities DEBUG 2025-01-13 01:14:40,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:14:40,472 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:14:40,478 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:14:40,499 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:14:40,534 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Working directory for job is: /galaxy/server/database/jobs_directory/000/90
galaxy.jobs.runners DEBUG 2025-01-13 01:14:40,542 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [90] queued (64.206 ms)
galaxy.jobs.handler INFO 2025-01-13 01:14:40,546 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:14:40,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 90
galaxy.jobs DEBUG 2025-01-13 01:14:40,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [89] prepared (111.923 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:14:40,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/89/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/89/registry.xml' '/galaxy/server/database/jobs_directory/000/89/upload_params.json' '92:/galaxy/server/database/objects/b/5/c/dataset_b5c7318f-3f22-4e4a-badc-d5a2630f9595_files:/galaxy/server/database/objects/b/5/c/dataset_b5c7318f-3f22-4e4a-badc-d5a2630f9595.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:14:40,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (89) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/89/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/89/galaxy_89.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:14:40,678 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [90] prepared (104.550 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:14:40,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/90/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/90/registry.xml' '/galaxy/server/database/jobs_directory/000/90/upload_params.json' '93:/galaxy/server/database/objects/b/c/5/dataset_bc568275-00ab-41cf-bf15-793f956b9cd1_files:/galaxy/server/database/objects/b/c/5/dataset_bc568275-00ab-41cf-bf15-793f956b9cd1.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:14:40,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (90) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/90/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/90/galaxy_90.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:14:50,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 89: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:14:50,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 90: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:14:55,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 90 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:14:55,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 89 finished
galaxy.model.metadata DEBUG 2025-01-13 01:14:55,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 93
galaxy.model.metadata DEBUG 2025-01-13 01:14:55,300 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 92
galaxy.jobs INFO 2025-01-13 01:14:55,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 90 in /galaxy/server/database/jobs_directory/000/90
galaxy.jobs INFO 2025-01-13 01:14:55,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 89 in /galaxy/server/database/jobs_directory/000/89
galaxy.jobs DEBUG 2025-01-13 01:14:55,417 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 90 executed (157.389 ms)
galaxy.jobs DEBUG 2025-01-13 01:14:55,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 89 executed (160.012 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:14:56,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 91
tpv.core.entities DEBUG 2025-01-13 01:14:56,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:14:56,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:14:56,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:14:56,969 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:14:56,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Working directory for job is: /galaxy/server/database/jobs_directory/000/91
galaxy.jobs.runners DEBUG 2025-01-13 01:14:57,001 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [91] queued (49.774 ms)
galaxy.jobs.handler INFO 2025-01-13 01:14:57,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:14:57,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 91
galaxy.jobs DEBUG 2025-01-13 01:14:57,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [91] prepared (68.067 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:14:57,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:14:57,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_concat/bcftools_concat/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:14:57,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:14:57,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/91/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/91/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/b/5/c/dataset_b5c7318f-3f22-4e4a-badc-d5a2630f9595.dat' > input0.vcf.gz && bcftools index input0.vcf.gz && echo 'input0.vcf.gz' >> vcfs_list &&  bgzip -c '/galaxy/server/database/objects/b/c/5/dataset_bc568275-00ab-41cf-bf15-793f956b9cd1.dat' > input1.vcf.gz && bcftools index input1.vcf.gz && echo 'input1.vcf.gz' >> vcfs_list &&         bcftools concat   --allow-overlaps   --min-PQ 30         --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input0.vcf.gz input1.vcf.gz  > '/galaxy/server/database/objects/4/b/d/dataset_4bd14a25-cd45-4d82-b706-7df9b1c84022.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:14:57,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (91) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/91/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/91/galaxy_91.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:14:57,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:14:57,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_concat/bcftools_concat/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:14:57,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:15:02,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 91: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:15:07,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 91 finished
galaxy.model.metadata DEBUG 2025-01-13 01:15:07,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 94
galaxy.jobs INFO 2025-01-13 01:15:07,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 91 in /galaxy/server/database/jobs_directory/000/91
galaxy.jobs DEBUG 2025-01-13 01:15:07,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 91 executed (122.527 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:15:09,263 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 92, 93
tpv.core.entities DEBUG 2025-01-13 01:15:09,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:15:09,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:15:09,310 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:15:09,327 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:15:09,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Working directory for job is: /galaxy/server/database/jobs_directory/000/92
galaxy.jobs.runners DEBUG 2025-01-13 01:15:09,358 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [92] queued (47.917 ms)
galaxy.jobs.handler INFO 2025-01-13 01:15:09,361 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:15:09,367 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 92
tpv.core.entities DEBUG 2025-01-13 01:15:09,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:15:09,381 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:15:09,388 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:15:09,406 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:15:09,440 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Working directory for job is: /galaxy/server/database/jobs_directory/000/93
galaxy.jobs.runners DEBUG 2025-01-13 01:15:09,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [93] queued (59.007 ms)
galaxy.jobs.handler INFO 2025-01-13 01:15:09,451 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:15:09,460 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 93
galaxy.jobs DEBUG 2025-01-13 01:15:09,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [92] prepared (115.702 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:15:09,541 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/92/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/92/registry.xml' '/galaxy/server/database/jobs_directory/000/92/upload_params.json' '95:/galaxy/server/database/objects/2/c/8/dataset_2c8dccb4-8826-4539-9c72-5f2f516bc0e2_files:/galaxy/server/database/objects/2/c/8/dataset_2c8dccb4-8826-4539-9c72-5f2f516bc0e2.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:15:09,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (92) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/92/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/92/galaxy_92.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:15:09,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [93] prepared (106.085 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:15:09,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/93/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/93/registry.xml' '/galaxy/server/database/jobs_directory/000/93/upload_params.json' '96:/galaxy/server/database/objects/b/6/5/dataset_b657726a-d328-488a-9f2e-ce7fb655f9a7_files:/galaxy/server/database/objects/b/6/5/dataset_b657726a-d328-488a-9f2e-ce7fb655f9a7.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:15:09,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (93) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/93/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/93/galaxy_93.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:15:19,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 93: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:15:19,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 92: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:15:24,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 92 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:15:24,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 93 finished
galaxy.model.metadata DEBUG 2025-01-13 01:15:24,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 95
galaxy.model.metadata DEBUG 2025-01-13 01:15:24,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 96
galaxy.jobs INFO 2025-01-13 01:15:24,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 92 in /galaxy/server/database/jobs_directory/000/92
galaxy.jobs INFO 2025-01-13 01:15:24,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 93 in /galaxy/server/database/jobs_directory/000/93
galaxy.jobs DEBUG 2025-01-13 01:15:24,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 92 executed (157.025 ms)
galaxy.jobs DEBUG 2025-01-13 01:15:24,256 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 93 executed (161.167 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:15:24,798 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 94
tpv.core.entities DEBUG 2025-01-13 01:15:24,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:15:24,838 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:15:24,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:15:24,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:15:24,874 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Working directory for job is: /galaxy/server/database/jobs_directory/000/94
galaxy.jobs.runners DEBUG 2025-01-13 01:15:24,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [94] queued (38.501 ms)
galaxy.jobs.handler INFO 2025-01-13 01:15:24,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:15:24,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 94
galaxy.jobs DEBUG 2025-01-13 01:15:24,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [94] prepared (58.904 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:15:24,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:15:24,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_concat/bcftools_concat/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:15:24,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:15:25,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/94/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/94/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/2/c/8/dataset_2c8dccb4-8826-4539-9c72-5f2f516bc0e2.dat' > input0.vcf.gz && bcftools index input0.vcf.gz && echo 'input0.vcf.gz' >> vcfs_list &&  bgzip -c '/galaxy/server/database/objects/b/6/5/dataset_b657726a-d328-488a-9f2e-ce7fb655f9a7.dat' > input1.vcf.gz && bcftools index input1.vcf.gz && echo 'input1.vcf.gz' >> vcfs_list &&         bcftools concat   --allow-overlaps --rm-dups none   --min-PQ 30         --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input0.vcf.gz input1.vcf.gz  > '/galaxy/server/database/objects/a/0/f/dataset_a0f92c5d-adb9-4dca-b4eb-3f7dcb0f1e7e.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:15:25,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (94) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/94/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/94/galaxy_94.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:15:25,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:15:25,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_concat/bcftools_concat/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:15:25,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:15:31,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 94: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:15:35,976 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 94 finished
galaxy.model.metadata DEBUG 2025-01-13 01:15:36,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 97
galaxy.jobs INFO 2025-01-13 01:15:36,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 94 in /galaxy/server/database/jobs_directory/000/94
galaxy.jobs DEBUG 2025-01-13 01:15:36,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 94 executed (152.627 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:15:37,154 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 95
tpv.core.entities DEBUG 2025-01-13 01:15:37,188 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:15:37,188 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:15:37,193 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:15:37,209 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:15:37,230 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Working directory for job is: /galaxy/server/database/jobs_directory/000/95
galaxy.jobs.runners DEBUG 2025-01-13 01:15:37,238 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [95] queued (44.864 ms)
galaxy.jobs.handler INFO 2025-01-13 01:15:37,241 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:15:37,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 95
galaxy.jobs DEBUG 2025-01-13 01:15:37,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [95] prepared (89.867 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:15:37,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/95/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/95/registry.xml' '/galaxy/server/database/jobs_directory/000/95/upload_params.json' '98:/galaxy/server/database/objects/3/5/3/dataset_3530bc51-ec10-4b61-aa00-e8efe239f0b9_files:/galaxy/server/database/objects/3/5/3/dataset_3530bc51-ec10-4b61-aa00-e8efe239f0b9.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:15:37,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (95) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/95/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/95/galaxy_95.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:15:38,244 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 96
tpv.core.entities DEBUG 2025-01-13 01:15:38,280 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:15:38,281 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:15:38,285 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:15:38,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:15:38,322 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Working directory for job is: /galaxy/server/database/jobs_directory/000/96
galaxy.jobs.runners DEBUG 2025-01-13 01:15:38,329 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [96] queued (43.905 ms)
galaxy.jobs.handler INFO 2025-01-13 01:15:38,333 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:15:38,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 96
galaxy.jobs DEBUG 2025-01-13 01:15:38,451 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [96] prepared (93.586 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:15:38,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/96/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/96/registry.xml' '/galaxy/server/database/jobs_directory/000/96/upload_params.json' '99:/galaxy/server/database/objects/e/5/5/dataset_e558a8f5-869f-47f2-a757-ec6eb5da7c26_files:/galaxy/server/database/objects/e/5/5/dataset_e558a8f5-869f-47f2-a757-ec6eb5da7c26.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:15:38,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (96) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/96/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/96/galaxy_96.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:15:45,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 95: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:15:45,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 96: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:15:51,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 96 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:15:51,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 95 finished
galaxy.model.metadata DEBUG 2025-01-13 01:15:51,129 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 99
galaxy.model.metadata DEBUG 2025-01-13 01:15:51,174 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 98
galaxy.jobs INFO 2025-01-13 01:15:51,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 96 in /galaxy/server/database/jobs_directory/000/96
galaxy.jobs INFO 2025-01-13 01:15:51,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 95 in /galaxy/server/database/jobs_directory/000/95
galaxy.jobs DEBUG 2025-01-13 01:15:51,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 96 executed (170.248 ms)
galaxy.jobs DEBUG 2025-01-13 01:15:51,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 95 executed (164.279 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:15:51,623 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 97
tpv.core.entities DEBUG 2025-01-13 01:15:51,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:15:51,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:15:51,673 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:15:51,690 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:15:51,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Working directory for job is: /galaxy/server/database/jobs_directory/000/97
galaxy.jobs.runners DEBUG 2025-01-13 01:15:51,722 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [97] queued (48.796 ms)
galaxy.jobs.handler INFO 2025-01-13 01:15:51,726 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:15:51,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 97
galaxy.jobs DEBUG 2025-01-13 01:15:51,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [97] prepared (66.440 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:15:51,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:15:51,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_concat/bcftools_concat/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:15:51,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:15:51,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/97/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/97/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/3/5/3/dataset_3530bc51-ec10-4b61-aa00-e8efe239f0b9.dat' > input0.vcf.gz && bcftools index input0.vcf.gz && echo 'input0.vcf.gz' >> vcfs_list &&  bgzip -c '/galaxy/server/database/objects/e/5/5/dataset_e558a8f5-869f-47f2-a757-ec6eb5da7c26.dat' > input1.vcf.gz && bcftools index input1.vcf.gz && echo 'input1.vcf.gz' >> vcfs_list &&         bcftools concat   --allow-overlaps   --min-PQ 30    --regions-overlap 1      --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input0.vcf.gz input1.vcf.gz  > '/galaxy/server/database/objects/3/e/0/dataset_3e00ac6e-b28b-4259-b02b-0b372c9cf667.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:15:51,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (97) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/97/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/97/galaxy_97.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:15:51,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:15:51,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_concat/bcftools_concat/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:15:51,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:15:58,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 97: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:16:02,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 97 finished
galaxy.model.metadata DEBUG 2025-01-13 01:16:02,871 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 100
galaxy.jobs INFO 2025-01-13 01:16:02,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 97 in /galaxy/server/database/jobs_directory/000/97
galaxy.jobs DEBUG 2025-01-13 01:16:02,990 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 97 executed (156.582 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:16:04,001 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 98
tpv.core.entities DEBUG 2025-01-13 01:16:04,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:16:04,039 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:16:04,045 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:16:04,063 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:16:04,084 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Working directory for job is: /galaxy/server/database/jobs_directory/000/98
galaxy.jobs.runners DEBUG 2025-01-13 01:16:04,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [98] queued (45.172 ms)
galaxy.jobs.handler INFO 2025-01-13 01:16:04,094 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:16:04,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 98
galaxy.jobs DEBUG 2025-01-13 01:16:04,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [98] prepared (103.346 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:16:04,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/98/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/98/registry.xml' '/galaxy/server/database/jobs_directory/000/98/upload_params.json' '101:/galaxy/server/database/objects/3/3/7/dataset_33770074-df42-4c1b-806a-e3f18259ab45_files:/galaxy/server/database/objects/3/3/7/dataset_33770074-df42-4c1b-806a-e3f18259ab45.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:16:04,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (98) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/98/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/98/galaxy_98.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:16:05,097 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 99
tpv.core.entities DEBUG 2025-01-13 01:16:05,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:16:05,134 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:16:05,139 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:16:05,155 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:16:05,177 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Working directory for job is: /galaxy/server/database/jobs_directory/000/99
galaxy.jobs.runners DEBUG 2025-01-13 01:16:05,184 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [99] queued (45.159 ms)
galaxy.jobs.handler INFO 2025-01-13 01:16:05,188 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:16:05,192 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 99
galaxy.jobs DEBUG 2025-01-13 01:16:05,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [99] prepared (98.154 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:16:05,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/99/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/99/registry.xml' '/galaxy/server/database/jobs_directory/000/99/upload_params.json' '102:/galaxy/server/database/objects/4/5/7/dataset_457da5cd-2ba2-440b-9602-8ab5bea2b5a1_files:/galaxy/server/database/objects/4/5/7/dataset_457da5cd-2ba2-440b-9602-8ab5bea2b5a1.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:16:05,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (99) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/99/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/99/galaxy_99.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:16:13,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 98: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:16:13,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 99: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:16:18,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 98 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:16:18,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 99 finished
galaxy.model.metadata DEBUG 2025-01-13 01:16:18,998 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 101
galaxy.model.metadata DEBUG 2025-01-13 01:16:19,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 102
galaxy.jobs INFO 2025-01-13 01:16:19,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 98 in /galaxy/server/database/jobs_directory/000/98
galaxy.jobs INFO 2025-01-13 01:16:19,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 99 in /galaxy/server/database/jobs_directory/000/99
galaxy.jobs DEBUG 2025-01-13 01:16:19,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 98 executed (167.171 ms)
galaxy.jobs DEBUG 2025-01-13 01:16:19,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 99 executed (163.130 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:16:20,544 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 100
tpv.core.entities DEBUG 2025-01-13 01:16:20,588 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:16:20,589 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:16:20,595 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:16:20,613 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:16:20,634 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Working directory for job is: /galaxy/server/database/jobs_directory/000/100
galaxy.jobs.runners DEBUG 2025-01-13 01:16:20,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [100] queued (47.677 ms)
galaxy.jobs.handler INFO 2025-01-13 01:16:20,646 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:16:20,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 100
galaxy.jobs DEBUG 2025-01-13 01:16:20,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [100] prepared (74.896 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:16:20,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:16:20,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_concat/bcftools_concat/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:16:20,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:16:20,793 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/100/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/100/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/3/3/7/dataset_33770074-df42-4c1b-806a-e3f18259ab45.dat' > input0.vcf.gz && bcftools index input0.vcf.gz && echo 'input0.vcf.gz' >> vcfs_list &&  bgzip -c '/galaxy/server/database/objects/4/5/7/dataset_457da5cd-2ba2-440b-9602-8ab5bea2b5a1.dat' > input1.vcf.gz && bcftools index input1.vcf.gz && echo 'input1.vcf.gz' >> vcfs_list &&         bcftools concat   --ligate --ligate-warn  --min-PQ 30         --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input0.vcf.gz input1.vcf.gz  > '/galaxy/server/database/objects/f/7/1/dataset_f715d245-82ef-4ea7-b479-809c7e24d0df.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:16:20,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (100) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/100/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/100/galaxy_100.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:16:20,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:16:20,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_concat/bcftools_concat/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:16:20,838 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes ERROR 2025-01-13 01:16:25,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://10.100.128.1:443/apis/batch/v1/namespaces/prod-25-01-13-00-39-1/jobs/gxy-cwrgt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 817, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 824, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 107, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-cwrgt": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:16:25,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (100/gxy-cwrgt) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-01-13 01:16:27,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 101
tpv.core.entities DEBUG 2025-01-13 01:16:27,836 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:16:27,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:16:27,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:16:27,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:16:27,880 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Working directory for job is: /galaxy/server/database/jobs_directory/000/101
galaxy.jobs.runners DEBUG 2025-01-13 01:16:27,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [101] queued (44.934 ms)
galaxy.jobs.handler INFO 2025-01-13 01:16:27,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:16:27,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 101
galaxy.jobs DEBUG 2025-01-13 01:16:28,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [101] prepared (100.543 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:16:28,039 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/101/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/101/registry.xml' '/galaxy/server/database/jobs_directory/000/101/upload_params.json' '104:/galaxy/server/database/objects/0/0/3/dataset_0031d0e8-f93a-4d14-a637-cb16eef8940a_files:/galaxy/server/database/objects/0/0/3/dataset_0031d0e8-f93a-4d14-a637-cb16eef8940a.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:16:28,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (101) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/101/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/101/galaxy_101.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:16:28,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 102, 103
tpv.core.entities DEBUG 2025-01-13 01:16:28,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:16:28,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:16:28,937 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:16:28,956 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:16:28,978 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Working directory for job is: /galaxy/server/database/jobs_directory/000/102
galaxy.jobs.runners DEBUG 2025-01-13 01:16:28,987 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [102] queued (49.125 ms)
galaxy.jobs.handler INFO 2025-01-13 01:16:28,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:16:28,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 102
tpv.core.entities DEBUG 2025-01-13 01:16:29,013 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:16:29,014 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:16:29,023 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:16:29,046 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:16:29,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Working directory for job is: /galaxy/server/database/jobs_directory/000/103
galaxy.jobs.runners DEBUG 2025-01-13 01:16:29,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [103] queued (63.770 ms)
galaxy.jobs.handler INFO 2025-01-13 01:16:29,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:16:29,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 103
galaxy.jobs DEBUG 2025-01-13 01:16:29,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [102] prepared (125.751 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:16:29,182 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/102/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/102/registry.xml' '/galaxy/server/database/jobs_directory/000/102/upload_params.json' '105:/galaxy/server/database/objects/d/4/9/dataset_d49de411-940c-4d78-a7bc-b7c76f8c5c8b_files:/galaxy/server/database/objects/d/4/9/dataset_d49de411-940c-4d78-a7bc-b7c76f8c5c8b.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:16:29,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (102) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/102/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/102/galaxy_102.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:16:29,231 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [103] prepared (112.691 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:16:29,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/103/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/103/registry.xml' '/galaxy/server/database/jobs_directory/000/103/upload_params.json' '106:/galaxy/server/database/objects/6/2/f/dataset_62f02c80-0a4c-4860-98d6-788a83919cab_files:/galaxy/server/database/objects/6/2/f/dataset_62f02c80-0a4c-4860-98d6-788a83919cab.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:16:29,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (103) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/103/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/103/galaxy_103.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:16:37,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 101: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:16:37,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 103: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:16:37,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 102: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:16:44,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 103 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:16:44,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 102 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:16:44,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 101 finished
galaxy.model.metadata DEBUG 2025-01-13 01:16:45,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 106
galaxy.model.metadata DEBUG 2025-01-13 01:16:45,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 105
galaxy.model.metadata DEBUG 2025-01-13 01:16:45,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 104
galaxy.jobs INFO 2025-01-13 01:16:45,087 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 103 in /galaxy/server/database/jobs_directory/000/103
galaxy.jobs INFO 2025-01-13 01:16:45,105 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 102 in /galaxy/server/database/jobs_directory/000/102
galaxy.jobs INFO 2025-01-13 01:16:45,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 101 in /galaxy/server/database/jobs_directory/000/101
galaxy.jobs DEBUG 2025-01-13 01:16:45,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 103 executed (218.923 ms)
galaxy.jobs DEBUG 2025-01-13 01:16:45,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 102 executed (210.225 ms)
galaxy.jobs DEBUG 2025-01-13 01:16:45,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 101 executed (195.909 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:16:46,576 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 104
tpv.core.entities DEBUG 2025-01-13 01:16:46,625 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:16:46,626 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:16:46,632 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:16:46,650 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:16:46,671 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Working directory for job is: /galaxy/server/database/jobs_directory/000/104
galaxy.jobs.runners DEBUG 2025-01-13 01:16:46,681 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [104] queued (49.580 ms)
galaxy.jobs.handler INFO 2025-01-13 01:16:46,685 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (104) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:16:46,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 104
galaxy.jobs DEBUG 2025-01-13 01:16:46,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [104] prepared (157.036 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:16:46,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:16:46,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:16:46,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:16:46,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/104/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/104/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/0/0/3/dataset_0031d0e8-f93a-4d14-a637-cb16eef8940a.dat' > input.vcf.gz && bcftools index input.vcf.gz &&   bgzip -c '/galaxy/server/database/objects/d/4/9/dataset_d49de411-940c-4d78-a7bc-b7c76f8c5c8b.dat' > annotations.tab.gz && tabix -s 1 -b 2 -e 2 annotations.tab.gz &&  bcftools annotate       --columns 'CHROM,POS,REF,ALT,ID,QUAL,INFO/T_INT,INFO/T_FLOAT,INDEL'  --annotations 'annotations.tab.gz' --header-lines '/galaxy/server/database/objects/6/2/f/dataset_62f02c80-0a4c-4860-98d6-788a83919cab.dat'                    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/f/5/d/dataset_f5dfe363-0f93-46cc-801b-e94f3081a886.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:16:46,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (104) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/104/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/104/galaxy_104.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:16:46,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:16:46,947 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:16:46,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:16:53,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 104: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:16:58,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 104 finished
galaxy.model.metadata DEBUG 2025-01-13 01:16:58,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 107
galaxy.jobs INFO 2025-01-13 01:16:58,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 104 in /galaxy/server/database/jobs_directory/000/104
galaxy.objectstore CRITICAL 2025-01-13 01:16:58,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] None delete error [Errno 13] Permission denied: 'tmp4hlk3qlr'
galaxy.jobs DEBUG 2025-01-13 01:16:58,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 104 executed (130.826 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:16:59,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 105
tpv.core.entities DEBUG 2025-01-13 01:17:00,029 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:17:00,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:17:00,035 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:17:00,052 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:17:00,075 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Working directory for job is: /galaxy/server/database/jobs_directory/000/105
galaxy.jobs.runners DEBUG 2025-01-13 01:17:00,083 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [105] queued (47.972 ms)
galaxy.jobs.handler INFO 2025-01-13 01:17:00,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (105) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:17:00,092 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 105
galaxy.jobs DEBUG 2025-01-13 01:17:00,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [105] prepared (95.340 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:17:00,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/105/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/105/registry.xml' '/galaxy/server/database/jobs_directory/000/105/upload_params.json' '108:/galaxy/server/database/objects/7/e/8/dataset_7e82284a-d1d9-4244-b5a4-94303a3e64a1_files:/galaxy/server/database/objects/7/e/8/dataset_7e82284a-d1d9-4244-b5a4-94303a3e64a1.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:17:00,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (105) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/105/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/105/galaxy_105.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:17:01,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 107, 106
tpv.core.entities DEBUG 2025-01-13 01:17:01,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:17:01,130 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:17:01,136 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:17:01,155 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:17:01,175 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Working directory for job is: /galaxy/server/database/jobs_directory/000/106
galaxy.jobs.runners DEBUG 2025-01-13 01:17:01,183 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [106] queued (46.864 ms)
galaxy.jobs.handler INFO 2025-01-13 01:17:01,187 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (106) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:17:01,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 106
tpv.core.entities DEBUG 2025-01-13 01:17:01,214 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:17:01,215 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:17:01,222 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:17:01,242 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:17:01,276 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Working directory for job is: /galaxy/server/database/jobs_directory/000/107
galaxy.jobs.runners DEBUG 2025-01-13 01:17:01,285 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [107] queued (62.949 ms)
galaxy.jobs.handler INFO 2025-01-13 01:17:01,290 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (107) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:17:01,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 107
galaxy.jobs DEBUG 2025-01-13 01:17:01,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [106] prepared (114.250 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:17:01,372 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/106/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/106/registry.xml' '/galaxy/server/database/jobs_directory/000/106/upload_params.json' '109:/galaxy/server/database/objects/d/c/0/dataset_dc024a7b-d4a9-4b05-a127-59d2231a93e8_files:/galaxy/server/database/objects/d/c/0/dataset_dc024a7b-d4a9-4b05-a127-59d2231a93e8.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:17:01,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (106) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/106/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/106/galaxy_106.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:17:01,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [107] prepared (112.949 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:17:01,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/107/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/107/registry.xml' '/galaxy/server/database/jobs_directory/000/107/upload_params.json' '110:/galaxy/server/database/objects/0/e/2/dataset_0e2dc76e-d2ab-434e-95ac-c40218e6ce3b_files:/galaxy/server/database/objects/0/e/2/dataset_0e2dc76e-d2ab-434e-95ac-c40218e6ce3b.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:17:01,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (107) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/107/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/107/galaxy_107.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:17:09,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 106: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:17:10,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 105: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:17:10,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 107: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:17:16,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 106 finished
galaxy.model.metadata DEBUG 2025-01-13 01:17:16,185 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 109
galaxy.jobs INFO 2025-01-13 01:17:16,240 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 106 in /galaxy/server/database/jobs_directory/000/106
galaxy.jobs DEBUG 2025-01-13 01:17:16,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 106 executed (153.260 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:17:17,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 107 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:17:17,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 105 finished
galaxy.model.metadata DEBUG 2025-01-13 01:17:17,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 110
galaxy.model.metadata DEBUG 2025-01-13 01:17:17,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 108
galaxy.jobs INFO 2025-01-13 01:17:17,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 105 in /galaxy/server/database/jobs_directory/000/105
galaxy.jobs INFO 2025-01-13 01:17:17,539 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 107 in /galaxy/server/database/jobs_directory/000/107
galaxy.jobs DEBUG 2025-01-13 01:17:17,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 105 executed (148.403 ms)
galaxy.jobs DEBUG 2025-01-13 01:17:17,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 107 executed (169.870 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:17:18,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 108
tpv.core.entities DEBUG 2025-01-13 01:17:18,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:17:18,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:17:18,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:17:18,958 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:17:18,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Working directory for job is: /galaxy/server/database/jobs_directory/000/108
galaxy.jobs.runners DEBUG 2025-01-13 01:17:18,985 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [108] queued (42.865 ms)
galaxy.jobs.handler INFO 2025-01-13 01:17:18,988 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (108) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:17:18,992 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 108
galaxy.jobs DEBUG 2025-01-13 01:17:19,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [108] prepared (79.240 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:17:19,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:17:19,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:17:19,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:17:19,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/108/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/108/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/7/e/8/dataset_7e82284a-d1d9-4244-b5a4-94303a3e64a1.dat' > input.vcf.gz && bcftools index input.vcf.gz &&   bgzip -c '/galaxy/server/database/objects/d/c/0/dataset_dc024a7b-d4a9-4b05-a127-59d2231a93e8.dat' > annotations.bed.gz && tabix -s 1 -b 2 -e 3 annotations.bed.gz &&  bcftools annotate       --columns 'CHROM,FROM,TO,T_STR'  --annotations 'annotations.bed.gz' --header-lines '/galaxy/server/database/objects/0/e/2/dataset_0e2dc76e-d2ab-434e-95ac-c40218e6ce3b.dat'  --set-id '%CHROM\_%POS\_%REF\_%FIRST_ALT'                   --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/d/2/d/dataset_d2db6926-7b72-493e-8f66-1f8bb0f9699f.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:17:19,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (108) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/108/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/108/galaxy_108.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:17:19,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:17:19,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:17:19,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:17:24,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 108: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:17:29,078 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 108 finished
galaxy.model.metadata DEBUG 2025-01-13 01:17:29,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 111
galaxy.jobs INFO 2025-01-13 01:17:29,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 108 in /galaxy/server/database/jobs_directory/000/108
galaxy.objectstore CRITICAL 2025-01-13 01:17:29,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] None delete error [Errno 13] Permission denied: 'tmp6lfq2699'
galaxy.jobs DEBUG 2025-01-13 01:17:29,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 108 executed (126.302 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:17:30,341 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 109
tpv.core.entities DEBUG 2025-01-13 01:17:30,382 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:17:30,383 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:17:30,388 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:17:30,407 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:17:30,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Working directory for job is: /galaxy/server/database/jobs_directory/000/109
galaxy.jobs.runners DEBUG 2025-01-13 01:17:30,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [109] queued (43.288 ms)
galaxy.jobs.handler INFO 2025-01-13 01:17:30,435 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (109) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:17:30,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 109
galaxy.jobs DEBUG 2025-01-13 01:17:30,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [109] prepared (105.720 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:17:30,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/109/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/109/registry.xml' '/galaxy/server/database/jobs_directory/000/109/upload_params.json' '112:/galaxy/server/database/objects/e/1/2/dataset_e12a8e87-6d9e-44dc-8336-5f6106908314_files:/galaxy/server/database/objects/e/1/2/dataset_e12a8e87-6d9e-44dc-8336-5f6106908314.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:17:30,606 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (109) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/109/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/109/galaxy_109.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:17:31,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 110
tpv.core.entities DEBUG 2025-01-13 01:17:31,475 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:17:31,476 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:17:31,481 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:17:31,497 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:17:31,516 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Working directory for job is: /galaxy/server/database/jobs_directory/000/110
galaxy.jobs.runners DEBUG 2025-01-13 01:17:31,523 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [110] queued (41.530 ms)
galaxy.jobs.handler INFO 2025-01-13 01:17:31,527 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (110) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:17:31,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 110
galaxy.jobs DEBUG 2025-01-13 01:17:31,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [110] prepared (91.362 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:17:31,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/110/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/110/registry.xml' '/galaxy/server/database/jobs_directory/000/110/upload_params.json' '113:/galaxy/server/database/objects/0/2/b/dataset_02bb2160-24c2-4372-a166-1d80020b7cfd_files:/galaxy/server/database/objects/0/2/b/dataset_02bb2160-24c2-4372-a166-1d80020b7cfd.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:17:31,677 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (110) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/110/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/110/galaxy_110.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:17:39,553 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 109: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:17:40,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 110: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:17:44,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 109 finished
galaxy.model.metadata DEBUG 2025-01-13 01:17:44,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 112
galaxy.jobs INFO 2025-01-13 01:17:44,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 109 in /galaxy/server/database/jobs_directory/000/109
galaxy.jobs DEBUG 2025-01-13 01:17:44,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 109 executed (134.612 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:17:45,584 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 110 finished
galaxy.model.metadata DEBUG 2025-01-13 01:17:45,647 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 113
galaxy.jobs INFO 2025-01-13 01:17:45,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 110 in /galaxy/server/database/jobs_directory/000/110
galaxy.jobs DEBUG 2025-01-13 01:17:45,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 110 executed (136.464 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:17:46,959 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 111
tpv.core.entities DEBUG 2025-01-13 01:17:46,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:17:46,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:17:47,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:17:47,025 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:17:47,046 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Working directory for job is: /galaxy/server/database/jobs_directory/000/111
galaxy.jobs.runners DEBUG 2025-01-13 01:17:47,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [111] queued (48.781 ms)
galaxy.jobs.handler INFO 2025-01-13 01:17:47,059 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (111) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:17:47,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 111
galaxy.jobs DEBUG 2025-01-13 01:17:47,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [111] prepared (73.400 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:17:47,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:17:47,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:17:47,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:17:47,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/111/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/111/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/e/1/2/dataset_e12a8e87-6d9e-44dc-8336-5f6106908314.dat' > input.vcf.gz && bcftools index input.vcf.gz &&   bgzip -c '/galaxy/server/database/objects/0/2/b/dataset_02bb2160-24c2-4372-a166-1d80020b7cfd.dat' > annotations.vcf.gz && bcftools index annotations.vcf.gz &&  bcftools annotate       --columns 'STR,ID,QUAL,FILTER'  --annotations 'annotations.vcf.gz'                    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/3/5/8/dataset_3582c8bf-3c5f-4a92-a360-451db2a289ea.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:17:47,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (111) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/111/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/111/galaxy_111.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:17:47,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:17:47,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:17:47,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:17:53,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 111: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:17:58,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 111 finished
galaxy.model.metadata DEBUG 2025-01-13 01:17:58,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 114
galaxy.jobs INFO 2025-01-13 01:17:58,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 111 in /galaxy/server/database/jobs_directory/000/111
galaxy.objectstore CRITICAL 2025-01-13 01:17:58,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] None delete error [Errno 13] Permission denied: 'tmpjn6hubh3'
galaxy.jobs DEBUG 2025-01-13 01:17:58,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 111 executed (113.775 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:18:00,340 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 112, 113
tpv.core.entities DEBUG 2025-01-13 01:18:00,377 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:18:00,377 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:18:00,381 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:18:00,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:18:00,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Working directory for job is: /galaxy/server/database/jobs_directory/000/112
galaxy.jobs.runners DEBUG 2025-01-13 01:18:00,423 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [112] queued (41.115 ms)
galaxy.jobs.handler INFO 2025-01-13 01:18:00,428 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (112) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:18:00,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 112
tpv.core.entities DEBUG 2025-01-13 01:18:00,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:18:00,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:18:00,454 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:18:00,472 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:18:00,503 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Working directory for job is: /galaxy/server/database/jobs_directory/000/113
galaxy.jobs.runners DEBUG 2025-01-13 01:18:00,512 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [113] queued (58.501 ms)
galaxy.jobs.handler INFO 2025-01-13 01:18:00,518 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (113) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:18:00,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 113
galaxy.jobs DEBUG 2025-01-13 01:18:00,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [112] prepared (110.904 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:18:00,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/112/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/112/registry.xml' '/galaxy/server/database/jobs_directory/000/112/upload_params.json' '115:/galaxy/server/database/objects/b/2/7/dataset_b27a42d3-83bd-4c42-a939-80e253c15c38_files:/galaxy/server/database/objects/b/2/7/dataset_b27a42d3-83bd-4c42-a939-80e253c15c38.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:18:00,616 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (112) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/112/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/112/galaxy_112.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:18:00,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [113] prepared (107.064 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:18:00,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/113/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/113/registry.xml' '/galaxy/server/database/jobs_directory/000/113/upload_params.json' '116:/galaxy/server/database/objects/7/0/7/dataset_707f02d0-0da5-4cf1-abbd-f05c2fe38fce_files:/galaxy/server/database/objects/7/0/7/dataset_707f02d0-0da5-4cf1-abbd-f05c2fe38fce.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:18:00,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (113) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/113/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/113/galaxy_113.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:18:10,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 112: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:18:10,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 113: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:18:15,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 112 finished
galaxy.model.metadata DEBUG 2025-01-13 01:18:15,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 115
galaxy.jobs.runners DEBUG 2025-01-13 01:18:15,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 113 finished
galaxy.jobs INFO 2025-01-13 01:18:15,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 112 in /galaxy/server/database/jobs_directory/000/112
galaxy.model.metadata DEBUG 2025-01-13 01:18:15,391 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 116
galaxy.jobs DEBUG 2025-01-13 01:18:15,412 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 112 executed (166.682 ms)
galaxy.jobs INFO 2025-01-13 01:18:15,474 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 113 in /galaxy/server/database/jobs_directory/000/113
galaxy.jobs DEBUG 2025-01-13 01:18:15,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 113 executed (181.828 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:18:15,861 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 114
tpv.core.entities DEBUG 2025-01-13 01:18:15,899 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:18:15,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:18:15,906 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:18:15,923 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:18:15,943 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Working directory for job is: /galaxy/server/database/jobs_directory/000/114
galaxy.jobs.runners DEBUG 2025-01-13 01:18:15,952 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [114] queued (46.044 ms)
galaxy.jobs.handler INFO 2025-01-13 01:18:15,956 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (114) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:18:15,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 114
galaxy.jobs DEBUG 2025-01-13 01:18:16,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [114] prepared (82.123 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:18:16,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:18:16,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:18:16,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:18:16,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/114/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/114/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/b/2/7/dataset_b27a42d3-83bd-4c42-a939-80e253c15c38.dat' > input.vcf.gz && bcftools index input.vcf.gz &&   ln -s '/galaxy/server/database/objects/7/0/7/dataset_707f02d0-0da5-4cf1-abbd-f05c2fe38fce.dat' annotations.bcf && ln -s '/galaxy/server/database/objects/_metadata_files/c/4/c/metadata_c4c4a392-1f14-407f-ab0b-6e0499084972.dat' annotations.bcf.csi &&  bcftools annotate       --columns 'STR,ID,QUAL,FILTER'  --annotations 'annotations.bcf'                    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/0/0/f/dataset_00f61620-7a2c-4b48-a1c6-e13693dad8a7.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:18:16,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (114) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/114/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/114/galaxy_114.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:18:16,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:18:16,139 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:18:16,159 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes ERROR 2025-01-13 01:18:21,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://10.100.128.1:443/apis/batch/v1/namespaces/prod-25-01-13-00-39-1/jobs/gxy-qbr65

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 817, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 824, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 107, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-qbr65": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:18:21,205 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (114/gxy-qbr65) Terminated at user's request
galaxy.objectstore CRITICAL 2025-01-13 01:18:21,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] None delete error [Errno 13] Permission denied: 'tmp6osw4f0i'
galaxy.jobs.handler DEBUG 2025-01-13 01:18:23,118 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 115, 116
tpv.core.entities DEBUG 2025-01-13 01:18:23,154 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:18:23,154 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:18:23,161 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:18:23,178 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:18:23,196 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Working directory for job is: /galaxy/server/database/jobs_directory/000/115
galaxy.jobs.runners DEBUG 2025-01-13 01:18:23,203 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [115] queued (42.401 ms)
galaxy.jobs.handler INFO 2025-01-13 01:18:23,208 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (115) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:18:23,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 115
tpv.core.entities DEBUG 2025-01-13 01:18:23,228 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:18:23,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:18:23,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:18:23,256 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:18:23,288 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Working directory for job is: /galaxy/server/database/jobs_directory/000/116
galaxy.jobs.runners DEBUG 2025-01-13 01:18:23,296 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [116] queued (60.589 ms)
galaxy.jobs.handler INFO 2025-01-13 01:18:23,301 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (116) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:18:23,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 116
galaxy.jobs DEBUG 2025-01-13 01:18:23,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [115] prepared (106.706 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:18:23,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/115/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/115/registry.xml' '/galaxy/server/database/jobs_directory/000/115/upload_params.json' '118:/galaxy/server/database/objects/0/3/7/dataset_037dd330-a6d5-4098-852f-2b56124f575c_files:/galaxy/server/database/objects/0/3/7/dataset_037dd330-a6d5-4098-852f-2b56124f575c.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:18:23,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (115) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/115/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/115/galaxy_115.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:18:23,435 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [116] prepared (107.959 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:18:23,461 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/116/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/116/registry.xml' '/galaxy/server/database/jobs_directory/000/116/upload_params.json' '119:/galaxy/server/database/objects/1/d/0/dataset_1d017b5a-eb27-4aed-adf5-21547539262f_files:/galaxy/server/database/objects/1/d/0/dataset_1d017b5a-eb27-4aed-adf5-21547539262f.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:18:23,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (116) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/116/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/116/galaxy_116.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:18:32,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 115: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:18:32,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 116: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:18:38,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 115 finished
galaxy.model.metadata DEBUG 2025-01-13 01:18:38,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 118
galaxy.jobs INFO 2025-01-13 01:18:38,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 115 in /galaxy/server/database/jobs_directory/000/115
galaxy.jobs.runners DEBUG 2025-01-13 01:18:38,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 116 finished
galaxy.jobs DEBUG 2025-01-13 01:18:38,413 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 115 executed (162.917 ms)
galaxy.model.metadata DEBUG 2025-01-13 01:18:38,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 119
galaxy.jobs INFO 2025-01-13 01:18:38,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 116 in /galaxy/server/database/jobs_directory/000/116
galaxy.jobs DEBUG 2025-01-13 01:18:38,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 116 executed (154.995 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:18:39,700 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 117
tpv.core.entities DEBUG 2025-01-13 01:18:39,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:18:39,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:18:39,751 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:18:39,771 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:18:39,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Working directory for job is: /galaxy/server/database/jobs_directory/000/117
galaxy.jobs.runners DEBUG 2025-01-13 01:18:39,802 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [117] queued (50.723 ms)
galaxy.jobs.handler INFO 2025-01-13 01:18:39,806 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (117) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:18:39,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 117
galaxy.jobs DEBUG 2025-01-13 01:18:39,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [117] prepared (83.101 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:18:39,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:18:39,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:18:39,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:18:39,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/117/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/117/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/0/3/7/dataset_037dd330-a6d5-4098-852f-2b56124f575c.dat' > input.vcf.gz && bcftools index input.vcf.gz &&   bgzip -c '/galaxy/server/database/objects/1/d/0/dataset_1d017b5a-eb27-4aed-adf5-21547539262f.dat' > annotations.vcf.gz && bcftools index annotations.vcf.gz &&  bcftools annotate       --columns 'ID,QUAL,FILTER,INFO,FMT'  --annotations 'annotations.vcf.gz'                    --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/e/e/1/dataset_ee14e95a-869d-4637-9d94-154203657b46.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:18:39,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (117) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/117/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/117/galaxy_117.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:18:39,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:18:39,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:18:40,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:18:46,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 117: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:18:50,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 117 finished
galaxy.model.metadata DEBUG 2025-01-13 01:18:51,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 120
galaxy.jobs INFO 2025-01-13 01:18:51,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 117 in /galaxy/server/database/jobs_directory/000/117
galaxy.objectstore CRITICAL 2025-01-13 01:18:51,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] None delete error [Errno 13] Permission denied: 'tmp6gxtxgwz'
galaxy.jobs DEBUG 2025-01-13 01:18:51,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 117 executed (137.535 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:18:53,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 119, 118
tpv.core.entities DEBUG 2025-01-13 01:18:53,137 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:18:53,138 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:18:53,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:18:53,162 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:18:53,183 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Working directory for job is: /galaxy/server/database/jobs_directory/000/118
galaxy.jobs.runners DEBUG 2025-01-13 01:18:53,191 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [118] queued (45.755 ms)
galaxy.jobs.handler INFO 2025-01-13 01:18:53,195 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (118) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:18:53,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 118
tpv.core.entities DEBUG 2025-01-13 01:18:53,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:18:53,216 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:18:53,223 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:18:53,241 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:18:53,273 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Working directory for job is: /galaxy/server/database/jobs_directory/000/119
galaxy.jobs.runners DEBUG 2025-01-13 01:18:53,281 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [119] queued (58.295 ms)
galaxy.jobs.handler INFO 2025-01-13 01:18:53,287 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (119) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:18:53,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 119
galaxy.jobs DEBUG 2025-01-13 01:18:53,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [118] prepared (116.432 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:18:53,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/118/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/118/registry.xml' '/galaxy/server/database/jobs_directory/000/118/upload_params.json' '121:/galaxy/server/database/objects/6/1/4/dataset_6143ae62-96a0-40be-9b0f-87359cf55095_files:/galaxy/server/database/objects/6/1/4/dataset_6143ae62-96a0-40be-9b0f-87359cf55095.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:18:53,397 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (118) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/118/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/118/galaxy_118.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:18:53,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [119] prepared (118.992 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:18:53,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/119/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/119/registry.xml' '/galaxy/server/database/jobs_directory/000/119/upload_params.json' '122:/galaxy/server/database/objects/a/0/c/dataset_a0c57f30-1382-4a27-8186-8bf6a9536a31_files:/galaxy/server/database/objects/a/0/c/dataset_a0c57f30-1382-4a27-8186-8bf6a9536a31.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:18:53,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (119) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/119/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/119/galaxy_119.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes ERROR 2025-01-13 01:19:02,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://10.100.128.1:443/apis/batch/v1/namespaces/prod-25-01-13-00-39-1/jobs/gxy-zkkcg

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 817, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 824, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 107, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-zkkcg": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:19:02,412 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (118/gxy-zkkcg) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-01-13 01:19:02,561 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 119: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-01-13 01:19:03,545 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 120
tpv.core.entities DEBUG 2025-01-13 01:19:03,674 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:19:03,674 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:19:03,679 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:19:03,698 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:19:03,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Working directory for job is: /galaxy/server/database/jobs_directory/000/120
galaxy.jobs.runners DEBUG 2025-01-13 01:19:03,726 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [120] queued (46.343 ms)
galaxy.jobs.handler INFO 2025-01-13 01:19:03,730 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (120) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:19:03,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 120
galaxy.jobs DEBUG 2025-01-13 01:19:03,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [120] prepared (98.030 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:19:03,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/120/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/120/registry.xml' '/galaxy/server/database/jobs_directory/000/120/upload_params.json' '123:/galaxy/server/database/objects/9/a/b/dataset_9ab152b7-1d5d-46fe-9b51-ce341af7d03f_files:/galaxy/server/database/objects/9/a/b/dataset_9ab152b7-1d5d-46fe-9b51-ce341af7d03f.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:19:03,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (120) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/120/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/120/galaxy_120.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:19:07,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 119 finished
galaxy.model.metadata DEBUG 2025-01-13 01:19:07,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 122
galaxy.jobs INFO 2025-01-13 01:19:07,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 119 in /galaxy/server/database/jobs_directory/000/119
galaxy.jobs DEBUG 2025-01-13 01:19:07,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 119 executed (149.725 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:19:12,840 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 120: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:19:17,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 120 finished
galaxy.model.metadata DEBUG 2025-01-13 01:19:17,755 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 123
galaxy.jobs INFO 2025-01-13 01:19:17,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 120 in /galaxy/server/database/jobs_directory/000/120
galaxy.jobs DEBUG 2025-01-13 01:19:17,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 120 executed (134.422 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:19:19,110 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 121
tpv.core.entities DEBUG 2025-01-13 01:19:19,148 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:19:19,149 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:19:19,155 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:19:19,171 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:19:19,190 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Working directory for job is: /galaxy/server/database/jobs_directory/000/121
galaxy.jobs.runners DEBUG 2025-01-13 01:19:19,199 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [121] queued (43.988 ms)
galaxy.jobs.handler INFO 2025-01-13 01:19:19,203 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (121) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:19:19,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 121
galaxy.jobs DEBUG 2025-01-13 01:19:19,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [121] prepared (71.177 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:19:19,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:19:19,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:19:19,325 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:19:19,350 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/121/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/121/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/9/a/b/dataset_9ab152b7-1d5d-46fe-9b51-ce341af7d03f.dat' > input.vcf.gz && bcftools index input.vcf.gz &&    bcftools annotate          --remove 'ID,QUAL,^FILTER/fltA,FILTER/fltB,^INFO/AA,INFO/BB,^FMT/GT,FMT/PL'                  --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/9/a/7/dataset_9a780a3d-ea61-493e-b724-04eacacd8a35.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:19:19,365 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (121) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/121/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/121/galaxy_121.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:19:19,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:19:19,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:19:19,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:19:24,983 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 121: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:19:29,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 121 finished
galaxy.model.metadata DEBUG 2025-01-13 01:19:29,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 124
galaxy.jobs INFO 2025-01-13 01:19:29,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 121 in /galaxy/server/database/jobs_directory/000/121
galaxy.objectstore CRITICAL 2025-01-13 01:19:30,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] None delete error [Errno 13] Permission denied: 'tmpj39__hdq'
galaxy.jobs DEBUG 2025-01-13 01:19:30,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 121 executed (127.573 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:19:31,465 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 122
tpv.core.entities DEBUG 2025-01-13 01:19:31,501 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:19:31,502 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:19:31,507 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:19:31,525 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:19:31,545 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Working directory for job is: /galaxy/server/database/jobs_directory/000/122
galaxy.jobs.runners DEBUG 2025-01-13 01:19:31,552 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [122] queued (45.011 ms)
galaxy.jobs.handler INFO 2025-01-13 01:19:31,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (122) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:19:31,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 122
galaxy.jobs DEBUG 2025-01-13 01:19:31,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [122] prepared (94.078 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:19:31,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/122/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/122/registry.xml' '/galaxy/server/database/jobs_directory/000/122/upload_params.json' '125:/galaxy/server/database/objects/9/2/6/dataset_926a5299-ef23-4538-9e6e-1a3fa36db55b_files:/galaxy/server/database/objects/9/2/6/dataset_926a5299-ef23-4538-9e6e-1a3fa36db55b.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:19:31,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (122) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/122/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/122/galaxy_122.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:19:40,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 122: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:19:45,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 122 finished
galaxy.model.metadata DEBUG 2025-01-13 01:19:45,080 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 125
galaxy.jobs INFO 2025-01-13 01:19:45,124 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 122 in /galaxy/server/database/jobs_directory/000/122
galaxy.jobs DEBUG 2025-01-13 01:19:45,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 122 executed (136.470 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:19:45,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 123
tpv.core.entities DEBUG 2025-01-13 01:19:45,907 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:19:45,908 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:19:45,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:19:45,931 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:19:45,947 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Working directory for job is: /galaxy/server/database/jobs_directory/000/123
galaxy.jobs.runners DEBUG 2025-01-13 01:19:45,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [123] queued (40.866 ms)
galaxy.jobs.handler INFO 2025-01-13 01:19:45,961 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (123) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:19:45,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 123
galaxy.jobs DEBUG 2025-01-13 01:19:46,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [123] prepared (68.908 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:19:46,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:19:46,053 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:19:46,076 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:19:46,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/123/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/123/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/9/2/6/dataset_926a5299-ef23-4538-9e6e-1a3fa36db55b.dat' > input.vcf.gz && bcftools index input.vcf.gz &&    bcftools annotate          --remove 'FORMAT'                  --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/e/5/b/dataset_e5bbcc4a-11a3-48e8-8b3a-0e0cca729b46.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:19:46,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (123) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/123/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/123/galaxy_123.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:19:46,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:19:46,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:19:46,145 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:19:52,304 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 123: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:19:57,218 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 123 finished
galaxy.model.metadata DEBUG 2025-01-13 01:19:57,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 126
galaxy.jobs INFO 2025-01-13 01:19:57,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 123 in /galaxy/server/database/jobs_directory/000/123
galaxy.objectstore CRITICAL 2025-01-13 01:19:57,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] None delete error [Errno 13] Permission denied: 'tmpgkx7rmio'
galaxy.jobs DEBUG 2025-01-13 01:19:57,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 123 executed (133.309 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:19:59,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 124
tpv.core.entities DEBUG 2025-01-13 01:19:59,279 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:19:59,280 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:19:59,286 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:19:59,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:19:59,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Working directory for job is: /galaxy/server/database/jobs_directory/000/124
galaxy.jobs.runners DEBUG 2025-01-13 01:19:59,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [124] queued (38.592 ms)
galaxy.jobs.handler INFO 2025-01-13 01:19:59,328 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (124) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:19:59,331 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 124
galaxy.jobs DEBUG 2025-01-13 01:19:59,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [124] prepared (111.634 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:19:59,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/124/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/124/registry.xml' '/galaxy/server/database/jobs_directory/000/124/upload_params.json' '127:/galaxy/server/database/objects/e/d/f/dataset_edfb6307-f6a0-48aa-ba49-fc7a1ac0fb42_files:/galaxy/server/database/objects/e/d/f/dataset_edfb6307-f6a0-48aa-ba49-fc7a1ac0fb42.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:19:59,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (124) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/124/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/124/galaxy_124.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:20:08,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 124: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:20:13,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 124 finished
galaxy.model.metadata DEBUG 2025-01-13 01:20:13,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 127
galaxy.jobs INFO 2025-01-13 01:20:13,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 124 in /galaxy/server/database/jobs_directory/000/124
galaxy.jobs DEBUG 2025-01-13 01:20:13,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 124 executed (130.824 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:20:14,668 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 125
tpv.core.entities DEBUG 2025-01-13 01:20:14,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:20:14,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:20:14,714 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:20:14,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:20:14,749 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Working directory for job is: /galaxy/server/database/jobs_directory/000/125
galaxy.jobs.runners DEBUG 2025-01-13 01:20:14,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [125] queued (43.925 ms)
galaxy.jobs.handler INFO 2025-01-13 01:20:14,761 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (125) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:20:14,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 125
galaxy.jobs DEBUG 2025-01-13 01:20:14,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [125] prepared (65.656 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:20:14,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:20:14,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:20:14,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:20:14,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/125/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/125/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/e/d/f/dataset_edfb6307-f6a0-48aa-ba49-fc7a1ac0fb42.dat' > input.vcf.gz && bcftools index input.vcf.gz &&    bcftools annotate          --remove 'FORMAT'           --regions-overlap 1        --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/2/b/1/dataset_2b14e97c-fbf6-4a44-a2d0-e7a19d8ddf27.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:20:14,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (125) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/125/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/125/galaxy_125.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:20:14,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:20:14,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:20:14,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners.kubernetes ERROR 2025-01-13 01:20:19,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://10.100.128.1:443/apis/batch/v1/namespaces/prod-25-01-13-00-39-1/jobs/gxy-5x9r9

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 817, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 824, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 107, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-5x9r9": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:20:19,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (125/gxy-5x9r9) Terminated at user's request
galaxy.objectstore CRITICAL 2025-01-13 01:20:19,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] None delete error [Errno 13] Permission denied: 'tmpr6l73ukx'
galaxy.jobs.handler DEBUG 2025-01-13 01:20:20,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 126
tpv.core.entities DEBUG 2025-01-13 01:20:20,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:20:20,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:20:20,925 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:20:20,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:20:20,962 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Working directory for job is: /galaxy/server/database/jobs_directory/000/126
galaxy.jobs.runners DEBUG 2025-01-13 01:20:20,969 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [126] queued (43.668 ms)
galaxy.jobs.handler INFO 2025-01-13 01:20:20,973 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (126) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:20:20,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 126
galaxy.jobs DEBUG 2025-01-13 01:20:21,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [126] prepared (97.267 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:20:21,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/126/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/126/registry.xml' '/galaxy/server/database/jobs_directory/000/126/upload_params.json' '129:/galaxy/server/database/objects/1/b/c/dataset_1bcd9cc8-0940-4e5c-a7f6-d885147bafa1_files:/galaxy/server/database/objects/1/b/c/dataset_1bcd9cc8-0940-4e5c-a7f6-d885147bafa1.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:20:21,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (126) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/126/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/126/galaxy_126.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:20:28,908 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 126: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:20:33,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 126 finished
galaxy.model.metadata DEBUG 2025-01-13 01:20:33,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 129
galaxy.jobs INFO 2025-01-13 01:20:33,784 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 126 in /galaxy/server/database/jobs_directory/000/126
galaxy.jobs DEBUG 2025-01-13 01:20:33,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 126 executed (138.136 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:20:34,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 127
tpv.core.entities DEBUG 2025-01-13 01:20:34,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:20:34,307 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:20:34,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:20:34,332 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:20:34,354 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Working directory for job is: /galaxy/server/database/jobs_directory/000/127
galaxy.jobs.runners DEBUG 2025-01-13 01:20:34,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [127] queued (50.324 ms)
galaxy.jobs.handler INFO 2025-01-13 01:20:34,369 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (127) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:20:34,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 127
galaxy.jobs DEBUG 2025-01-13 01:20:34,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [127] prepared (66.761 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:20:34,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:20:34,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:20:34,484 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:20:34,507 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/127/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/127/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/1/b/c/dataset_1bcd9cc8-0940-4e5c-a7f6-d885147bafa1.dat' > input.vcf.gz && bcftools index input.vcf.gz &&    bcftools annotate         --min-overlap '0.5'  --remove 'FORMAT'                  --output-type 'v'   --threads ${GALAXY_SLOTS:-4}    input.vcf.gz  > '/galaxy/server/database/objects/7/3/8/dataset_7389f120-9c5d-464b-8f93-a54e6c096adf.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:20:34,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (127) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/127/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/127/galaxy_127.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:20:34,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:20:34,536 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-13 01:20:34,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:20:40,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 127: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:20:44,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 127 finished
galaxy.model.metadata DEBUG 2025-01-13 01:20:44,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 130
galaxy.jobs INFO 2025-01-13 01:20:45,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 127 in /galaxy/server/database/jobs_directory/000/127
galaxy.objectstore CRITICAL 2025-01-13 01:20:45,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] None delete error [Errno 13] Permission denied: 'tmp4bhgtmzr'
galaxy.jobs DEBUG 2025-01-13 01:20:45,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 127 executed (127.780 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:20:47,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 128
tpv.core.entities DEBUG 2025-01-13 01:20:47,706 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:20:47,706 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:20:47,712 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:20:47,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:20:47,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Working directory for job is: /galaxy/server/database/jobs_directory/000/128
galaxy.jobs.runners DEBUG 2025-01-13 01:20:47,757 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [128] queued (45.333 ms)
galaxy.jobs.handler INFO 2025-01-13 01:20:47,761 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (128) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:20:47,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 128
galaxy.jobs DEBUG 2025-01-13 01:20:47,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [128] prepared (106.599 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:20:47,918 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/128/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/128/registry.xml' '/galaxy/server/database/jobs_directory/000/128/upload_params.json' '131:/galaxy/server/database/objects/2/7/8/dataset_278db73b-4f93-4705-84fb-b08b29cd5ada_files:/galaxy/server/database/objects/2/7/8/dataset_278db73b-4f93-4705-84fb-b08b29cd5ada.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:20:47,934 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (128) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/128/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/128/galaxy_128.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:20:56,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 128: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:21:01,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 128 finished
galaxy.model.metadata DEBUG 2025-01-13 01:21:01,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 131
galaxy.jobs INFO 2025-01-13 01:21:01,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 128 in /galaxy/server/database/jobs_directory/000/128
galaxy.jobs DEBUG 2025-01-13 01:21:01,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 128 executed (134.447 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:21:02,081 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 129
tpv.core.entities DEBUG 2025-01-13 01:21:02,123 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:21:02,123 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:21:02,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:21:02,144 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:21:02,162 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Working directory for job is: /galaxy/server/database/jobs_directory/000/129
galaxy.jobs.runners DEBUG 2025-01-13 01:21:02,171 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [129] queued (42.002 ms)
galaxy.jobs.handler INFO 2025-01-13 01:21:02,175 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (129) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:21:02,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 129
galaxy.jobs DEBUG 2025-01-13 01:21:02,265 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [129] prepared (65.037 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:21:02,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:21:02,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcffixup/vcffixup/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-01-13 01:21:02,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:21:02,521 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/129/tool_script.sh] for tool command [vcffixup '/galaxy/server/database/objects/2/7/8/dataset_278db73b-4f93-4705-84fb-b08b29cd5ada.dat' > '/galaxy/server/database/objects/3/4/0/dataset_34023d19-d6bc-4b5a-87d9-5e00b34dceb3.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:21:02,534 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (129) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/129/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/129/galaxy_129.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:21:02,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:21:02,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcffixup/vcffixup/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-01-13 01:21:02,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:21:14,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 129: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:21:19,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 129 finished
galaxy.model.metadata DEBUG 2025-01-13 01:21:19,588 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 132
galaxy.jobs INFO 2025-01-13 01:21:19,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 129 in /galaxy/server/database/jobs_directory/000/129
galaxy.jobs DEBUG 2025-01-13 01:21:19,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 129 executed (136.323 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:21:22,635 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 130
tpv.core.entities DEBUG 2025-01-13 01:21:22,674 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:21:22,675 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:21:22,680 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:21:22,698 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:21:22,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Working directory for job is: /galaxy/server/database/jobs_directory/000/130
galaxy.jobs.runners DEBUG 2025-01-13 01:21:22,739 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [130] queued (58.542 ms)
galaxy.jobs.handler INFO 2025-01-13 01:21:22,743 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (130) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:21:22,748 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 130
galaxy.jobs DEBUG 2025-01-13 01:21:22,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [130] prepared (96.513 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:21:22,886 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/130/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/130/registry.xml' '/galaxy/server/database/jobs_directory/000/130/upload_params.json' '133:/galaxy/server/database/objects/5/5/e/dataset_55e78601-e11a-4f91-a227-b2e19e84f7f9_files:/galaxy/server/database/objects/5/5/e/dataset_55e78601-e11a-4f91-a227-b2e19e84f7f9.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:21:22,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (130) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/130/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/130/galaxy_130.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes ERROR 2025-01-13 01:21:30,808 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://10.100.128.1:443/apis/batch/v1/namespaces/prod-25-01-13-00-39-1/jobs/gxy-9mqm7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 817, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 824, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 107, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-9mqm7": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:21:30,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (130/gxy-9mqm7) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-01-13 01:21:32,964 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 131
tpv.core.entities DEBUG 2025-01-13 01:21:33,002 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:21:33,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:21:33,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:21:33,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:21:33,048 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Working directory for job is: /galaxy/server/database/jobs_directory/000/131
galaxy.jobs.runners DEBUG 2025-01-13 01:21:33,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [131] queued (46.966 ms)
galaxy.jobs.handler INFO 2025-01-13 01:21:33,059 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (131) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:21:33,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 131
galaxy.jobs DEBUG 2025-01-13 01:21:33,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [131] prepared (101.455 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:21:33,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/131/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/131/registry.xml' '/galaxy/server/database/jobs_directory/000/131/upload_params.json' '134:/galaxy/server/database/objects/8/3/6/dataset_836017af-9607-484d-9893-eb040d64f951_files:/galaxy/server/database/objects/8/3/6/dataset_836017af-9607-484d-9893-eb040d64f951.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:21:33,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (131) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/131/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/131/galaxy_131.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:21:41,125 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 131: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:21:45,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 131 finished
galaxy.model.metadata DEBUG 2025-01-13 01:21:45,911 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 134
galaxy.jobs INFO 2025-01-13 01:21:45,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 131 in /galaxy/server/database/jobs_directory/000/131
galaxy.jobs DEBUG 2025-01-13 01:21:46,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 131 executed (139.720 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:21:47,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 132
tpv.core.entities DEBUG 2025-01-13 01:21:47,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/.*, abstract=False, cores=1, mem=40, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:21:47,402 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:21:47,407 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:21:47,423 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:21:47,439 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Working directory for job is: /galaxy/server/database/jobs_directory/000/132
galaxy.jobs.runners DEBUG 2025-01-13 01:21:47,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [132] queued (39.717 ms)
galaxy.jobs.handler INFO 2025-01-13 01:21:47,450 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (132) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:21:47,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 132
galaxy.jobs DEBUG 2025-01-13 01:21:47,537 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [132] prepared (62.178 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:21:47,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:21:47,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/0.2.4+galaxy0: porechop:0.2.4
galaxy.tool_util.deps.containers INFO 2025-01-13 01:21:47,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/porechop:0.2.4--py312hf731ba3_9,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:21:47,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/132/tool_script.sh] for tool command [porechop --version > /galaxy/server/database/jobs_directory/000/132/outputs/COMMAND_VERSION 2>&1;
porechop -i '/galaxy/server/database/objects/8/3/6/dataset_836017af-9607-484d-9893-eb040d64f951.dat' --format 'fasta' --barcode_threshold '75.0' --barcode_diff '5.0'   --adapter_threshold '90.0' --check_reads '10000' --scoring_scheme '3,-6,-5,-2' --end_size '150' --min_trim_size '4' --extra_end_trim '2' --end_threshold '75.0'   --middle_threshold '85.0' --extra_middle_trim_good_side '10' --extra_middle_trim_bad_side '100' --min_split_read_size '1000' -o 'out.fasta']
galaxy.jobs.runners DEBUG 2025-01-13 01:21:47,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (132) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/132/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/132/galaxy_132.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/132/working/out."*"" ] ; then cp "/galaxy/server/database/jobs_directory/000/132/working/out."*"" "/galaxy/server/database/objects/9/c/1/dataset_9c1f56dd-11a0-4a68-ad37-4cb62af0c0a8.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:21:47,805 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:21:47,806 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/0.2.4+galaxy0: porechop:0.2.4
galaxy.tool_util.deps.containers INFO 2025-01-13 01:21:47,825 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/porechop:0.2.4--py312hf731ba3_9,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:21:59,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 132: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:22:04,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 132 finished
galaxy.model.metadata DEBUG 2025-01-13 01:22:04,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 135
galaxy.util WARNING 2025-01-13 01:22:04,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/c/1/dataset_9c1f56dd-11a0-4a68-ad37-4cb62af0c0a8.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/c/1/dataset_9c1f56dd-11a0-4a68-ad37-4cb62af0c0a8.dat'
galaxy.jobs INFO 2025-01-13 01:22:04,330 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 132 in /galaxy/server/database/jobs_directory/000/132
galaxy.jobs DEBUG 2025-01-13 01:22:04,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 132 executed (137.066 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:22:05,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 133
tpv.core.entities DEBUG 2025-01-13 01:22:05,878 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:22:05,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:22:05,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:22:05,901 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:22:05,922 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Working directory for job is: /galaxy/server/database/jobs_directory/000/133
galaxy.jobs.runners DEBUG 2025-01-13 01:22:05,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [133] queued (46.076 ms)
galaxy.jobs.handler INFO 2025-01-13 01:22:05,934 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (133) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:22:05,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 133
galaxy.jobs DEBUG 2025-01-13 01:22:06,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [133] prepared (107.581 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:22:06,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/133/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/133/registry.xml' '/galaxy/server/database/jobs_directory/000/133/upload_params.json' '136:/galaxy/server/database/objects/9/5/7/dataset_957794ad-78f0-4198-8a39-498bd8e6ea92_files:/galaxy/server/database/objects/9/5/7/dataset_957794ad-78f0-4198-8a39-498bd8e6ea92.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:22:06,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (133) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/133/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/133/galaxy_133.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:22:14,759 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 133: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:22:19,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 133 finished
galaxy.model.metadata DEBUG 2025-01-13 01:22:19,768 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 136
galaxy.jobs INFO 2025-01-13 01:22:19,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 133 in /galaxy/server/database/jobs_directory/000/133
galaxy.jobs DEBUG 2025-01-13 01:22:19,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 133 executed (138.217 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:22:20,256 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 134
tpv.core.entities DEBUG 2025-01-13 01:22:20,297 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/.*, abstract=False, cores=1, mem=40, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:22:20,298 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:22:20,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:22:20,323 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:22:20,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Working directory for job is: /galaxy/server/database/jobs_directory/000/134
galaxy.jobs.runners DEBUG 2025-01-13 01:22:20,354 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [134] queued (50.358 ms)
galaxy.jobs.handler INFO 2025-01-13 01:22:20,358 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (134) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:22:20,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 134
galaxy.jobs DEBUG 2025-01-13 01:22:20,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [134] prepared (66.156 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:22:20,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:22:20,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/0.2.4+galaxy0: porechop:0.2.4
galaxy.tool_util.deps.containers INFO 2025-01-13 01:22:20,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/porechop:0.2.4--py312hf731ba3_9,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:22:20,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/134/tool_script.sh] for tool command [porechop --version > /galaxy/server/database/jobs_directory/000/134/outputs/COMMAND_VERSION 2>&1;
porechop -i '/galaxy/server/database/objects/9/5/7/dataset_957794ad-78f0-4198-8a39-498bd8e6ea92.dat' --format 'fastq' --barcode_threshold '75.0' --barcode_diff '5.0'   --adapter_threshold '90.0' --check_reads '10000' --scoring_scheme '3,-6,-5,-2' --end_size '150' --min_trim_size '4' --extra_end_trim '2' --end_threshold '75.0'   --middle_threshold '85.0' --extra_middle_trim_good_side '10' --extra_middle_trim_bad_side '100' --min_split_read_size '1000' -o 'out.fastq']
galaxy.jobs.runners DEBUG 2025-01-13 01:22:20,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (134) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/134/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/134/galaxy_134.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/134/working/out."*"" ] ; then cp "/galaxy/server/database/jobs_directory/000/134/working/out."*"" "/galaxy/server/database/objects/3/2/9/dataset_329f9ff7-fa63-4f1a-b158-a28c66fc438a.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:22:20,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:22:20,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/0.2.4+galaxy0: porechop:0.2.4
galaxy.tool_util.deps.containers INFO 2025-01-13 01:22:20,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/porechop:0.2.4--py312hf731ba3_9,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:22:25,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 134: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:22:30,841 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 134 finished
galaxy.model.metadata DEBUG 2025-01-13 01:22:30,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 137
galaxy.util WARNING 2025-01-13 01:22:30,913 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/2/9/dataset_329f9ff7-fa63-4f1a-b158-a28c66fc438a.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/2/9/dataset_329f9ff7-fa63-4f1a-b158-a28c66fc438a.dat'
galaxy.jobs INFO 2025-01-13 01:22:30,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 134 in /galaxy/server/database/jobs_directory/000/134
galaxy.jobs DEBUG 2025-01-13 01:22:30,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 134 executed (129.389 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:22:32,628 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 135
tpv.core.entities DEBUG 2025-01-13 01:22:32,666 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:22:32,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:22:32,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:22:32,690 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:22:32,710 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Working directory for job is: /galaxy/server/database/jobs_directory/000/135
galaxy.jobs.runners DEBUG 2025-01-13 01:22:32,718 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [135] queued (45.382 ms)
galaxy.jobs.handler INFO 2025-01-13 01:22:32,721 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (135) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:22:32,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 135
galaxy.jobs DEBUG 2025-01-13 01:22:32,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [135] prepared (95.793 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:22:32,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/135/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/135/registry.xml' '/galaxy/server/database/jobs_directory/000/135/upload_params.json' '138:/galaxy/server/database/objects/5/e/e/dataset_5ee91789-a836-41e4-9865-5ca2c9144526_files:/galaxy/server/database/objects/5/e/e/dataset_5ee91789-a836-41e4-9865-5ca2c9144526.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:22:32,878 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (135) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/135/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/135/galaxy_135.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:22:41,063 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 135: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:22:45,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 135 finished
galaxy.model.metadata DEBUG 2025-01-13 01:22:45,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 138
galaxy.jobs INFO 2025-01-13 01:22:45,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 135 in /galaxy/server/database/jobs_directory/000/135
galaxy.jobs DEBUG 2025-01-13 01:22:45,991 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 135 executed (132.625 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:22:47,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 136
tpv.core.entities DEBUG 2025-01-13 01:22:47,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/.*, abstract=False, cores=1, mem=40, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:22:47,091 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:22:47,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:22:47,110 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:22:47,126 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Working directory for job is: /galaxy/server/database/jobs_directory/000/136
galaxy.jobs.runners DEBUG 2025-01-13 01:22:47,135 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [136] queued (39.390 ms)
galaxy.jobs.handler INFO 2025-01-13 01:22:47,139 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (136) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:22:47,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 136
galaxy.jobs DEBUG 2025-01-13 01:22:47,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [136] prepared (61.594 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:22:47,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:22:47,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/0.2.4+galaxy0: porechop:0.2.4
galaxy.tool_util.deps.containers INFO 2025-01-13 01:22:47,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/porechop:0.2.4--py312hf731ba3_9,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:22:47,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/136/tool_script.sh] for tool command [porechop --version > /galaxy/server/database/jobs_directory/000/136/outputs/COMMAND_VERSION 2>&1;
porechop -i '/galaxy/server/database/objects/5/e/e/dataset_5ee91789-a836-41e4-9865-5ca2c9144526.dat' --format 'fastq' --barcode_threshold '75.0' --barcode_diff '5.0'   --adapter_threshold '90.0' --check_reads '10000' --scoring_scheme '3,-6,-5,-2' --end_size '150' --min_trim_size '4' --extra_end_trim '2' --end_threshold '75.0'   --middle_threshold '85.0' --extra_middle_trim_good_side '10' --extra_middle_trim_bad_side '100' --min_split_read_size '1000' -o 'out.fastq']
galaxy.jobs.runners DEBUG 2025-01-13 01:22:47,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (136) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/136/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/136/galaxy_136.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/136/working/out."*"" ] ; then cp "/galaxy/server/database/jobs_directory/000/136/working/out."*"" "/galaxy/server/database/objects/b/a/6/dataset_ba66e335-7aa7-4d56-bd86-32e976d45e8b.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:22:47,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:22:47,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/0.2.4+galaxy0: porechop:0.2.4
galaxy.tool_util.deps.containers INFO 2025-01-13 01:22:47,488 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/porechop:0.2.4--py312hf731ba3_9,type=docker]]
galaxy.jobs.runners.kubernetes ERROR 2025-01-13 01:22:52,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://10.100.128.1:443/apis/batch/v1/namespaces/prod-25-01-13-00-39-1/jobs/gxy-swwkb

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 817, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 824, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 107, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-swwkb": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:22:52,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (136/gxy-swwkb) Terminated at user's request
galaxy.util WARNING 2025-01-13 01:22:52,227 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/a/6/dataset_ba66e335-7aa7-4d56-bd86-32e976d45e8b.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/a/6/dataset_ba66e335-7aa7-4d56-bd86-32e976d45e8b.dat'
galaxy.jobs.handler DEBUG 2025-01-13 01:22:54,284 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 137
tpv.core.entities DEBUG 2025-01-13 01:22:54,321 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:22:54,321 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:22:54,327 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:22:54,345 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:22:54,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Working directory for job is: /galaxy/server/database/jobs_directory/000/137
galaxy.jobs.runners DEBUG 2025-01-13 01:22:54,372 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [137] queued (45.176 ms)
galaxy.jobs.handler INFO 2025-01-13 01:22:54,376 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (137) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:22:54,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 137
galaxy.jobs DEBUG 2025-01-13 01:22:54,493 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [137] prepared (91.626 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:22:54,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/137/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/137/registry.xml' '/galaxy/server/database/jobs_directory/000/137/upload_params.json' '140:/galaxy/server/database/objects/b/9/5/dataset_b95e8466-34c2-4d12-a852-8c90e87b8a18_files:/galaxy/server/database/objects/b/9/5/dataset_b95e8466-34c2-4d12-a852-8c90e87b8a18.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:22:54,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (137) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/137/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/137/galaxy_137.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:23:03,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 137: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:23:08,379 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 137 finished
galaxy.model.metadata DEBUG 2025-01-13 01:23:08,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 140
galaxy.jobs INFO 2025-01-13 01:23:08,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 137 in /galaxy/server/database/jobs_directory/000/137
galaxy.jobs DEBUG 2025-01-13 01:23:08,532 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 137 executed (125.816 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:23:09,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 138
tpv.core.entities DEBUG 2025-01-13 01:23:09,756 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/.*, abstract=False, cores=1, mem=40, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:23:09,756 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:23:09,761 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:23:09,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:23:09,799 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Working directory for job is: /galaxy/server/database/jobs_directory/000/138
galaxy.jobs.runners DEBUG 2025-01-13 01:23:09,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [138] queued (45.201 ms)
galaxy.jobs.handler INFO 2025-01-13 01:23:09,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (138) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:23:09,816 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 138
galaxy.jobs DEBUG 2025-01-13 01:23:09,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [138] prepared (55.192 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:23:09,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:23:09,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/0.2.4+galaxy0: porechop:0.2.4
galaxy.tool_util.deps.containers INFO 2025-01-13 01:23:09,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/porechop:0.2.4--py312hf731ba3_9,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:23:09,941 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/138/tool_script.sh] for tool command [porechop --version > /galaxy/server/database/jobs_directory/000/138/outputs/COMMAND_VERSION 2>&1;
porechop -i '/galaxy/server/database/objects/b/9/5/dataset_b95e8466-34c2-4d12-a852-8c90e87b8a18.dat' --format 'fasta.gz' --barcode_threshold '75.0' --barcode_diff '5.0'   --adapter_threshold '90.0' --check_reads '10000' --scoring_scheme '3,-6,-5,-2' --end_size '150' --min_trim_size '4' --extra_end_trim '2' --end_threshold '75.0'   --middle_threshold '85.0' --extra_middle_trim_good_side '10' --extra_middle_trim_bad_side '100' --min_split_read_size '1000' -o 'out.fasta.gz']
galaxy.jobs.runners DEBUG 2025-01-13 01:23:09,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (138) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/138/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/138/galaxy_138.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/138/working/out."*"" ] ; then cp "/galaxy/server/database/jobs_directory/000/138/working/out."*"" "/galaxy/server/database/objects/f/0/3/dataset_f031a4c7-363a-431f-a084-d9add912e0a9.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:23:09,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:23:09,972 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/0.2.4+galaxy0: porechop:0.2.4
galaxy.tool_util.deps.containers INFO 2025-01-13 01:23:09,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/porechop:0.2.4--py312hf731ba3_9,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:23:14,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 138: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:23:19,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 138 finished
galaxy.model.metadata DEBUG 2025-01-13 01:23:19,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 141
galaxy.util WARNING 2025-01-13 01:23:19,640 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/0/3/dataset_f031a4c7-363a-431f-a084-d9add912e0a9.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/0/3/dataset_f031a4c7-363a-431f-a084-d9add912e0a9.dat'
galaxy.jobs INFO 2025-01-13 01:23:19,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 138 in /galaxy/server/database/jobs_directory/000/138
galaxy.jobs DEBUG 2025-01-13 01:23:19,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 138 executed (131.108 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:23:21,051 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 139
tpv.core.entities DEBUG 2025-01-13 01:23:21,088 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:23:21,089 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:23:21,095 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:23:21,112 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:23:21,130 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Working directory for job is: /galaxy/server/database/jobs_directory/000/139
galaxy.jobs.runners DEBUG 2025-01-13 01:23:21,138 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [139] queued (43.630 ms)
galaxy.jobs.handler INFO 2025-01-13 01:23:21,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (139) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:23:21,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 139
galaxy.jobs DEBUG 2025-01-13 01:23:21,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [139] prepared (101.831 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:23:21,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/139/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/139/registry.xml' '/galaxy/server/database/jobs_directory/000/139/upload_params.json' '142:/galaxy/server/database/objects/9/4/5/dataset_94506754-f292-4413-ab5d-47b96f3e747d_files:/galaxy/server/database/objects/9/4/5/dataset_94506754-f292-4413-ab5d-47b96f3e747d.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:23:21,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (139) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/139/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/139/galaxy_139.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:23:29,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 139: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:23:34,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 139 finished
galaxy.model.metadata DEBUG 2025-01-13 01:23:34,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 142
galaxy.jobs INFO 2025-01-13 01:23:34,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 139 in /galaxy/server/database/jobs_directory/000/139
galaxy.jobs DEBUG 2025-01-13 01:23:34,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 139 executed (137.757 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:23:35,449 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 140
tpv.core.entities DEBUG 2025-01-13 01:23:35,490 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/.*, abstract=False, cores=1, mem=40, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:23:35,491 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:23:35,496 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:23:35,512 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:23:35,531 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Working directory for job is: /galaxy/server/database/jobs_directory/000/140
galaxy.jobs.runners DEBUG 2025-01-13 01:23:35,542 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [140] queued (45.667 ms)
galaxy.jobs.handler INFO 2025-01-13 01:23:35,546 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (140) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:23:35,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 140
galaxy.jobs DEBUG 2025-01-13 01:23:35,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [140] prepared (63.757 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:23:35,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:23:35,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/0.2.4+galaxy0: porechop:0.2.4
galaxy.tool_util.deps.containers INFO 2025-01-13 01:23:35,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/porechop:0.2.4--py312hf731ba3_9,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:23:35,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/140/tool_script.sh] for tool command [porechop --version > /galaxy/server/database/jobs_directory/000/140/outputs/COMMAND_VERSION 2>&1;
porechop -i '/galaxy/server/database/objects/9/4/5/dataset_94506754-f292-4413-ab5d-47b96f3e747d.dat' --format 'fastq.gz' --barcode_threshold '75.0' --barcode_diff '5.0'   --adapter_threshold '90.0' --check_reads '10000' --scoring_scheme '3,-6,-5,-2' --end_size '150' --min_trim_size '4' --extra_end_trim '2' --end_threshold '75.0'   --middle_threshold '85.0' --extra_middle_trim_good_side '10' --extra_middle_trim_bad_side '100' --min_split_read_size '1000' -o 'out.fastq.gz']
galaxy.jobs.runners DEBUG 2025-01-13 01:23:35,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (140) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/140/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/140/galaxy_140.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/140/working/out."*"" ] ; then cp "/galaxy/server/database/jobs_directory/000/140/working/out."*"" "/galaxy/server/database/objects/2/c/4/dataset_2c4e8e5d-c9d9-4051-a7ed-eb40479d5c28.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:23:35,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:23:35,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/0.2.4+galaxy0: porechop:0.2.4
galaxy.tool_util.deps.containers INFO 2025-01-13 01:23:35,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/porechop:0.2.4--py312hf731ba3_9,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:23:41,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 140: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:23:45,861 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 140 finished
galaxy.model.metadata DEBUG 2025-01-13 01:23:45,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 143
galaxy.util WARNING 2025-01-13 01:23:45,927 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/c/4/dataset_2c4e8e5d-c9d9-4051-a7ed-eb40479d5c28.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/c/4/dataset_2c4e8e5d-c9d9-4051-a7ed-eb40479d5c28.dat'
galaxy.jobs INFO 2025-01-13 01:23:45,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 140 in /galaxy/server/database/jobs_directory/000/140
galaxy.jobs DEBUG 2025-01-13 01:23:46,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 140 executed (125.966 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:23:47,805 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 141
tpv.core.entities DEBUG 2025-01-13 01:23:47,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:23:47,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:23:47,846 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:23:47,863 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:23:47,884 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Working directory for job is: /galaxy/server/database/jobs_directory/000/141
galaxy.jobs.runners DEBUG 2025-01-13 01:23:47,891 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [141] queued (44.297 ms)
galaxy.jobs.handler INFO 2025-01-13 01:23:47,895 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (141) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:23:47,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 141
galaxy.jobs DEBUG 2025-01-13 01:23:48,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [141] prepared (101.342 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:23:48,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/141/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/141/registry.xml' '/galaxy/server/database/jobs_directory/000/141/upload_params.json' '144:/galaxy/server/database/objects/b/3/b/dataset_b3b19eb8-004b-438c-8105-99835adc0ca5_files:/galaxy/server/database/objects/b/3/b/dataset_b3b19eb8-004b-438c-8105-99835adc0ca5.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:23:48,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (141) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/141/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/141/galaxy_141.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:23:56,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 141: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:24:01,126 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 141 finished
galaxy.model.metadata DEBUG 2025-01-13 01:24:01,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 144
galaxy.jobs INFO 2025-01-13 01:24:01,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 141 in /galaxy/server/database/jobs_directory/000/141
galaxy.jobs DEBUG 2025-01-13 01:24:01,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 141 executed (149.647 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:24:02,209 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 142
tpv.core.entities DEBUG 2025-01-13 01:24:02,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/.*, abstract=False, cores=1, mem=40, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:24:02,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:24:02,253 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:24:02,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:24:02,292 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Working directory for job is: /galaxy/server/database/jobs_directory/000/142
galaxy.jobs.runners DEBUG 2025-01-13 01:24:02,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [142] queued (48.760 ms)
galaxy.jobs.handler INFO 2025-01-13 01:24:02,306 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (142) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:24:02,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 142
galaxy.jobs DEBUG 2025-01-13 01:24:02,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [142] prepared (63.720 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:24:02,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:24:02,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/0.2.4+galaxy0: porechop:0.2.4
galaxy.tool_util.deps.containers INFO 2025-01-13 01:24:02,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/porechop:0.2.4--py312hf731ba3_9,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:24:02,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/142/tool_script.sh] for tool command [porechop --version > /galaxy/server/database/jobs_directory/000/142/outputs/COMMAND_VERSION 2>&1;
porechop -i '/galaxy/server/database/objects/b/3/b/dataset_b3b19eb8-004b-438c-8105-99835adc0ca5.dat' --format 'fasta' --barcode_threshold '70.0' --barcode_diff '4.0' --require_two_barcodes --discard_unassigned --adapter_threshold '90.0' --check_reads '10000' --scoring_scheme '3,-6,-5,-2' --end_size '100' --min_trim_size '2' --extra_end_trim '1' --end_threshold '80.0'  --discard_middle --middle_threshold '90.0' --extra_middle_trim_good_side '10' --extra_middle_trim_bad_side '100' --min_split_read_size '1500' -o 'out.fasta']
galaxy.jobs.runners DEBUG 2025-01-13 01:24:02,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (142) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/142/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/142/galaxy_142.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/142/working/out."*"" ] ; then cp "/galaxy/server/database/jobs_directory/000/142/working/out."*"" "/galaxy/server/database/objects/0/b/8/dataset_0b8ed3af-5568-40ff-b72c-a3b12ddebaa7.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:24:02,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:24:02,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/porechop/porechop/0.2.4+galaxy0: porechop:0.2.4
galaxy.tool_util.deps.containers INFO 2025-01-13 01:24:02,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/porechop:0.2.4--py312hf731ba3_9,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:24:08,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 142: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:24:13,127 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 142 finished
galaxy.model.metadata DEBUG 2025-01-13 01:24:13,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 145
galaxy.util WARNING 2025-01-13 01:24:13,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/b/8/dataset_0b8ed3af-5568-40ff-b72c-a3b12ddebaa7.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/b/8/dataset_0b8ed3af-5568-40ff-b72c-a3b12ddebaa7.dat'
galaxy.jobs INFO 2025-01-13 01:24:13,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 142 in /galaxy/server/database/jobs_directory/000/142
galaxy.jobs DEBUG 2025-01-13 01:24:13,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 142 executed (125.682 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:24:16,620 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 144, 143
tpv.core.entities DEBUG 2025-01-13 01:24:16,661 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:24:16,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:24:16,667 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:24:16,686 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:24:16,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Working directory for job is: /galaxy/server/database/jobs_directory/000/143
galaxy.jobs.runners DEBUG 2025-01-13 01:24:16,714 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [143] queued (46.156 ms)
galaxy.jobs.handler INFO 2025-01-13 01:24:16,740 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (143) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:24:16,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 143
tpv.core.entities DEBUG 2025-01-13 01:24:16,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:24:16,759 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:24:16,766 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:24:16,787 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:24:16,852 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Working directory for job is: /galaxy/server/database/jobs_directory/000/144
galaxy.jobs.runners DEBUG 2025-01-13 01:24:16,861 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [144] queued (94.363 ms)
galaxy.jobs.handler INFO 2025-01-13 01:24:16,865 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (144) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:24:16,873 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 144
galaxy.jobs DEBUG 2025-01-13 01:24:16,912 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [143] prepared (146.191 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:24:16,940 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/143/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/143/registry.xml' '/galaxy/server/database/jobs_directory/000/143/upload_params.json' '146:/galaxy/server/database/objects/1/2/6/dataset_126aaf33-5aad-4118-9def-a04d16b842ad_files:/galaxy/server/database/objects/1/2/6/dataset_126aaf33-5aad-4118-9def-a04d16b842ad.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:24:16,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (143) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/143/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/143/galaxy_143.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:24:17,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [144] prepared (105.510 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:24:17,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/144/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/144/registry.xml' '/galaxy/server/database/jobs_directory/000/144/upload_params.json' '147:/galaxy/server/database/objects/5/2/8/dataset_528d87cf-4b45-4377-938b-6f7ba87fc7e2_files:/galaxy/server/database/objects/5/2/8/dataset_528d87cf-4b45-4377-938b-6f7ba87fc7e2.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:24:17,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (144) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/144/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/144/galaxy_144.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:24:25,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 143: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:24:25,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 144: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:24:31,033 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 143 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:24:31,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 144 finished
galaxy.model.metadata DEBUG 2025-01-13 01:24:31,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 146
galaxy.model.metadata DEBUG 2025-01-13 01:24:31,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 147
galaxy.jobs INFO 2025-01-13 01:24:31,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 144 in /galaxy/server/database/jobs_directory/000/144
galaxy.jobs INFO 2025-01-13 01:24:31,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 143 in /galaxy/server/database/jobs_directory/000/143
galaxy.jobs DEBUG 2025-01-13 01:24:31,260 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 144 executed (164.451 ms)
galaxy.jobs DEBUG 2025-01-13 01:24:31,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 143 executed (206.423 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:24:32,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 145
tpv.core.entities DEBUG 2025-01-13 01:24:32,308 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_gc_bias/deeptools_compute_gc_bias/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:24:32,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:24:32,314 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:24:32,333 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:24:32,352 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Working directory for job is: /galaxy/server/database/jobs_directory/000/145
galaxy.jobs.runners DEBUG 2025-01-13 01:24:32,362 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [145] queued (47.299 ms)
galaxy.jobs.handler INFO 2025-01-13 01:24:32,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (145) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:24:32,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 145
galaxy.jobs DEBUG 2025-01-13 01:24:32,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [145] prepared (77.785 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:24:32,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:24:32,472 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_gc_bias/deeptools_compute_gc_bias/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-13 01:24:32,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:24:32,673 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/145/tool_script.sh] for tool command [computeGCBias --version > /galaxy/server/database/jobs_directory/000/145/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/1/2/6/dataset_126aaf33-5aad-4118-9def-a04d16b842ad.dat' local_bamInput.bam && ln -s '/galaxy/server/database/objects/_metadata_files/9/4/a/metadata_94ae6b01-af57-4e6f-b82e-59ac12561fe1.dat' local_bamInput.bam.bai &&  computeGCBias --numberOfProcessors "${GALAXY_SLOTS:-4}" --bamfile local_bamInput.bam --GCbiasFrequenciesFile /galaxy/server/database/objects/f/9/8/dataset_f985c683-a9c2-4c7b-8e8b-abf492a234b2.dat  --fragmentLength 300   --genome /galaxy/server/database/objects/5/2/8/dataset_528d87cf-4b45-4377-938b-6f7ba87fc7e2.dat   --effectiveGenomeSize 10050  --region 'chr2L'  --sampleSize '10' --regionSize '1']
galaxy.jobs.runners DEBUG 2025-01-13 01:24:32,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (145) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/145/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/145/galaxy_145.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:24:32,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:24:32,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_gc_bias/deeptools_compute_gc_bias/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-13 01:24:32,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:24:56,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 145: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:25:01,025 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 145 finished
galaxy.model.metadata DEBUG 2025-01-13 01:25:01,098 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 148
galaxy.jobs INFO 2025-01-13 01:25:01,143 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 145 in /galaxy/server/database/jobs_directory/000/145
galaxy.objectstore CRITICAL 2025-01-13 01:25:01,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-13 01:25:01,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 145 executed (136.840 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:25:04,093 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 146, 147
tpv.core.entities DEBUG 2025-01-13 01:25:04,126 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:25:04,127 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:25:04,132 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:25:04,148 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:25:04,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Working directory for job is: /galaxy/server/database/jobs_directory/000/146
galaxy.jobs.runners DEBUG 2025-01-13 01:25:04,176 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [146] queued (43.826 ms)
galaxy.jobs.handler INFO 2025-01-13 01:25:04,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (146) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:25:04,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 146
tpv.core.entities DEBUG 2025-01-13 01:25:04,199 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:25:04,199 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:25:04,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:25:04,226 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:25:04,259 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Working directory for job is: /galaxy/server/database/jobs_directory/000/147
galaxy.jobs.runners DEBUG 2025-01-13 01:25:04,269 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [147] queued (62.302 ms)
galaxy.jobs.handler INFO 2025-01-13 01:25:04,274 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (147) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:25:04,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 147
galaxy.jobs DEBUG 2025-01-13 01:25:04,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [146] prepared (112.356 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:25:04,353 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/146/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/146/registry.xml' '/galaxy/server/database/jobs_directory/000/146/upload_params.json' '149:/galaxy/server/database/objects/8/0/6/dataset_806de7cb-009a-4850-a813-334725637b62_files:/galaxy/server/database/objects/8/0/6/dataset_806de7cb-009a-4850-a813-334725637b62.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:25:04,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (146) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/146/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/146/galaxy_146.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:25:04,405 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [147] prepared (103.635 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:25:04,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/147/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/147/registry.xml' '/galaxy/server/database/jobs_directory/000/147/upload_params.json' '150:/galaxy/server/database/objects/7/0/b/dataset_70bcee59-d239-4bd4-9659-b4c52add19f6_files:/galaxy/server/database/objects/7/0/b/dataset_70bcee59-d239-4bd4-9659-b4c52add19f6.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:25:04,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (147) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/147/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/147/galaxy_147.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:25:13,637 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 146: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:25:13,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 147: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:25:18,888 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 146 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:25:18,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 147 finished
galaxy.model.metadata DEBUG 2025-01-13 01:25:18,955 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 149
galaxy.model.metadata DEBUG 2025-01-13 01:25:18,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 150
galaxy.jobs INFO 2025-01-13 01:25:19,004 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 146 in /galaxy/server/database/jobs_directory/000/146
galaxy.jobs INFO 2025-01-13 01:25:19,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 147 in /galaxy/server/database/jobs_directory/000/147
galaxy.jobs DEBUG 2025-01-13 01:25:19,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 146 executed (149.831 ms)
galaxy.jobs DEBUG 2025-01-13 01:25:19,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 147 executed (144.499 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:25:19,795 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 148
tpv.core.entities DEBUG 2025-01-13 01:25:19,839 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:25:19,840 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:25:19,845 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:25:19,860 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:25:19,880 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Working directory for job is: /galaxy/server/database/jobs_directory/000/148
galaxy.jobs.runners DEBUG 2025-01-13 01:25:19,890 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [148] queued (44.421 ms)
galaxy.jobs.handler INFO 2025-01-13 01:25:19,896 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (148) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:25:19,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 148
galaxy.jobs DEBUG 2025-01-13 01:25:20,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [148] prepared (131.766 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:25:20,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:25:20,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/0.7.18: mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e
galaxy.tool_util.deps.containers INFO 2025-01-13 01:25:20,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:25:20,310 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/148/tool_script.sh] for tool command [set -o | grep -q pipefail && set -o pipefail;  ln -s '/galaxy/server/database/objects/8/0/6/dataset_806de7cb-009a-4850-a813-334725637b62.dat' 'localref.fa' && bwa index 'localref.fa' &&                 bwa aln -t "${GALAXY_SLOTS:-1}"     'localref.fa' '/galaxy/server/database/objects/7/0/b/dataset_70bcee59-d239-4bd4-9659-b4c52add19f6.dat' > first.sai &&  bwa samse    'localref.fa' first.sai '/galaxy/server/database/objects/7/0/b/dataset_70bcee59-d239-4bd4-9659-b4c52add19f6.dat'    | samtools sort -@${GALAXY_SLOTS:-2} -T "${TMPDIR:-.}" -O bam -o '/galaxy/server/database/objects/6/9/1/dataset_691071cd-2000-4a17-89ed-b3b0f038b3b6.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:25:20,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (148) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/148/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/148/galaxy_148.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:25:20,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:25:20,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/0.7.18: mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e
galaxy.tool_util.deps.containers INFO 2025-01-13 01:25:20,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:25:30,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 148: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:25:35,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 148 finished
galaxy.model.metadata DEBUG 2025-01-13 01:25:35,879 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 151
galaxy.jobs INFO 2025-01-13 01:25:35,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 148 in /galaxy/server/database/jobs_directory/000/148
galaxy.jobs DEBUG 2025-01-13 01:25:36,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 148 executed (183.383 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:25:37,275 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 149
tpv.core.entities DEBUG 2025-01-13 01:25:37,311 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:25:37,312 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:25:37,317 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:25:37,335 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:25:37,353 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Working directory for job is: /galaxy/server/database/jobs_directory/000/149
galaxy.jobs.runners DEBUG 2025-01-13 01:25:37,361 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [149] queued (43.073 ms)
galaxy.jobs.handler INFO 2025-01-13 01:25:37,365 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (149) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:25:37,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 149
galaxy.jobs DEBUG 2025-01-13 01:25:37,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [149] prepared (101.140 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:25:37,515 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/149/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/149/registry.xml' '/galaxy/server/database/jobs_directory/000/149/upload_params.json' '152:/galaxy/server/database/objects/3/e/f/dataset_3ef2853b-fc56-442a-a6a2-1f0387ac70fe_files:/galaxy/server/database/objects/3/e/f/dataset_3ef2853b-fc56-442a-a6a2-1f0387ac70fe.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:25:37,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (149) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/149/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/149/galaxy_149.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:25:38,368 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 151, 150
tpv.core.entities DEBUG 2025-01-13 01:25:38,407 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:25:38,408 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:25:38,413 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:25:38,433 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:25:38,455 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Working directory for job is: /galaxy/server/database/jobs_directory/000/150
galaxy.jobs.runners DEBUG 2025-01-13 01:25:38,463 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [150] queued (49.703 ms)
galaxy.jobs.handler INFO 2025-01-13 01:25:38,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (150) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:25:38,473 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 150
tpv.core.entities DEBUG 2025-01-13 01:25:38,489 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:25:38,490 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:25:38,497 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:25:38,520 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:25:38,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Working directory for job is: /galaxy/server/database/jobs_directory/000/151
galaxy.jobs.runners DEBUG 2025-01-13 01:25:38,567 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [151] queued (69.198 ms)
galaxy.jobs.handler INFO 2025-01-13 01:25:38,571 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (151) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:25:38,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 151
galaxy.jobs DEBUG 2025-01-13 01:25:38,613 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [150] prepared (114.112 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:25:38,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/150/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/150/registry.xml' '/galaxy/server/database/jobs_directory/000/150/upload_params.json' '153:/galaxy/server/database/objects/e/7/b/dataset_e7b846cc-0d0a-4111-9365-4ede92fa3d36_files:/galaxy/server/database/objects/e/7/b/dataset_e7b846cc-0d0a-4111-9365-4ede92fa3d36.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:25:38,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (150) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/150/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/150/galaxy_150.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:25:38,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [151] prepared (107.752 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:25:38,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/151/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/151/registry.xml' '/galaxy/server/database/jobs_directory/000/151/upload_params.json' '154:/galaxy/server/database/objects/8/2/0/dataset_82018e80-2ae9-4e24-b6c8-af183d0ca1cf_files:/galaxy/server/database/objects/8/2/0/dataset_82018e80-2ae9-4e24-b6c8-af183d0ca1cf.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:25:38,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (151) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/151/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/151/galaxy_151.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:25:47,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 149: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:25:47,524 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 150: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:25:47,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 151: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:25:54,993 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 149 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:25:55,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 150 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:25:55,069 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 151 finished
galaxy.model.metadata DEBUG 2025-01-13 01:25:55,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 152
galaxy.model.metadata DEBUG 2025-01-13 01:25:55,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 153
galaxy.model.metadata DEBUG 2025-01-13 01:25:55,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 154
galaxy.jobs INFO 2025-01-13 01:25:55,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 149 in /galaxy/server/database/jobs_directory/000/149
galaxy.jobs INFO 2025-01-13 01:25:55,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 150 in /galaxy/server/database/jobs_directory/000/150
galaxy.jobs INFO 2025-01-13 01:25:55,234 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 151 in /galaxy/server/database/jobs_directory/000/151
galaxy.jobs DEBUG 2025-01-13 01:25:55,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 149 executed (216.771 ms)
galaxy.jobs DEBUG 2025-01-13 01:25:55,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 150 executed (215.123 ms)
galaxy.jobs DEBUG 2025-01-13 01:25:55,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 151 executed (205.622 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:25:56,080 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 152
tpv.core.entities DEBUG 2025-01-13 01:25:56,123 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:25:56,124 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:25:56,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:25:56,144 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:25:56,163 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Working directory for job is: /galaxy/server/database/jobs_directory/000/152
galaxy.jobs.runners DEBUG 2025-01-13 01:25:56,173 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [152] queued (43.861 ms)
galaxy.jobs.handler INFO 2025-01-13 01:25:56,176 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (152) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:25:56,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 152
galaxy.jobs DEBUG 2025-01-13 01:25:56,276 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [152] prepared (76.296 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:25:56,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:25:56,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/0.7.18: mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e
galaxy.tool_util.deps.containers INFO 2025-01-13 01:25:56,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:25:56,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/152/tool_script.sh] for tool command [set -o | grep -q pipefail && set -o pipefail;  ln -s '/galaxy/server/database/objects/3/e/f/dataset_3ef2853b-fc56-442a-a6a2-1f0387ac70fe.dat' 'localref.fa' && bwa index 'localref.fa' &&                 bwa aln -t "${GALAXY_SLOTS:-1}"   'localref.fa' '/galaxy/server/database/objects/e/7/b/dataset_e7b846cc-0d0a-4111-9365-4ede92fa3d36.dat' > first.sai &&  bwa aln -t "${GALAXY_SLOTS:-1}"   'localref.fa' '/galaxy/server/database/objects/8/2/0/dataset_82018e80-2ae9-4e24-b6c8-af183d0ca1cf.dat' > second.sai &&  bwa sampe   'localref.fa' first.sai second.sai '/galaxy/server/database/objects/e/7/b/dataset_e7b846cc-0d0a-4111-9365-4ede92fa3d36.dat' '/galaxy/server/database/objects/8/2/0/dataset_82018e80-2ae9-4e24-b6c8-af183d0ca1cf.dat'    | samtools sort -@${GALAXY_SLOTS:-2} -T "${TMPDIR:-.}" -O bam -o '/galaxy/server/database/objects/3/b/f/dataset_3bf331ca-a514-4188-862c-12de3c0d06a3.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:25:56,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (152) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/152/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/152/galaxy_152.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:25:56,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:25:56,355 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/0.7.18: mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e
galaxy.tool_util.deps.containers INFO 2025-01-13 01:25:56,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:26:02,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 152: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:26:07,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 152 finished
galaxy.model.metadata DEBUG 2025-01-13 01:26:07,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 155
galaxy.jobs INFO 2025-01-13 01:26:07,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 152 in /galaxy/server/database/jobs_directory/000/152
galaxy.jobs DEBUG 2025-01-13 01:26:07,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 152 executed (181.511 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:26:09,461 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 153, 154
tpv.core.entities DEBUG 2025-01-13 01:26:09,496 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:26:09,496 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:26:09,503 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:26:09,521 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:26:09,539 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Working directory for job is: /galaxy/server/database/jobs_directory/000/153
galaxy.jobs.runners DEBUG 2025-01-13 01:26:09,547 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [153] queued (43.571 ms)
galaxy.jobs.handler INFO 2025-01-13 01:26:09,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (153) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:26:09,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 153
tpv.core.entities DEBUG 2025-01-13 01:26:09,571 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:26:09,572 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:26:09,579 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:26:09,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:26:09,632 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Working directory for job is: /galaxy/server/database/jobs_directory/000/154
galaxy.jobs.runners DEBUG 2025-01-13 01:26:09,641 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [154] queued (62.280 ms)
galaxy.jobs.handler INFO 2025-01-13 01:26:09,646 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (154) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:26:09,651 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 154
galaxy.jobs DEBUG 2025-01-13 01:26:09,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [153] prepared (108.123 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:26:09,716 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/153/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/153/registry.xml' '/galaxy/server/database/jobs_directory/000/153/upload_params.json' '156:/galaxy/server/database/objects/0/2/b/dataset_02b9a70b-67dc-47c6-9d95-5536238ad208_files:/galaxy/server/database/objects/0/2/b/dataset_02b9a70b-67dc-47c6-9d95-5536238ad208.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:26:09,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (153) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/153/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/153/galaxy_153.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:26:09,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [154] prepared (115.636 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:26:09,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/154/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/154/registry.xml' '/galaxy/server/database/jobs_directory/000/154/upload_params.json' '157:/galaxy/server/database/objects/b/6/1/dataset_b61a5e70-374a-43a1-ac0d-72c0be24a8e3_files:/galaxy/server/database/objects/b/6/1/dataset_b61a5e70-374a-43a1-ac0d-72c0be24a8e3.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:26:09,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (154) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/154/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/154/galaxy_154.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:26:10,648 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 155
tpv.core.entities DEBUG 2025-01-13 01:26:10,684 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:26:10,684 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:26:10,688 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:26:10,706 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:26:10,727 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Working directory for job is: /galaxy/server/database/jobs_directory/000/155
galaxy.jobs.runners DEBUG 2025-01-13 01:26:10,734 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [155] queued (45.606 ms)
galaxy.jobs.handler INFO 2025-01-13 01:26:10,738 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (155) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:26:10,743 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 155
galaxy.jobs DEBUG 2025-01-13 01:26:10,850 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [155] prepared (85.258 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:26:10,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/155/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/155/registry.xml' '/galaxy/server/database/jobs_directory/000/155/upload_params.json' '158:/galaxy/server/database/objects/1/c/7/dataset_1c7ee952-8699-4dae-bb65-4d1316b08c69_files:/galaxy/server/database/objects/1/c/7/dataset_1c7ee952-8699-4dae-bb65-4d1316b08c69.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:26:10,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (155) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/155/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/155/galaxy_155.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:26:17,277 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 154: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:26:19,334 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 153: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:26:20,563 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 155: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:26:23,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 154 finished
galaxy.model.metadata DEBUG 2025-01-13 01:26:23,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 157
galaxy.jobs INFO 2025-01-13 01:26:23,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 154 in /galaxy/server/database/jobs_directory/000/154
galaxy.jobs DEBUG 2025-01-13 01:26:23,661 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 154 executed (188.474 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:26:25,402 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 153 finished
galaxy.model.metadata DEBUG 2025-01-13 01:26:25,467 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 156
galaxy.jobs INFO 2025-01-13 01:26:25,509 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 153 in /galaxy/server/database/jobs_directory/000/153
galaxy.jobs DEBUG 2025-01-13 01:26:25,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 153 executed (137.491 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:26:26,374 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 155 finished
galaxy.model.metadata DEBUG 2025-01-13 01:26:26,438 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 158
galaxy.jobs INFO 2025-01-13 01:26:26,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 155 in /galaxy/server/database/jobs_directory/000/155
galaxy.jobs DEBUG 2025-01-13 01:26:26,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 155 executed (127.939 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:26:27,173 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 156
tpv.core.entities DEBUG 2025-01-13 01:26:27,224 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:26:27,224 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:26:27,231 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:26:27,250 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:26:27,273 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Working directory for job is: /galaxy/server/database/jobs_directory/000/156
galaxy.jobs.runners DEBUG 2025-01-13 01:26:27,282 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [156] queued (50.961 ms)
galaxy.jobs.handler INFO 2025-01-13 01:26:27,286 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (156) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:26:27,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 156
galaxy.jobs DEBUG 2025-01-13 01:26:27,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [156] prepared (69.630 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:26:27,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:26:27,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/0.7.18: mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e
galaxy.tool_util.deps.containers INFO 2025-01-13 01:26:27,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:26:27,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/156/tool_script.sh] for tool command [set -o | grep -q pipefail && set -o pipefail;  ln -s '/galaxy/server/database/objects/0/2/b/dataset_02b9a70b-67dc-47c6-9d95-5536238ad208.dat' 'localref.fa' && bwa index 'localref.fa' &&                 bwa aln -t "${GALAXY_SLOTS:-1}"   'localref.fa' '/galaxy/server/database/objects/b/6/1/dataset_b61a5e70-374a-43a1-ac0d-72c0be24a8e3.dat' > first.sai &&  bwa aln -t "${GALAXY_SLOTS:-1}"   'localref.fa' '/galaxy/server/database/objects/1/c/7/dataset_1c7ee952-8699-4dae-bb65-4d1316b08c69.dat' > second.sai &&  bwa sampe   'localref.fa' first.sai second.sai '/galaxy/server/database/objects/b/6/1/dataset_b61a5e70-374a-43a1-ac0d-72c0be24a8e3.dat' '/galaxy/server/database/objects/1/c/7/dataset_1c7ee952-8699-4dae-bb65-4d1316b08c69.dat'    | samtools sort -@${GALAXY_SLOTS:-2} -T "${TMPDIR:-.}" -O bam -o '/galaxy/server/database/objects/e/8/e/dataset_e8e337ba-3b16-4d07-9560-15d148c536ed.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:26:27,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (156) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/156/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/156/galaxy_156.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:26:27,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:26:27,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/0.7.18: mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e
galaxy.tool_util.deps.containers INFO 2025-01-13 01:26:27,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:26:32,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 156: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:26:37,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 156 finished
galaxy.model.metadata DEBUG 2025-01-13 01:26:37,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 159
galaxy.jobs INFO 2025-01-13 01:26:37,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 156 in /galaxy/server/database/jobs_directory/000/156
galaxy.jobs DEBUG 2025-01-13 01:26:37,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 156 executed (176.156 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:26:38,656 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 157
tpv.core.entities DEBUG 2025-01-13 01:26:38,689 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:26:38,690 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:26:38,695 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:26:38,714 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:26:38,734 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Working directory for job is: /galaxy/server/database/jobs_directory/000/157
galaxy.jobs.runners DEBUG 2025-01-13 01:26:38,741 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [157] queued (45.823 ms)
galaxy.jobs.handler INFO 2025-01-13 01:26:38,745 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (157) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:26:38,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 157
galaxy.jobs DEBUG 2025-01-13 01:26:38,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [157] prepared (96.781 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:26:38,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/157/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/157/registry.xml' '/galaxy/server/database/jobs_directory/000/157/upload_params.json' '160:/galaxy/server/database/objects/2/0/f/dataset_20f9f94a-200b-4696-917d-e9960f8fa23a_files:/galaxy/server/database/objects/2/0/f/dataset_20f9f94a-200b-4696-917d-e9960f8fa23a.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:26:38,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (157) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/157/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/157/galaxy_157.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:26:39,747 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 158
tpv.core.entities DEBUG 2025-01-13 01:26:39,783 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:26:39,783 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:26:39,788 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:26:39,805 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:26:39,823 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Working directory for job is: /galaxy/server/database/jobs_directory/000/158
galaxy.jobs.runners DEBUG 2025-01-13 01:26:39,830 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [158] queued (42.320 ms)
galaxy.jobs.handler INFO 2025-01-13 01:26:39,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (158) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:26:39,839 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 158
galaxy.jobs DEBUG 2025-01-13 01:26:39,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [158] prepared (91.807 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:26:39,973 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/158/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/158/registry.xml' '/galaxy/server/database/jobs_directory/000/158/upload_params.json' '161:/galaxy/server/database/objects/7/b/d/dataset_7bde0858-74a3-4046-a1c7-eda86b0c3409_files:/galaxy/server/database/objects/7/b/d/dataset_7bde0858-74a3-4046-a1c7-eda86b0c3409.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:26:39,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (158) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/158/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/158/galaxy_158.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:26:47,948 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 157: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:26:49,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 158: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:26:52,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 157 finished
galaxy.model.metadata DEBUG 2025-01-13 01:26:53,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 160
galaxy.jobs INFO 2025-01-13 01:26:53,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 157 in /galaxy/server/database/jobs_directory/000/157
galaxy.jobs DEBUG 2025-01-13 01:26:53,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 157 executed (128.201 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:26:53,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 158 finished
galaxy.model.metadata DEBUG 2025-01-13 01:26:53,963 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 161
galaxy.jobs INFO 2025-01-13 01:26:54,034 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 158 in /galaxy/server/database/jobs_directory/000/158
galaxy.jobs DEBUG 2025-01-13 01:26:54,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 158 executed (159.957 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:26:55,265 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 159
tpv.core.entities DEBUG 2025-01-13 01:26:55,304 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:26:55,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:26:55,310 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:26:55,329 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:26:55,349 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Working directory for job is: /galaxy/server/database/jobs_directory/000/159
galaxy.jobs.runners DEBUG 2025-01-13 01:26:55,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [159] queued (46.712 ms)
galaxy.jobs.handler INFO 2025-01-13 01:26:55,361 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (159) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:26:55,366 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 159
galaxy.jobs DEBUG 2025-01-13 01:26:55,453 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [159] prepared (67.011 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:26:55,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:26:55,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/0.7.18: mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e
galaxy.tool_util.deps.containers INFO 2025-01-13 01:26:55,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:26:55,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/159/tool_script.sh] for tool command [set -o | grep -q pipefail && set -o pipefail;  ln -s '/galaxy/server/database/objects/2/0/f/dataset_20f9f94a-200b-4696-917d-e9960f8fa23a.dat' 'localref.fa' && bwa index 'localref.fa' &&                 bwa aln -t "${GALAXY_SLOTS:-1}" -b -1   'localref.fa' '/galaxy/server/database/objects/7/b/d/dataset_7bde0858-74a3-4046-a1c7-eda86b0c3409.dat' > first.sai &&  bwa aln -t "${GALAXY_SLOTS:-1}" -b -2   'localref.fa' '/galaxy/server/database/objects/7/b/d/dataset_7bde0858-74a3-4046-a1c7-eda86b0c3409.dat' > second.sai &&  bwa sampe    'localref.fa' first.sai second.sai '/galaxy/server/database/objects/7/b/d/dataset_7bde0858-74a3-4046-a1c7-eda86b0c3409.dat' '/galaxy/server/database/objects/7/b/d/dataset_7bde0858-74a3-4046-a1c7-eda86b0c3409.dat'    | samtools sort -@${GALAXY_SLOTS:-2} -T "${TMPDIR:-.}" -O bam -o '/galaxy/server/database/objects/f/7/8/dataset_f785a1d1-c0ff-4dad-baab-a952f46b69ec.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:26:55,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (159) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/159/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/159/galaxy_159.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:26:55,525 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:26:55,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/0.7.18: mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e
galaxy.tool_util.deps.containers INFO 2025-01-13 01:26:55,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:27:00,222 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 159: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:27:04,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 159 finished
galaxy.model.metadata DEBUG 2025-01-13 01:27:05,055 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 162
galaxy.jobs INFO 2025-01-13 01:27:05,141 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 159 in /galaxy/server/database/jobs_directory/000/159
galaxy.jobs DEBUG 2025-01-13 01:27:05,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 159 executed (194.801 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:27:06,600 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 160
tpv.core.entities DEBUG 2025-01-13 01:27:06,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:27:06,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:27:06,642 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:27:06,659 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:27:06,679 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Working directory for job is: /galaxy/server/database/jobs_directory/000/160
galaxy.jobs.runners DEBUG 2025-01-13 01:27:06,688 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [160] queued (45.397 ms)
galaxy.jobs.handler INFO 2025-01-13 01:27:06,692 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (160) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:27:06,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 160
galaxy.jobs DEBUG 2025-01-13 01:27:06,849 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [160] prepared (133.708 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:27:06,875 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/160/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/160/registry.xml' '/galaxy/server/database/jobs_directory/000/160/upload_params.json' '163:/galaxy/server/database/objects/4/4/4/dataset_444a0b60-779b-4344-9e55-660df86284bb_files:/galaxy/server/database/objects/4/4/4/dataset_444a0b60-779b-4344-9e55-660df86284bb.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:27:06,890 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (160) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/160/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/160/galaxy_160.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:27:07,694 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 162, 161
tpv.core.entities DEBUG 2025-01-13 01:27:07,735 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:27:07,736 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:27:07,741 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:27:07,760 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:27:07,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Working directory for job is: /galaxy/server/database/jobs_directory/000/161
galaxy.jobs.runners DEBUG 2025-01-13 01:27:07,788 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [161] queued (46.773 ms)
galaxy.jobs.handler INFO 2025-01-13 01:27:07,792 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (161) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:27:07,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 161
tpv.core.entities DEBUG 2025-01-13 01:27:07,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:27:07,814 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:27:07,821 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:27:07,839 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:27:07,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Working directory for job is: /galaxy/server/database/jobs_directory/000/162
galaxy.jobs.runners DEBUG 2025-01-13 01:27:07,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [162] queued (55.965 ms)
galaxy.jobs.handler INFO 2025-01-13 01:27:07,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (162) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:27:07,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 162
galaxy.jobs DEBUG 2025-01-13 01:27:07,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [161] prepared (112.278 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:27:07,969 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/161/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/161/registry.xml' '/galaxy/server/database/jobs_directory/000/161/upload_params.json' '164:/galaxy/server/database/objects/f/9/5/dataset_f9593210-a56b-496f-9b59-916db06aad79_files:/galaxy/server/database/objects/f/9/5/dataset_f9593210-a56b-496f-9b59-916db06aad79.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:27:07,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (161) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/161/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/161/galaxy_161.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:27:08,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [162] prepared (116.424 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:27:08,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/162/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/162/registry.xml' '/galaxy/server/database/jobs_directory/000/162/upload_params.json' '165:/galaxy/server/database/objects/b/d/6/dataset_bd6adec7-38e4-410d-9d75-d0e7b5bbcd16_files:/galaxy/server/database/objects/b/d/6/dataset_bd6adec7-38e4-410d-9d75-d0e7b5bbcd16.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:27:08,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (162) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/162/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/162/galaxy_162.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:27:16,153 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 160: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:27:18,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 161: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:27:18,263 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 162: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:27:22,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 160 finished
galaxy.model.metadata DEBUG 2025-01-13 01:27:22,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 163
galaxy.jobs INFO 2025-01-13 01:27:22,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 160 in /galaxy/server/database/jobs_directory/000/160
galaxy.jobs DEBUG 2025-01-13 01:27:23,021 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 160 executed (164.973 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:27:24,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 162 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:27:24,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 161 finished
galaxy.model.metadata DEBUG 2025-01-13 01:27:24,822 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 165
galaxy.model.metadata DEBUG 2025-01-13 01:27:24,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 164
galaxy.jobs INFO 2025-01-13 01:27:24,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 162 in /galaxy/server/database/jobs_directory/000/162
galaxy.jobs INFO 2025-01-13 01:27:24,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 161 in /galaxy/server/database/jobs_directory/000/161
galaxy.jobs DEBUG 2025-01-13 01:27:24,929 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 162 executed (150.819 ms)
galaxy.jobs DEBUG 2025-01-13 01:27:24,952 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 161 executed (132.982 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:27:25,419 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 163
tpv.core.entities DEBUG 2025-01-13 01:27:25,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/.*, abstract=False, cores=8, mem=20, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:27:25,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:27:25,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:27:25,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:27:25,513 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Working directory for job is: /galaxy/server/database/jobs_directory/000/163
galaxy.jobs.runners DEBUG 2025-01-13 01:27:25,523 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [163] queued (48.871 ms)
galaxy.jobs.handler INFO 2025-01-13 01:27:25,526 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (163) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:27:25,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 163
galaxy.jobs DEBUG 2025-01-13 01:27:25,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [163] prepared (75.671 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:27:25,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:27:25,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/0.7.18: mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e
galaxy.tool_util.deps.containers INFO 2025-01-13 01:27:25,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:27:25,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/163/tool_script.sh] for tool command [set -o | grep -q pipefail && set -o pipefail;  ln -s '/galaxy/server/database/objects/4/4/4/dataset_444a0b60-779b-4344-9e55-660df86284bb.dat' 'localref.fa' && bwa index 'localref.fa' &&                            bwa aln -t "${GALAXY_SLOTS:-1}"   'localref.fa' '/galaxy/server/database/objects/f/9/5/dataset_f9593210-a56b-496f-9b59-916db06aad79.dat' > first.sai &&  bwa aln -t "${GALAXY_SLOTS:-1}"   'localref.fa' '/galaxy/server/database/objects/b/d/6/dataset_bd6adec7-38e4-410d-9d75-d0e7b5bbcd16.dat' > second.sai &&  bwa sampe    -r '@RG\tID:rg1\tPL:CAPILLARY'  'localref.fa' first.sai second.sai '/galaxy/server/database/objects/f/9/5/dataset_f9593210-a56b-496f-9b59-916db06aad79.dat' '/galaxy/server/database/objects/b/d/6/dataset_bd6adec7-38e4-410d-9d75-d0e7b5bbcd16.dat'    | samtools sort -@${GALAXY_SLOTS:-2} -T "${TMPDIR:-.}" -O bam -o '/galaxy/server/database/objects/2/4/3/dataset_2432d4e3-d187-4a63-8833-4d5fb188a034.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:27:25,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (163) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/163/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/163/galaxy_163.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:27:25,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:27:25,701 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa/0.7.18: mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e
galaxy.tool_util.deps.containers INFO 2025-01-13 01:27:25,720 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:1bd8542a8a0b42e0981337910954371d0230828e-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:27:31,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 163: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:27:36,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 163 finished
galaxy.model.metadata DEBUG 2025-01-13 01:27:36,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 166
galaxy.jobs INFO 2025-01-13 01:27:36,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 163 in /galaxy/server/database/jobs_directory/000/163
galaxy.jobs DEBUG 2025-01-13 01:27:36,344 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 163 executed (179.581 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:27:39,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 164
tpv.core.entities DEBUG 2025-01-13 01:27:39,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:27:39,868 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:27:39,872 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:27:39,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:27:39,906 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Working directory for job is: /galaxy/server/database/jobs_directory/000/164
galaxy.jobs.runners DEBUG 2025-01-13 01:27:39,913 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [164] queued (40.369 ms)
galaxy.jobs.handler INFO 2025-01-13 01:27:39,916 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (164) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:27:39,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 164
galaxy.jobs DEBUG 2025-01-13 01:27:40,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [164] prepared (94.936 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:27:40,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/164/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/164/registry.xml' '/galaxy/server/database/jobs_directory/000/164/upload_params.json' '167:/galaxy/server/database/objects/8/b/0/dataset_8b004704-004a-431a-b78e-4fdf0c235b41_files:/galaxy/server/database/objects/8/b/0/dataset_8b004704-004a-431a-b78e-4fdf0c235b41.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:27:40,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (164) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/164/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/164/galaxy_164.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:27:47,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 164: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:27:52,447 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 164 finished
galaxy.model.metadata DEBUG 2025-01-13 01:27:52,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 167
galaxy.jobs INFO 2025-01-13 01:27:52,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 164 in /galaxy/server/database/jobs_directory/000/164
galaxy.jobs DEBUG 2025-01-13 01:27:52,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 164 executed (125.098 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:27:53,207 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 165
tpv.core.entities DEBUG 2025-01-13 01:27:53,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/.*, abstract=False, cores=1, mem=4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:27:53,249 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:27:53,255 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:27:53,274 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:27:53,294 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Working directory for job is: /galaxy/server/database/jobs_directory/000/165
galaxy.jobs.runners DEBUG 2025-01-13 01:27:53,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [165] queued (47.549 ms)
galaxy.jobs.handler INFO 2025-01-13 01:27:53,307 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (165) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:27:53,312 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 165
galaxy.jobs DEBUG 2025-01-13 01:27:53,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [165] prepared (79.050 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:27:53,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:27:53,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:27:53,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:27:53,622 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/165/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", statmod version" $(R --vanilla --slave -e "library(statmod); cat(sessionInfo()\$otherPkgs\$statmod\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", gplots version" $(R --vanilla --slave -e "library(gplots); cat(sessionInfo()\$otherPkgs\$gplots\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", Glimma version" $(R --vanilla --slave -e "library(Glimma); cat(sessionInfo()\$otherPkgs\$Glimma\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/165/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/119b069fc845/limma_voom/limma_voom.R'  -R '/galaxy/server/database/objects/d/5/4/dataset_d544c453-6675-415d-b449-576046645b30.dat' -o '/galaxy/server/database/objects/d/5/4/dataset_d544c453-6675-415d-b449-576046645b30_files'  -m '/galaxy/server/database/objects/8/b/0/dataset_8b004704-004a-431a-b78e-4fdf0c235b41.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'   -D 'Mut-WT,WT-Mut'   -P i       -l '0.0' -p '0.05' -d 'BH' -G '6'   -n 'TMM'  -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/d/5/4/dataset_d544c453-6675-415d-b449-576046645b30_files'/*tsv output_dir/  && cp -r ./glimma* '/galaxy/server/database/objects/d/5/4/dataset_d544c453-6675-415d-b449-576046645b30_files']
galaxy.jobs.runners DEBUG 2025-01-13 01:27:53,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (165) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/165/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/165/galaxy_165.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:27:53,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:27:53,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:27:53,666 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:28:53,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 165: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:28:58,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 165 finished
galaxy.model.store.discover DEBUG 2025-01-13 01:28:58,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (165) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/165/working/output_dir/limma-voom_Mut-WT.tsv] with element identifier [limma-voom_Mut-WT] for output [outTables] (3.742 ms)
galaxy.model.store.discover DEBUG 2025-01-13 01:28:58,649 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (165) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/165/working/output_dir/limma-voom_WT-Mut.tsv] with element identifier [limma-voom_WT-Mut] for output [outTables] (0.499 ms)
galaxy.model.store.discover DEBUG 2025-01-13 01:28:58,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (165) Add dynamic collection datasets to history for output [outTables] (25.761 ms)
galaxy.model.metadata DEBUG 2025-01-13 01:28:58,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 168
galaxy.jobs INFO 2025-01-13 01:28:58,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 165 in /galaxy/server/database/jobs_directory/000/165
galaxy.objectstore CRITICAL 2025-01-13 01:28:58,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] None delete error [Errno 13] Permission denied: 'limma-voom_WT-Mut.tsv'
galaxy.jobs DEBUG 2025-01-13 01:28:58,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 165 executed (176.356 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:29:00,974 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 167, 166
tpv.core.entities DEBUG 2025-01-13 01:29:01,011 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:29:01,012 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:29:01,017 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:29:01,034 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:29:01,052 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Working directory for job is: /galaxy/server/database/jobs_directory/000/166
galaxy.jobs.runners DEBUG 2025-01-13 01:29:01,058 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [166] queued (40.831 ms)
galaxy.jobs.handler INFO 2025-01-13 01:29:01,062 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (166) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:29:01,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 166
tpv.core.entities DEBUG 2025-01-13 01:29:01,078 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:29:01,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:29:01,083 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:29:01,102 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:29:01,133 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Working directory for job is: /galaxy/server/database/jobs_directory/000/167
galaxy.jobs.runners DEBUG 2025-01-13 01:29:01,141 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [167] queued (57.722 ms)
galaxy.jobs.handler INFO 2025-01-13 01:29:01,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (167) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:29:01,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 167
galaxy.jobs DEBUG 2025-01-13 01:29:01,203 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [166] prepared (111.196 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:29:01,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/166/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/166/registry.xml' '/galaxy/server/database/jobs_directory/000/166/upload_params.json' '171:/galaxy/server/database/objects/d/6/b/dataset_d6b3610e-faf2-43a8-9d5e-c1ec2c1938f5_files:/galaxy/server/database/objects/d/6/b/dataset_d6b3610e-faf2-43a8-9d5e-c1ec2c1938f5.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:29:01,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (166) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/166/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/166/galaxy_166.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:29:01,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [167] prepared (108.567 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:29:01,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/167/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/167/registry.xml' '/galaxy/server/database/jobs_directory/000/167/upload_params.json' '172:/galaxy/server/database/objects/9/d/0/dataset_9d02ae6a-5060-4e44-942b-eb3aa8aa996b_files:/galaxy/server/database/objects/9/d/0/dataset_9d02ae6a-5060-4e44-942b-eb3aa8aa996b.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:29:01,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (167) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/167/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/167/galaxy_167.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:29:10,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 166: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:29:10,154 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 167: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:29:15,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 166 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:29:15,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 167 finished
galaxy.model.metadata DEBUG 2025-01-13 01:29:15,138 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 171
galaxy.model.metadata DEBUG 2025-01-13 01:29:15,156 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 172
galaxy.jobs INFO 2025-01-13 01:29:15,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 166 in /galaxy/server/database/jobs_directory/000/166
galaxy.jobs INFO 2025-01-13 01:29:15,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 167 in /galaxy/server/database/jobs_directory/000/167
galaxy.jobs DEBUG 2025-01-13 01:29:15,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 166 executed (157.010 ms)
galaxy.jobs DEBUG 2025-01-13 01:29:15,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 167 executed (154.729 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:29:16,471 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 168
tpv.core.entities DEBUG 2025-01-13 01:29:16,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/.*, abstract=False, cores=1, mem=4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:29:16,515 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:29:16,521 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:29:16,540 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:29:16,561 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Working directory for job is: /galaxy/server/database/jobs_directory/000/168
galaxy.jobs.runners DEBUG 2025-01-13 01:29:16,571 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [168] queued (49.364 ms)
galaxy.jobs.handler INFO 2025-01-13 01:29:16,574 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (168) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:29:16,579 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 168
galaxy.jobs DEBUG 2025-01-13 01:29:16,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [168] prepared (71.406 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:29:16,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:29:16,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:29:16,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:29:16,741 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/168/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", statmod version" $(R --vanilla --slave -e "library(statmod); cat(sessionInfo()\$otherPkgs\$statmod\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", gplots version" $(R --vanilla --slave -e "library(gplots); cat(sessionInfo()\$otherPkgs\$gplots\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", Glimma version" $(R --vanilla --slave -e "library(Glimma); cat(sessionInfo()\$otherPkgs\$Glimma\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/168/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/119b069fc845/limma_voom/limma_voom.R'  -R '/galaxy/server/database/objects/2/0/3/dataset_20343ccb-9b21-4037-8146-ff3256986279.dat' -o '/galaxy/server/database/objects/2/0/3/dataset_20343ccb-9b21-4037-8146-ff3256986279_files'  -m '/galaxy/server/database/objects/d/6/b/dataset_d6b3610e-faf2-43a8-9d5e-c1ec2c1938f5.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'  -a '/galaxy/server/database/objects/9/d/0/dataset_9d02ae6a-5060-4e44-942b-eb3aa8aa996b.dat'  -D 'Mut-WT'   -P i       -l '0.0' -p '0.05' -d 'BH' -G '6'   -n 'TMM'  -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/2/0/3/dataset_20343ccb-9b21-4037-8146-ff3256986279_files'/*tsv output_dir/  && cp -r ./glimma* '/galaxy/server/database/objects/2/0/3/dataset_20343ccb-9b21-4037-8146-ff3256986279_files']
galaxy.jobs.runners DEBUG 2025-01-13 01:29:16,756 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (168) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/168/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/168/galaxy_168.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:29:16,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:29:16,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:29:16,791 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:29:40,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 168: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:29:45,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 168 finished
galaxy.model.store.discover DEBUG 2025-01-13 01:29:45,470 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (168) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/168/working/output_dir/limma-voom_Mut-WT.tsv] with element identifier [limma-voom_Mut-WT] for output [outTables] (5.602 ms)
galaxy.model.store.discover DEBUG 2025-01-13 01:29:45,487 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (168) Add dynamic collection datasets to history for output [outTables] (16.610 ms)
galaxy.model.metadata DEBUG 2025-01-13 01:29:45,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 173
galaxy.jobs INFO 2025-01-13 01:29:45,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 168 in /galaxy/server/database/jobs_directory/000/168
galaxy.objectstore CRITICAL 2025-01-13 01:29:45,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] None delete error [Errno 13] Permission denied: 'limma-voom_Mut-WT.tsv'
galaxy.jobs DEBUG 2025-01-13 01:29:45,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 168 executed (187.388 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:29:47,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 169
tpv.core.entities DEBUG 2025-01-13 01:29:47,272 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:29:47,273 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:29:47,278 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:29:47,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:29:47,311 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Working directory for job is: /galaxy/server/database/jobs_directory/000/169
galaxy.jobs.runners DEBUG 2025-01-13 01:29:47,318 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [169] queued (39.828 ms)
galaxy.jobs.handler INFO 2025-01-13 01:29:47,321 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (169) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:29:47,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 169
galaxy.jobs DEBUG 2025-01-13 01:29:47,434 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [169] prepared (89.794 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:29:47,456 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/169/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/169/registry.xml' '/galaxy/server/database/jobs_directory/000/169/upload_params.json' '175:/galaxy/server/database/objects/2/7/a/dataset_27a130e4-5eab-425d-a6a8-246fb22eedf9_files:/galaxy/server/database/objects/2/7/a/dataset_27a130e4-5eab-425d-a6a8-246fb22eedf9.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:29:47,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (169) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/169/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/169/galaxy_169.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:29:54,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 169: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:29:59,609 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 169 finished
galaxy.model.metadata DEBUG 2025-01-13 01:29:59,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 175
galaxy.jobs INFO 2025-01-13 01:29:59,700 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 169 in /galaxy/server/database/jobs_directory/000/169
galaxy.jobs DEBUG 2025-01-13 01:29:59,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 169 executed (113.442 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:30:00,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 170
tpv.core.entities DEBUG 2025-01-13 01:30:00,632 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/.*, abstract=False, cores=1, mem=4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:30:00,632 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:30:00,637 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:30:00,654 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:30:00,677 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Working directory for job is: /galaxy/server/database/jobs_directory/000/170
galaxy.jobs.runners DEBUG 2025-01-13 01:30:00,686 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [170] queued (48.681 ms)
galaxy.jobs.handler INFO 2025-01-13 01:30:00,690 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (170) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:30:00,694 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 170
galaxy.jobs DEBUG 2025-01-13 01:30:00,786 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [170] prepared (72.931 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:30:00,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:30:00,787 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:30:00,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:30:00,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/170/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", statmod version" $(R --vanilla --slave -e "library(statmod); cat(sessionInfo()\$otherPkgs\$statmod\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", gplots version" $(R --vanilla --slave -e "library(gplots); cat(sessionInfo()\$otherPkgs\$gplots\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", Glimma version" $(R --vanilla --slave -e "library(Glimma); cat(sessionInfo()\$otherPkgs\$Glimma\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/170/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/119b069fc845/limma_voom/limma_voom.R'  -R '/galaxy/server/database/objects/d/1/c/dataset_d1c6742c-a0ef-48a0-a0bf-d8804089829e.dat' -o '/galaxy/server/database/objects/d/1/c/dataset_d1c6742c-a0ef-48a0-a0bf-d8804089829e_files'  -m '/galaxy/server/database/objects/2/7/a/dataset_27a130e4-5eab-425d-a6a8-246fb22eedf9.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'   -D 'Mut-WT'   -P i     -r   -l '0.0' -p '0.05' -d 'BH' -G '6'   -n 'TMM'  -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/d/1/c/dataset_d1c6742c-a0ef-48a0-a0bf-d8804089829e_files'/*tsv output_dir/  && cp -r ./glimma* '/galaxy/server/database/objects/d/1/c/dataset_d1c6742c-a0ef-48a0-a0bf-d8804089829e_files'   && cp '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/119b069fc845/limma_voom/limma_voom.R' '/galaxy/server/database/objects/0/d/d/dataset_0dd99191-7b72-447d-8d54-f41216a34147.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:30:00,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (170) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/170/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/170/galaxy_170.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:30:00,869 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:30:00,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:30:00,889 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:30:23,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 170: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:30:28,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 170 finished
galaxy.model.store.discover DEBUG 2025-01-13 01:30:28,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (170) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/170/working/output_dir/limma-voom_Mut-WT.tsv] with element identifier [limma-voom_Mut-WT] for output [outTables] (5.161 ms)
galaxy.model.store.discover DEBUG 2025-01-13 01:30:28,211 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (170) Add dynamic collection datasets to history for output [outTables] (15.102 ms)
galaxy.model.metadata DEBUG 2025-01-13 01:30:28,239 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 176
galaxy.model.metadata DEBUG 2025-01-13 01:30:28,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 177
galaxy.util WARNING 2025-01-13 01:30:28,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/d/d/dataset_0dd99191-7b72-447d-8d54-f41216a34147.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/d/d/dataset_0dd99191-7b72-447d-8d54-f41216a34147.dat'
galaxy.jobs INFO 2025-01-13 01:30:28,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 170 in /galaxy/server/database/jobs_directory/000/170
galaxy.objectstore CRITICAL 2025-01-13 01:30:28,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] None delete error [Errno 13] Permission denied: 'limma-voom_Mut-WT.tsv'
galaxy.jobs DEBUG 2025-01-13 01:30:28,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 170 executed (180.575 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:30:30,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 171
tpv.core.entities DEBUG 2025-01-13 01:30:30,511 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:30:30,512 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:30:30,518 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:30:30,536 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:30:30,552 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Working directory for job is: /galaxy/server/database/jobs_directory/000/171
galaxy.jobs.runners DEBUG 2025-01-13 01:30:30,559 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [171] queued (41.169 ms)
galaxy.jobs.handler INFO 2025-01-13 01:30:30,562 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (171) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:30:30,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 171
galaxy.jobs DEBUG 2025-01-13 01:30:30,682 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [171] prepared (94.490 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:30:30,704 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/171/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/171/registry.xml' '/galaxy/server/database/jobs_directory/000/171/upload_params.json' '179:/galaxy/server/database/objects/4/7/5/dataset_475617d3-c75b-4dde-bb70-e32b3d84ffb3_files:/galaxy/server/database/objects/4/7/5/dataset_475617d3-c75b-4dde-bb70-e32b3d84ffb3.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:30:30,718 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (171) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/171/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/171/galaxy_171.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:30:39,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 171: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:30:44,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 171 finished
galaxy.model.metadata DEBUG 2025-01-13 01:30:44,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 179
galaxy.jobs INFO 2025-01-13 01:30:44,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 171 in /galaxy/server/database/jobs_directory/000/171
galaxy.jobs DEBUG 2025-01-13 01:30:44,416 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 171 executed (136.943 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:30:44,861 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 172
tpv.core.entities DEBUG 2025-01-13 01:30:44,899 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/.*, abstract=False, cores=1, mem=4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:30:44,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:30:44,905 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:30:44,922 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:30:44,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Working directory for job is: /galaxy/server/database/jobs_directory/000/172
galaxy.jobs.runners DEBUG 2025-01-13 01:30:44,950 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [172] queued (44.502 ms)
galaxy.jobs.handler INFO 2025-01-13 01:30:44,954 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (172) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:30:44,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 172
galaxy.jobs DEBUG 2025-01-13 01:30:45,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [172] prepared (71.853 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:30:45,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:30:45,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:30:45,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:30:45,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/172/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", statmod version" $(R --vanilla --slave -e "library(statmod); cat(sessionInfo()\$otherPkgs\$statmod\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", gplots version" $(R --vanilla --slave -e "library(gplots); cat(sessionInfo()\$otherPkgs\$gplots\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", Glimma version" $(R --vanilla --slave -e "library(Glimma); cat(sessionInfo()\$otherPkgs\$Glimma\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/172/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/119b069fc845/limma_voom/limma_voom.R'  -R '/galaxy/server/database/objects/4/c/8/dataset_4c8cabf8-e102-43cb-8a96-1349c03ea60f.dat' -o '/galaxy/server/database/objects/4/c/8/dataset_4c8cabf8-e102-43cb-8a96-1349c03ea60f_files'  -m '/galaxy/server/database/objects/4/7/5/dataset_475617d3-c75b-4dde-bb70-e32b3d84ffb3.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT|Batch::b1,b2,b3,b1,b2,b3'   -D 'Mut-WT'   -P i       -l '0.0' -p '0.05' -d 'BH' -G '6'   -n 'TMM'  -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/4/c/8/dataset_4c8cabf8-e102-43cb-8a96-1349c03ea60f_files'/*tsv output_dir/  && cp -r ./glimma* '/galaxy/server/database/objects/4/c/8/dataset_4c8cabf8-e102-43cb-8a96-1349c03ea60f_files']
galaxy.jobs.runners DEBUG 2025-01-13 01:30:45,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (172) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/172/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/172/galaxy_172.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:30:45,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:30:45,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:30:45,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:31:07,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 172: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:31:12,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 172 finished
galaxy.model.store.discover DEBUG 2025-01-13 01:31:12,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (172) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/172/working/output_dir/limma-voom_Mut-WT.tsv] with element identifier [limma-voom_Mut-WT] for output [outTables] (4.216 ms)
galaxy.model.store.discover DEBUG 2025-01-13 01:31:12,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (172) Add dynamic collection datasets to history for output [outTables] (16.680 ms)
galaxy.model.metadata DEBUG 2025-01-13 01:31:12,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 180
galaxy.jobs INFO 2025-01-13 01:31:12,796 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 172 in /galaxy/server/database/jobs_directory/000/172
galaxy.objectstore CRITICAL 2025-01-13 01:31:12,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] None delete error [Errno 13] Permission denied: 'limma-voom_Mut-WT.tsv'
galaxy.jobs DEBUG 2025-01-13 01:31:12,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 172 executed (165.454 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:31:14,602 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 174, 173
tpv.core.entities DEBUG 2025-01-13 01:31:14,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:31:14,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:31:14,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:31:14,659 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:31:14,674 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Working directory for job is: /galaxy/server/database/jobs_directory/000/173
galaxy.jobs.runners DEBUG 2025-01-13 01:31:14,681 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [173] queued (36.532 ms)
galaxy.jobs.handler INFO 2025-01-13 01:31:14,684 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (173) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:31:14,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 173
tpv.core.entities DEBUG 2025-01-13 01:31:14,703 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:31:14,704 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:31:14,710 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:31:14,729 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:31:14,755 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Working directory for job is: /galaxy/server/database/jobs_directory/000/174
galaxy.jobs.runners DEBUG 2025-01-13 01:31:14,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [174] queued (52.226 ms)
galaxy.jobs.handler INFO 2025-01-13 01:31:14,767 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (174) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:31:14,772 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 174
galaxy.jobs DEBUG 2025-01-13 01:31:14,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [173] prepared (111.345 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:31:14,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/173/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/173/registry.xml' '/galaxy/server/database/jobs_directory/000/173/upload_params.json' '182:/galaxy/server/database/objects/0/f/c/dataset_0fc3762a-a505-4f68-9929-f6e7d7a2162f_files:/galaxy/server/database/objects/0/f/c/dataset_0fc3762a-a505-4f68-9929-f6e7d7a2162f.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:31:14,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (173) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/173/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/173/galaxy_173.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:31:14,892 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [174] prepared (97.864 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:31:14,916 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/174/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/174/registry.xml' '/galaxy/server/database/jobs_directory/000/174/upload_params.json' '183:/galaxy/server/database/objects/7/4/1/dataset_7412c2cb-000d-4e4a-b71b-4666f5c85ff7_files:/galaxy/server/database/objects/7/4/1/dataset_7412c2cb-000d-4e4a-b71b-4666f5c85ff7.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:31:14,930 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (174) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/174/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/174/galaxy_174.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:31:23,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 174: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:31:23,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 173: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:31:28,414 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 174 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:31:28,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 173 finished
galaxy.model.metadata DEBUG 2025-01-13 01:31:28,480 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 183
galaxy.model.metadata DEBUG 2025-01-13 01:31:28,513 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 182
galaxy.jobs INFO 2025-01-13 01:31:28,530 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 174 in /galaxy/server/database/jobs_directory/000/174
galaxy.jobs INFO 2025-01-13 01:31:28,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 173 in /galaxy/server/database/jobs_directory/000/173
galaxy.jobs DEBUG 2025-01-13 01:31:28,596 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 174 executed (152.690 ms)
galaxy.jobs DEBUG 2025-01-13 01:31:28,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 173 executed (147.326 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:31:29,088 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 175
tpv.core.entities DEBUG 2025-01-13 01:31:29,124 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/.*, abstract=False, cores=1, mem=4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:31:29,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:31:29,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:31:29,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:31:29,161 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Working directory for job is: /galaxy/server/database/jobs_directory/000/175
galaxy.jobs.runners DEBUG 2025-01-13 01:31:29,168 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [175] queued (38.787 ms)
galaxy.jobs.handler INFO 2025-01-13 01:31:29,172 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (175) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:31:29,177 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 175
galaxy.jobs DEBUG 2025-01-13 01:31:29,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [175] prepared (55.295 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:31:29,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:31:29,251 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:31:29,272 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:31:29,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/175/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", statmod version" $(R --vanilla --slave -e "library(statmod); cat(sessionInfo()\$otherPkgs\$statmod\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", gplots version" $(R --vanilla --slave -e "library(gplots); cat(sessionInfo()\$otherPkgs\$gplots\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", Glimma version" $(R --vanilla --slave -e "library(Glimma); cat(sessionInfo()\$otherPkgs\$Glimma\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/175/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/119b069fc845/limma_voom/limma_voom.R'  -R '/galaxy/server/database/objects/6/9/0/dataset_690f9502-c117-4a3f-84aa-27b1c51df306.dat' -o '/galaxy/server/database/objects/6/9/0/dataset_690f9502-c117-4a3f-84aa-27b1c51df306_files'  -m '/galaxy/server/database/objects/0/f/c/dataset_0fc3762a-a505-4f68-9929-f6e7d7a2162f.dat' -f '/galaxy/server/database/objects/7/4/1/dataset_7412c2cb-000d-4e4a-b71b-4666f5c85ff7.dat'   -D 'Mut-WT'   -P i       -l '0.0' -p '0.05' -d 'BH' -G '6'   -n 'TMM'  -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/6/9/0/dataset_690f9502-c117-4a3f-84aa-27b1c51df306_files'/*tsv output_dir/  && cp -r ./glimma* '/galaxy/server/database/objects/6/9/0/dataset_690f9502-c117-4a3f-84aa-27b1c51df306_files']
galaxy.jobs.runners DEBUG 2025-01-13 01:31:29,307 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (175) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/175/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/175/galaxy_175.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:31:29,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:31:29,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:31:29,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:31:52,744 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 175: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:31:57,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 175 finished
galaxy.model.store.discover DEBUG 2025-01-13 01:31:57,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (175) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/175/working/output_dir/limma-voom_Mut-WT.tsv] with element identifier [limma-voom_Mut-WT] for output [outTables] (7.184 ms)
galaxy.model.store.discover DEBUG 2025-01-13 01:31:57,574 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (175) Add dynamic collection datasets to history for output [outTables] (17.831 ms)
galaxy.model.metadata DEBUG 2025-01-13 01:31:57,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 184
galaxy.jobs INFO 2025-01-13 01:31:57,662 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 175 in /galaxy/server/database/jobs_directory/000/175
galaxy.objectstore CRITICAL 2025-01-13 01:31:57,702 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] None delete error [Errno 13] Permission denied: 'limma-voom_Mut-WT.tsv'
galaxy.jobs DEBUG 2025-01-13 01:31:57,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 175 executed (178.335 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:31:58,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 176
tpv.core.entities DEBUG 2025-01-13 01:31:58,922 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:31:58,923 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:31:58,927 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:31:58,943 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:31:58,960 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Working directory for job is: /galaxy/server/database/jobs_directory/000/176
galaxy.jobs.runners DEBUG 2025-01-13 01:31:58,967 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [176] queued (40.192 ms)
galaxy.jobs.handler INFO 2025-01-13 01:31:58,972 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (176) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:31:58,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 176
galaxy.jobs DEBUG 2025-01-13 01:31:59,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [176] prepared (92.597 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:31:59,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/176/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/176/registry.xml' '/galaxy/server/database/jobs_directory/000/176/upload_params.json' '186:/galaxy/server/database/objects/d/4/0/dataset_d404b792-7679-4217-bd2b-e33756c9e43e_files:/galaxy/server/database/objects/d/4/0/dataset_d404b792-7679-4217-bd2b-e33756c9e43e.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:31:59,123 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (176) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/176/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/176/galaxy_176.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:32:07,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 176: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:32:12,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 176 finished
galaxy.model.metadata DEBUG 2025-01-13 01:32:12,740 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 186
galaxy.jobs INFO 2025-01-13 01:32:12,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 176 in /galaxy/server/database/jobs_directory/000/176
galaxy.jobs DEBUG 2025-01-13 01:32:12,830 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 176 executed (122.623 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:32:13,280 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 177
tpv.core.entities DEBUG 2025-01-13 01:32:13,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/.*, abstract=False, cores=1, mem=4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:13,320 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:13,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:13,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:32:13,368 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Working directory for job is: /galaxy/server/database/jobs_directory/000/177
galaxy.jobs.runners DEBUG 2025-01-13 01:32:13,377 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [177] queued (51.140 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:13,380 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (177) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:13,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 177
galaxy.jobs DEBUG 2025-01-13 01:32:13,478 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [177] prepared (75.016 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:32:13,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:32:13,479 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:32:13,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:32:13,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/177/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", statmod version" $(R --vanilla --slave -e "library(statmod); cat(sessionInfo()\$otherPkgs\$statmod\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", gplots version" $(R --vanilla --slave -e "library(gplots); cat(sessionInfo()\$otherPkgs\$gplots\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", Glimma version" $(R --vanilla --slave -e "library(Glimma); cat(sessionInfo()\$otherPkgs\$Glimma\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/177/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/119b069fc845/limma_voom/limma_voom.R'  -R '/galaxy/server/database/objects/c/4/c/dataset_c4c10365-260e-404d-90c6-dfc2c6b6a92f.dat' -o '/galaxy/server/database/objects/c/4/c/dataset_c4c10365-260e-404d-90c6-dfc2c6b6a92f_files'  -m '/galaxy/server/database/objects/d/4/0/dataset_d404b792-7679-4217-bd2b-e33756c9e43e.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'   -D 'Mut-WT'  -z '10' -s '3'  -P i  -F  -x     -l '0.0' -p '0.05' -d 'BH' -G '6'   -n 'TMM'  -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/c/4/c/dataset_c4c10365-260e-404d-90c6-dfc2c6b6a92f_files'/*tsv output_dir/  && cp -r ./glimma* '/galaxy/server/database/objects/c/4/c/dataset_c4c10365-260e-404d-90c6-dfc2c6b6a92f_files'  && cp '/galaxy/server/database/objects/c/4/c/dataset_c4c10365-260e-404d-90c6-dfc2c6b6a92f_files'/*counts output_dir/]
galaxy.jobs.runners DEBUG 2025-01-13 01:32:13,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (177) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/177/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/177/galaxy_177.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/177/working/output_dir/"*"_filtcounts" ] ; then cp "/galaxy/server/database/jobs_directory/000/177/working/output_dir/"*"_filtcounts" "/galaxy/server/database/objects/6/d/9/dataset_6d93cfb3-6ec5-4ef9-ad62-8df2f15e39f4.dat" ; fi; 
if [ -f "/galaxy/server/database/jobs_directory/000/177/working/output_dir/"*"_normcounts" ] ; then cp "/galaxy/server/database/jobs_directory/000/177/working/output_dir/"*"_normcounts" "/galaxy/server/database/objects/0/8/a/dataset_08adc0f3-0a87-4567-9a9d-6ac581055905.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:32:13,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:32:13,570 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:32:13,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.runners.kubernetes ERROR 2025-01-13 01:32:35,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://10.100.128.1:443/apis/batch/v1/namespaces/prod-25-01-13-00-39-1/jobs/gxy-lqxnh

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 817, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 824, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 107, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-lqxnh": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:35,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (177/gxy-lqxnh) Terminated at user's request
galaxy.util WARNING 2025-01-13 01:32:35,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/d/9/dataset_6d93cfb3-6ec5-4ef9-ad62-8df2f15e39f4.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/d/9/dataset_6d93cfb3-6ec5-4ef9-ad62-8df2f15e39f4.dat'
galaxy.util WARNING 2025-01-13 01:32:35,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/8/a/dataset_08adc0f3-0a87-4567-9a9d-6ac581055905.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/8/a/dataset_08adc0f3-0a87-4567-9a9d-6ac581055905.dat'
galaxy.objectstore CRITICAL 2025-01-13 01:32:35,369 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] None delete error [Errno 13] Permission denied: 'limma-voom_Mut-WT.tsv'
galaxy.jobs.handler DEBUG 2025-01-13 01:32:36,886 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 179, 178
tpv.core.entities DEBUG 2025-01-13 01:32:36,918 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:36,919 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:36,923 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:36,935 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:32:36,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Working directory for job is: /galaxy/server/database/jobs_directory/000/178
galaxy.jobs.runners DEBUG 2025-01-13 01:32:36,958 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [178] queued (35.399 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:36,962 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (178) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:36,966 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 178
tpv.core.entities DEBUG 2025-01-13 01:32:36,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:36,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:36,983 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:36,999 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:32:37,026 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Working directory for job is: /galaxy/server/database/jobs_directory/000/179
galaxy.jobs.runners DEBUG 2025-01-13 01:32:37,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [179] queued (49.312 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:37,036 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (179) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:37,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 179
galaxy.jobs DEBUG 2025-01-13 01:32:37,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [178] prepared (100.648 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:32:37,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/178/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/178/registry.xml' '/galaxy/server/database/jobs_directory/000/178/upload_params.json' '190:/galaxy/server/database/objects/c/5/6/dataset_c56d64f2-c4db-4a9e-80a3-c6606e7f00a9_files:/galaxy/server/database/objects/c/5/6/dataset_c56d64f2-c4db-4a9e-80a3-c6606e7f00a9.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:32:37,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (178) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/178/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/178/galaxy_178.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:32:37,172 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [179] prepared (102.836 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:32:37,196 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/179/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/179/registry.xml' '/galaxy/server/database/jobs_directory/000/179/upload_params.json' '191:/galaxy/server/database/objects/5/5/1/dataset_551aef98-bd46-408a-82fc-11a90403b7c3_files:/galaxy/server/database/objects/5/5/1/dataset_551aef98-bd46-408a-82fc-11a90403b7c3.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:32:37,210 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (179) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/179/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/179/galaxy_179.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:32:38,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 181, 184, 180, 183, 182
tpv.core.entities DEBUG 2025-01-13 01:32:38,075 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:38,076 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:38,080 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:38,097 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:32:38,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Working directory for job is: /galaxy/server/database/jobs_directory/000/180
galaxy.jobs.runners DEBUG 2025-01-13 01:32:38,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [180] queued (39.862 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:38,124 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (180) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:38,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 180
tpv.core.entities DEBUG 2025-01-13 01:32:38,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:38,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:38,150 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:38,167 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:32:38,197 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Working directory for job is: /galaxy/server/database/jobs_directory/000/181
galaxy.jobs.runners DEBUG 2025-01-13 01:32:38,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [181] queued (55.960 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:38,211 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (181) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:38,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 181
tpv.core.entities DEBUG 2025-01-13 01:32:38,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:38,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:38,258 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:38,292 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:32:38,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [180] prepared (138.877 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:32:38,342 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/180/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/180/registry.xml' '/galaxy/server/database/jobs_directory/000/180/upload_params.json' '192:/galaxy/server/database/objects/5/6/a/dataset_56a12770-75fd-4ede-a1e3-be7fe60752f4_files:/galaxy/server/database/objects/5/6/a/dataset_56a12770-75fd-4ede-a1e3-be7fe60752f4.dat']
galaxy.jobs DEBUG 2025-01-13 01:32:38,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Working directory for job is: /galaxy/server/database/jobs_directory/000/182
galaxy.jobs.runners DEBUG 2025-01-13 01:32:38,360 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [182] queued (101.553 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:38,366 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (182) Job dispatched
galaxy.jobs.runners DEBUG 2025-01-13 01:32:38,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (180) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/180/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/180/galaxy_180.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:38,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 182
tpv.core.entities DEBUG 2025-01-13 01:32:38,397 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:38,398 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:38,412 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:38,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [181] prepared (178.470 ms)
galaxy.jobs DEBUG 2025-01-13 01:32:38,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Persisting job destination (destination id: k8s)
galaxy.jobs.command_factory INFO 2025-01-13 01:32:38,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/181/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/181/registry.xml' '/galaxy/server/database/jobs_directory/000/181/upload_params.json' '193:/galaxy/server/database/objects/c/0/5/dataset_c053abe3-e7a9-4c49-b59e-9b0b42eea02e_files:/galaxy/server/database/objects/c/0/5/dataset_c053abe3-e7a9-4c49-b59e-9b0b42eea02e.dat']
galaxy.jobs DEBUG 2025-01-13 01:32:38,482 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Working directory for job is: /galaxy/server/database/jobs_directory/000/183
galaxy.jobs.runners DEBUG 2025-01-13 01:32:38,493 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [183] queued (76.509 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:38,500 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (183) Job dispatched
galaxy.jobs.runners DEBUG 2025-01-13 01:32:38,504 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (181) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/181/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/181/galaxy_181.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:38,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 183
tpv.core.entities DEBUG 2025-01-13 01:32:38,533 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:38,534 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:38,547 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:38,583 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:32:38,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [182] prepared (176.133 ms)
galaxy.jobs DEBUG 2025-01-13 01:32:38,626 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Working directory for job is: /galaxy/server/database/jobs_directory/000/184
galaxy.jobs.runners DEBUG 2025-01-13 01:32:38,634 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [184] queued (86.245 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:38,639 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (184) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:38,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 184
galaxy.jobs.command_factory INFO 2025-01-13 01:32:38,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/182/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/182/registry.xml' '/galaxy/server/database/jobs_directory/000/182/upload_params.json' '194:/galaxy/server/database/objects/5/0/8/dataset_508885b1-029c-4019-a708-06e7e93f9936_files:/galaxy/server/database/objects/5/0/8/dataset_508885b1-029c-4019-a708-06e7e93f9936.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:32:38,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (182) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/182/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/182/galaxy_182.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:32:38,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [183] prepared (145.187 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:32:38,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/183/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/183/registry.xml' '/galaxy/server/database/jobs_directory/000/183/upload_params.json' '195:/galaxy/server/database/objects/b/b/7/dataset_bb7d4fc4-0f7c-4262-ad32-406834b39e0f_files:/galaxy/server/database/objects/b/b/7/dataset_bb7d4fc4-0f7c-4262-ad32-406834b39e0f.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:32:38,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (183) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/183/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/183/galaxy_183.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:32:38,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [184] prepared (124.892 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:32:38,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/184/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/184/registry.xml' '/galaxy/server/database/jobs_directory/000/184/upload_params.json' '196:/galaxy/server/database/objects/3/a/6/dataset_3a67db80-0072-43a2-a3d8-5865951f34b7_files:/galaxy/server/database/objects/3/a/6/dataset_3a67db80-0072-43a2-a3d8-5865951f34b7.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:32:38,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (184) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/184/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/184/galaxy_184.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:32:39,643 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 188, 189, 187, 186, 185
tpv.core.entities DEBUG 2025-01-13 01:32:39,696 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:39,696 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:39,703 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:39,722 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:32:39,744 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Working directory for job is: /galaxy/server/database/jobs_directory/000/185
galaxy.jobs.runners DEBUG 2025-01-13 01:32:39,752 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [185] queued (49.064 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:39,757 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (185) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:39,762 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 185
tpv.core.entities DEBUG 2025-01-13 01:32:39,779 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:39,779 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:39,786 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:39,804 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:32:39,840 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Working directory for job is: /galaxy/server/database/jobs_directory/000/186
galaxy.jobs.runners DEBUG 2025-01-13 01:32:39,847 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [186] queued (61.353 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:39,851 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (186) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:39,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 186
tpv.core.entities DEBUG 2025-01-13 01:32:39,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:39,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:39,888 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:39,914 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:32:39,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [185] prepared (126.956 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:32:39,953 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/185/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/185/registry.xml' '/galaxy/server/database/jobs_directory/000/185/upload_params.json' '197:/galaxy/server/database/objects/e/2/4/dataset_e24ef712-bd68-471e-9a59-ed64254a662a_files:/galaxy/server/database/objects/e/2/4/dataset_e24ef712-bd68-471e-9a59-ed64254a662a.dat']
galaxy.jobs DEBUG 2025-01-13 01:32:39,959 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Working directory for job is: /galaxy/server/database/jobs_directory/000/187
galaxy.jobs.runners DEBUG 2025-01-13 01:32:39,970 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [187] queued (81.722 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:39,979 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (187) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:39,984 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 187
galaxy.jobs.runners DEBUG 2025-01-13 01:32:40,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (185) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/185/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/185/galaxy_185.ec; sh -c "exit $return_code"
tpv.core.entities DEBUG 2025-01-13 01:32:40,022 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:40,023 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:40,038 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:40,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [186] prepared (162.037 ms)
galaxy.jobs DEBUG 2025-01-13 01:32:40,077 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Persisting job destination (destination id: k8s)
galaxy.jobs.command_factory INFO 2025-01-13 01:32:40,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/186/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/186/registry.xml' '/galaxy/server/database/jobs_directory/000/186/upload_params.json' '198:/galaxy/server/database/objects/4/8/f/dataset_48f85d2a-fb29-43b5-9db1-f025e28bfea8_files:/galaxy/server/database/objects/4/8/f/dataset_48f85d2a-fb29-43b5-9db1-f025e28bfea8.dat']
galaxy.jobs DEBUG 2025-01-13 01:32:40,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Working directory for job is: /galaxy/server/database/jobs_directory/000/188
galaxy.jobs.runners DEBUG 2025-01-13 01:32:40,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (186) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/186/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/186/galaxy_186.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:32:40,136 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [188] queued (97.663 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:40,141 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (188) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:40,150 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 188
tpv.core.entities DEBUG 2025-01-13 01:32:40,175 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:40,176 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:40,185 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:40,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [187] prepared (183.503 ms)
galaxy.jobs DEBUG 2025-01-13 01:32:40,211 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Persisting job destination (destination id: k8s)
galaxy.jobs.command_factory INFO 2025-01-13 01:32:40,255 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/187/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/187/registry.xml' '/galaxy/server/database/jobs_directory/000/187/upload_params.json' '199:/galaxy/server/database/objects/a/1/4/dataset_a14edc93-eabd-48f8-87c9-fda3cc31cce4_files:/galaxy/server/database/objects/a/1/4/dataset_a14edc93-eabd-48f8-87c9-fda3cc31cce4.dat']
galaxy.jobs DEBUG 2025-01-13 01:32:40,256 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Working directory for job is: /galaxy/server/database/jobs_directory/000/189
galaxy.jobs.runners DEBUG 2025-01-13 01:32:40,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [189] queued (80.328 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:40,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (189) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:40,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 189
galaxy.jobs.runners DEBUG 2025-01-13 01:32:40,283 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (187) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/187/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/187/galaxy_187.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:32:40,351 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [188] prepared (157.268 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:32:40,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/188/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/188/registry.xml' '/galaxy/server/database/jobs_directory/000/188/upload_params.json' '200:/galaxy/server/database/objects/b/5/3/dataset_b53ac959-67f4-4249-b272-080a728a062f_files:/galaxy/server/database/objects/b/5/3/dataset_b53ac959-67f4-4249-b272-080a728a062f.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:32:40,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (188) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/188/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/188/galaxy_188.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:32:40,450 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [189] prepared (137.633 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:32:40,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/189/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/189/registry.xml' '/galaxy/server/database/jobs_directory/000/189/upload_params.json' '201:/galaxy/server/database/objects/a/8/3/dataset_a83d6ef7-14f2-4966-b7a4-744fba390e56_files:/galaxy/server/database/objects/a/8/3/dataset_a83d6ef7-14f2-4966-b7a4-744fba390e56.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:32:40,492 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (189) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/189/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/189/galaxy_189.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-13 01:32:41,274 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 190
tpv.core.entities DEBUG 2025-01-13 01:32:41,315 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:32:41,316 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:32:41,322 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:32:41,338 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:32:41,355 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Working directory for job is: /galaxy/server/database/jobs_directory/000/190
galaxy.jobs.runners DEBUG 2025-01-13 01:32:41,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [190] queued (41.046 ms)
galaxy.jobs.handler INFO 2025-01-13 01:32:41,369 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (190) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:32:41,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 190
galaxy.jobs DEBUG 2025-01-13 01:32:41,496 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [190] prepared (101.365 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:32:41,518 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/190/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/190/registry.xml' '/galaxy/server/database/jobs_directory/000/190/upload_params.json' '202:/galaxy/server/database/objects/0/6/3/dataset_06399377-6add-4e19-9983-bd6d08c31173_files:/galaxy/server/database/objects/0/6/3/dataset_06399377-6add-4e19-9983-bd6d08c31173.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:32:41,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (190) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/190/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/190/galaxy_190.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:32:47,560 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 178: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:32:47,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 179: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:32:47,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 183: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:32:49,148 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 180: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:32:57,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 179 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:32:57,577 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 178 finished
galaxy.model.metadata DEBUG 2025-01-13 01:32:57,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 190
galaxy.model.metadata DEBUG 2025-01-13 01:32:57,687 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 191
galaxy.jobs INFO 2025-01-13 01:32:57,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 178 in /galaxy/server/database/jobs_directory/000/178
galaxy.jobs INFO 2025-01-13 01:32:57,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 179 in /galaxy/server/database/jobs_directory/000/179
galaxy.jobs.runners DEBUG 2025-01-13 01:32:57,789 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 183 finished
galaxy.jobs DEBUG 2025-01-13 01:32:57,874 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 178 executed (229.379 ms)
galaxy.model.metadata DEBUG 2025-01-13 01:32:57,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 195
galaxy.jobs DEBUG 2025-01-13 01:32:57,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 179 executed (238.654 ms)
galaxy.jobs INFO 2025-01-13 01:32:57,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 183 in /galaxy/server/database/jobs_directory/000/183
galaxy.jobs DEBUG 2025-01-13 01:32:57,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 183 executed (152.343 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:32:58,284 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 181: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:32:58,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 184: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:32:58,440 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 182: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:32:58,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 180 finished
galaxy.model.metadata DEBUG 2025-01-13 01:32:58,859 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 192
galaxy.jobs INFO 2025-01-13 01:32:58,896 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 180 in /galaxy/server/database/jobs_directory/000/180
galaxy.jobs DEBUG 2025-01-13 01:32:58,995 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 180 executed (217.076 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:32:59,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 186: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:33:08,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 181 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:33:08,564 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 184 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:33:08,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 182 finished
galaxy.model.metadata DEBUG 2025-01-13 01:33:08,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 193
galaxy.model.metadata DEBUG 2025-01-13 01:33:08,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 196
galaxy.jobs INFO 2025-01-13 01:33:08,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 181 in /galaxy/server/database/jobs_directory/000/181
galaxy.jobs INFO 2025-01-13 01:33:08,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 184 in /galaxy/server/database/jobs_directory/000/184
galaxy.model.metadata DEBUG 2025-01-13 01:33:08,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 194
galaxy.jobs INFO 2025-01-13 01:33:08,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 182 in /galaxy/server/database/jobs_directory/000/182
galaxy.jobs DEBUG 2025-01-13 01:33:08,837 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 181 executed (250.975 ms)
galaxy.jobs DEBUG 2025-01-13 01:33:08,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 184 executed (259.046 ms)
galaxy.jobs DEBUG 2025-01-13 01:33:08,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 182 executed (194.411 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:33:09,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 188: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:33:09,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 189: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:33:09,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 186 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:33:09,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 185: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.model.metadata DEBUG 2025-01-13 01:33:09,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 198
galaxy.jobs INFO 2025-01-13 01:33:09,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 186 in /galaxy/server/database/jobs_directory/000/186
galaxy.jobs DEBUG 2025-01-13 01:33:09,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 186 executed (260.148 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:33:09,881 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 187: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:33:19,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 188 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:33:19,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 189 finished
galaxy.model.metadata DEBUG 2025-01-13 01:33:19,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 200
galaxy.model.metadata DEBUG 2025-01-13 01:33:19,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 201
galaxy.jobs INFO 2025-01-13 01:33:19,501 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 188 in /galaxy/server/database/jobs_directory/000/188
galaxy.jobs.runners DEBUG 2025-01-13 01:33:19,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 185 finished
galaxy.jobs INFO 2025-01-13 01:33:19,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 189 in /galaxy/server/database/jobs_directory/000/189
galaxy.jobs DEBUG 2025-01-13 01:33:19,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 188 executed (250.897 ms)
galaxy.model.metadata DEBUG 2025-01-13 01:33:19,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 197
galaxy.jobs DEBUG 2025-01-13 01:33:19,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 189 executed (226.785 ms)
galaxy.jobs INFO 2025-01-13 01:33:19,696 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 185 in /galaxy/server/database/jobs_directory/000/185
galaxy.jobs DEBUG 2025-01-13 01:33:19,770 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 185 executed (175.628 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:33:19,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 190: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:33:19,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 187 finished
galaxy.model.metadata DEBUG 2025-01-13 01:33:20,009 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 199
galaxy.jobs INFO 2025-01-13 01:33:20,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 187 in /galaxy/server/database/jobs_directory/000/187
galaxy.jobs DEBUG 2025-01-13 01:33:20,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 187 executed (131.260 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:33:24,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 190 finished
galaxy.model.metadata DEBUG 2025-01-13 01:33:24,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 202
galaxy.jobs INFO 2025-01-13 01:33:24,779 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 190 in /galaxy/server/database/jobs_directory/000/190
galaxy.jobs DEBUG 2025-01-13 01:33:24,833 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 190 executed (122.578 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:33:25,603 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 191
tpv.core.entities DEBUG 2025-01-13 01:33:25,661 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/.*, abstract=False, cores=1, mem=4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:33:25,662 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:33:25,668 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:33:25,688 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:33:25,710 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Working directory for job is: /galaxy/server/database/jobs_directory/000/191
galaxy.jobs.runners DEBUG 2025-01-13 01:33:25,723 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [191] queued (54.008 ms)
galaxy.jobs.handler INFO 2025-01-13 01:33:25,728 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (191) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:33:25,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 191
galaxy.jobs DEBUG 2025-01-13 01:33:25,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [191] prepared (98.835 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:33:25,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:33:25,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:33:26,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:33:26,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/191/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", statmod version" $(R --vanilla --slave -e "library(statmod); cat(sessionInfo()\$otherPkgs\$statmod\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", gplots version" $(R --vanilla --slave -e "library(gplots); cat(sessionInfo()\$otherPkgs\$gplots\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", Glimma version" $(R --vanilla --slave -e "library(Glimma); cat(sessionInfo()\$otherPkgs\$Glimma\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/191/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/119b069fc845/limma_voom/limma_voom.R'  -R '/galaxy/server/database/objects/9/7/9/dataset_9790a65b-6918-406e-adc2-c44eaf9b65be.dat' -o '/galaxy/server/database/objects/9/7/9/dataset_9790a65b-6918-406e-adc2-c44eaf9b65be_files'                          -j '[["Genotype", [{"Mut": ["/galaxy/server/database/objects/e/2/4/dataset_e24ef712-bd68-471e-9a59-ed64254a662a.dat", "/galaxy/server/database/objects/a/1/4/dataset_a14edc93-eabd-48f8-87c9-fda3cc31cce4.dat", "/galaxy/server/database/objects/a/8/3/dataset_a83d6ef7-14f2-4966-b7a4-744fba390e56.dat"]}, {"WT": ["/galaxy/server/database/objects/3/a/6/dataset_3a67db80-0072-43a2-a3d8-5865951f34b7.dat", "/galaxy/server/database/objects/4/8/f/dataset_48f85d2a-fb29-43b5-9db1-f025e28bfea8.dat", "/galaxy/server/database/objects/b/5/3/dataset_b53ac959-67f4-4249-b272-080a728a062f.dat"]}]], ["Batch", [{"b3": ["/galaxy/server/database/objects/b/5/3/dataset_b53ac959-67f4-4249-b272-080a728a062f.dat", "/galaxy/server/database/objects/a/8/3/dataset_a83d6ef7-14f2-4966-b7a4-744fba390e56.dat"]}, {"b2": ["/galaxy/server/database/objects/4/8/f/dataset_48f85d2a-fb29-43b5-9db1-f025e28bfea8.dat", "/galaxy/server/database/objects/a/1/4/dataset_a14edc93-eabd-48f8-87c9-fda3cc31cce4.dat"]}, {"b1": ["/galaxy/server/database/objects/3/a/6/dataset_3a67db80-0072-43a2-a3d8-5865951f34b7.dat", "/galaxy/server/database/objects/e/2/4/dataset_e24ef712-bd68-471e-9a59-ed64254a662a.dat"]}]]]'   -a '/galaxy/server/database/objects/0/6/3/dataset_06399377-6add-4e19-9983-bd6d08c31173.dat'  -D 'Mut-WT,WT-Mut'   -P i   -x     -l '0.0' -p '0.05' -d 'BH' -G '6'   -n 'TMM'  -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/9/7/9/dataset_9790a65b-6918-406e-adc2-c44eaf9b65be_files'/*tsv output_dir/  && cp -r ./glimma* '/galaxy/server/database/objects/9/7/9/dataset_9790a65b-6918-406e-adc2-c44eaf9b65be_files'  && cp '/galaxy/server/database/objects/9/7/9/dataset_9790a65b-6918-406e-adc2-c44eaf9b65be_files'/*counts output_dir/]
galaxy.jobs.runners DEBUG 2025-01-13 01:33:26,089 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (191) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/191/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/191/galaxy_191.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/191/working/output_dir/"*"_normcounts" ] ; then cp "/galaxy/server/database/jobs_directory/000/191/working/output_dir/"*"_normcounts" "/galaxy/server/database/objects/6/4/5/dataset_6455cf80-60b2-4985-996d-8ff7fdde221b.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:33:26,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:33:26,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:33:26,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:33:52,244 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 191: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:33:57,000 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 191 finished
galaxy.model.store.discover DEBUG 2025-01-13 01:33:57,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (191) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/191/working/output_dir/limma-voom_Mut-WT.tsv] with element identifier [limma-voom_Mut-WT] for output [outTables] (14.640 ms)
galaxy.model.store.discover DEBUG 2025-01-13 01:33:57,074 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (191) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/191/working/output_dir/limma-voom_WT-Mut.tsv] with element identifier [limma-voom_WT-Mut] for output [outTables] (0.608 ms)
galaxy.model.store.discover DEBUG 2025-01-13 01:33:57,102 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (191) Add dynamic collection datasets to history for output [outTables] (27.932 ms)
galaxy.model.metadata DEBUG 2025-01-13 01:33:57,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 203
galaxy.model.metadata DEBUG 2025-01-13 01:33:57,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 204
galaxy.util WARNING 2025-01-13 01:33:57,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/4/5/dataset_6455cf80-60b2-4985-996d-8ff7fdde221b.dat, group remains grp.struct_group(gr_name='nogroup', gr_passwd='x', gr_gid=65534, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/4/5/dataset_6455cf80-60b2-4985-996d-8ff7fdde221b.dat'
galaxy.jobs INFO 2025-01-13 01:33:57,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 191 in /galaxy/server/database/jobs_directory/000/191
galaxy.objectstore CRITICAL 2025-01-13 01:33:57,316 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] None delete error [Errno 13] Permission denied: 'limma-voom_WT-Mut.tsv'
galaxy.jobs DEBUG 2025-01-13 01:33:57,317 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 191 executed (287.091 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:33:59,487 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 192
tpv.core.entities DEBUG 2025-01-13 01:33:59,524 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:33:59,524 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:33:59,530 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:33:59,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:33:59,568 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Working directory for job is: /galaxy/server/database/jobs_directory/000/192
galaxy.jobs.runners DEBUG 2025-01-13 01:33:59,575 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [192] queued (45.484 ms)
galaxy.jobs.handler INFO 2025-01-13 01:33:59,579 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (192) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:33:59,582 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 192
galaxy.jobs DEBUG 2025-01-13 01:33:59,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [192] prepared (88.253 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:33:59,711 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/192/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/192/registry.xml' '/galaxy/server/database/jobs_directory/000/192/upload_params.json' '207:/galaxy/server/database/objects/6/6/4/dataset_664aa755-1b71-4848-9fe6-88cff80bd293_files:/galaxy/server/database/objects/6/6/4/dataset_664aa755-1b71-4848-9fe6-88cff80bd293.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:33:59,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (192) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/192/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/192/galaxy_192.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:34:08,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 192: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:34:13,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 192 finished
galaxy.model.metadata DEBUG 2025-01-13 01:34:13,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 207
galaxy.jobs INFO 2025-01-13 01:34:13,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 192 in /galaxy/server/database/jobs_directory/000/192
galaxy.jobs DEBUG 2025-01-13 01:34:13,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 192 executed (118.406 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:34:13,880 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 193
tpv.core.entities DEBUG 2025-01-13 01:34:13,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/.*, abstract=False, cores=1, mem=4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:34:13,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:34:13,926 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:34:13,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:34:13,964 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Working directory for job is: /galaxy/server/database/jobs_directory/000/193
galaxy.jobs.runners DEBUG 2025-01-13 01:34:13,973 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [193] queued (46.582 ms)
galaxy.jobs.handler INFO 2025-01-13 01:34:13,977 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (193) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:34:13,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 193
galaxy.jobs DEBUG 2025-01-13 01:34:14,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [193] prepared (72.643 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:34:14,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:34:14,073 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:34:14,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:34:14,116 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/193/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", statmod version" $(R --vanilla --slave -e "library(statmod); cat(sessionInfo()\$otherPkgs\$statmod\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", gplots version" $(R --vanilla --slave -e "library(gplots); cat(sessionInfo()\$otherPkgs\$gplots\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", Glimma version" $(R --vanilla --slave -e "library(Glimma); cat(sessionInfo()\$otherPkgs\$Glimma\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/193/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/119b069fc845/limma_voom/limma_voom.R'  -R '/galaxy/server/database/objects/d/3/0/dataset_d30eec29-a293-41ba-8118-835b292d8fec.dat' -o '/galaxy/server/database/objects/d/3/0/dataset_d30eec29-a293-41ba-8118-835b292d8fec_files'  -m '/galaxy/server/database/objects/6/6/4/dataset_664aa755-1b71-4848-9fe6-88cff80bd293.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'   -D 'Mut-WT'   -P i       -l '0.0' -p '0.05' -d 'BH' -G '6'  -t 3.0  -n 'TMM'  -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/d/3/0/dataset_d30eec29-a293-41ba-8118-835b292d8fec_files'/*tsv output_dir/  && cp -r ./glimma* '/galaxy/server/database/objects/d/3/0/dataset_d30eec29-a293-41ba-8118-835b292d8fec_files']
galaxy.jobs.runners DEBUG 2025-01-13 01:34:14,130 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (193) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/193/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/193/galaxy_193.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:34:14,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:34:14,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:34:14,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.runners.kubernetes ERROR 2025-01-13 01:34:35,709 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://10.100.128.1:443/apis/batch/v1/namespaces/prod-25-01-13-00-39-1/jobs/gxy-g8ngm

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 817, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 824, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 107, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-g8ngm": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:34:35,745 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (193/gxy-g8ngm) Terminated at user's request
galaxy.objectstore CRITICAL 2025-01-13 01:34:35,832 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] None delete error [Errno 13] Permission denied: 'limma-trend_Mut-WT.tsv'
galaxy.jobs.handler DEBUG 2025-01-13 01:34:37,472 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 195, 194
tpv.core.entities DEBUG 2025-01-13 01:34:37,503 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:34:37,504 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:34:37,508 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:34:37,524 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:34:37,542 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Working directory for job is: /galaxy/server/database/jobs_directory/000/194
galaxy.jobs.runners DEBUG 2025-01-13 01:34:37,550 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [194] queued (41.990 ms)
galaxy.jobs.handler INFO 2025-01-13 01:34:37,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (194) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:34:37,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 194
tpv.core.entities DEBUG 2025-01-13 01:34:37,578 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:34:37,578 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:34:37,586 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:34:37,608 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:34:37,636 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Working directory for job is: /galaxy/server/database/jobs_directory/000/195
galaxy.jobs.runners DEBUG 2025-01-13 01:34:37,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [195] queued (59.142 ms)
galaxy.jobs.handler INFO 2025-01-13 01:34:37,650 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (195) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:34:37,654 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 195
galaxy.jobs DEBUG 2025-01-13 01:34:37,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [194] prepared (104.252 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:34:37,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/194/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/194/registry.xml' '/galaxy/server/database/jobs_directory/000/194/upload_params.json' '209:/galaxy/server/database/objects/6/6/e/dataset_66e8010f-1651-407e-aa88-3377da6cab5a_files:/galaxy/server/database/objects/6/6/e/dataset_66e8010f-1651-407e-aa88-3377da6cab5a.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:34:37,733 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (194) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/194/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/194/galaxy_194.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:34:37,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [195] prepared (108.041 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:34:37,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/195/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/195/registry.xml' '/galaxy/server/database/jobs_directory/000/195/upload_params.json' '210:/galaxy/server/database/objects/e/3/3/dataset_e33723aa-e59c-4bf2-bba1-d315fc4640c8_files:/galaxy/server/database/objects/e/3/3/dataset_e33723aa-e59c-4bf2-bba1-d315fc4640c8.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:34:37,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (195) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/195/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/195/galaxy_195.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:34:47,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 194: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:34:47,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 195: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:34:52,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 194 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:34:52,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 195 finished
galaxy.model.metadata DEBUG 2025-01-13 01:34:52,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 209
galaxy.jobs INFO 2025-01-13 01:34:52,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 194 in /galaxy/server/database/jobs_directory/000/194
galaxy.model.metadata DEBUG 2025-01-13 01:34:52,412 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 210
galaxy.jobs INFO 2025-01-13 01:34:52,457 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 195 in /galaxy/server/database/jobs_directory/000/195
galaxy.jobs DEBUG 2025-01-13 01:34:52,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 194 executed (152.729 ms)
galaxy.jobs DEBUG 2025-01-13 01:34:52,520 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 195 executed (152.333 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:34:52,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 196
tpv.core.entities DEBUG 2025-01-13 01:34:53,035 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/.*, abstract=False, cores=1, mem=4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:34:53,035 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:34:53,041 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:34:53,059 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:34:53,077 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Working directory for job is: /galaxy/server/database/jobs_directory/000/196
galaxy.jobs.runners DEBUG 2025-01-13 01:34:53,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [196] queued (45.394 ms)
galaxy.jobs.handler INFO 2025-01-13 01:34:53,091 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (196) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:34:53,095 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 196
galaxy.jobs DEBUG 2025-01-13 01:34:53,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [196] prepared (71.841 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:34:53,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:34:53,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:34:53,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:34:53,232 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/196/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", statmod version" $(R --vanilla --slave -e "library(statmod); cat(sessionInfo()\$otherPkgs\$statmod\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", gplots version" $(R --vanilla --slave -e "library(gplots); cat(sessionInfo()\$otherPkgs\$gplots\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", Glimma version" $(R --vanilla --slave -e "library(Glimma); cat(sessionInfo()\$otherPkgs\$Glimma\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/196/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/119b069fc845/limma_voom/limma_voom.R'  -R '/galaxy/server/database/objects/6/6/6/dataset_6669f230-b06a-44c5-85ce-0de04075d6a0.dat' -o '/galaxy/server/database/objects/6/6/6/dataset_6669f230-b06a-44c5-85ce-0de04075d6a0_files'  -m '/galaxy/server/database/objects/6/6/e/dataset_66e8010f-1651-407e-aa88-3377da6cab5a.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'  -a '/galaxy/server/database/objects/e/3/3/dataset_e33723aa-e59c-4bf2-bba1-d315fc4640c8.dat'  -D 'Mut-WT'   -P i       -l '0.0' -p '0.05' -d 'BH' -G '6'  -t 3.0  -n 'TMM'  -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/6/6/6/dataset_6669f230-b06a-44c5-85ce-0de04075d6a0_files'/*tsv output_dir/  && cp -r ./glimma* '/galaxy/server/database/objects/6/6/6/dataset_6669f230-b06a-44c5-85ce-0de04075d6a0_files']
galaxy.jobs.runners DEBUG 2025-01-13 01:34:53,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (196) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/196/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/196/galaxy_196.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:34:53,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:34:53,261 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:34:53,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:35:16,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 196: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:35:21,543 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 196 finished
galaxy.model.store.discover DEBUG 2025-01-13 01:35:21,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (196) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/196/working/output_dir/limma-trend_Mut-WT.tsv] with element identifier [limma-trend_Mut-WT] for output [outTables] (6.420 ms)
galaxy.model.store.discover DEBUG 2025-01-13 01:35:21,621 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (196) Add dynamic collection datasets to history for output [outTables] (15.571 ms)
galaxy.model.metadata DEBUG 2025-01-13 01:35:21,670 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 211
galaxy.jobs INFO 2025-01-13 01:35:21,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 196 in /galaxy/server/database/jobs_directory/000/196
galaxy.objectstore CRITICAL 2025-01-13 01:35:21,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] None delete error [Errno 13] Permission denied: 'limma-trend_Mut-WT.tsv'
galaxy.jobs DEBUG 2025-01-13 01:35:21,765 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 196 executed (194.234 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:35:23,766 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 198, 197
tpv.core.entities DEBUG 2025-01-13 01:35:23,804 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:35:23,805 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:35:23,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:35:23,828 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:35:23,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Working directory for job is: /galaxy/server/database/jobs_directory/000/197
galaxy.jobs.runners DEBUG 2025-01-13 01:35:23,855 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [197] queued (44.564 ms)
galaxy.jobs.handler INFO 2025-01-13 01:35:23,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (197) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:35:23,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 197
tpv.core.entities DEBUG 2025-01-13 01:35:23,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:35:23,878 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:35:23,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:35:23,903 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:35:23,932 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Working directory for job is: /galaxy/server/database/jobs_directory/000/198
galaxy.jobs.runners DEBUG 2025-01-13 01:35:23,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [198] queued (57.325 ms)
galaxy.jobs.handler INFO 2025-01-13 01:35:23,946 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (198) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:35:23,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 198
galaxy.jobs DEBUG 2025-01-13 01:35:23,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [197] prepared (108.873 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:35:24,029 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/197/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/197/registry.xml' '/galaxy/server/database/jobs_directory/000/197/upload_params.json' '213:/galaxy/server/database/objects/1/9/1/dataset_191a4cd5-74da-46de-a22b-8bf178e5d39f_files:/galaxy/server/database/objects/1/9/1/dataset_191a4cd5-74da-46de-a22b-8bf178e5d39f.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:35:24,048 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (197) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/197/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/197/galaxy_197.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:35:24,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [198] prepared (107.995 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:35:24,109 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/198/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/198/registry.xml' '/galaxy/server/database/jobs_directory/000/198/upload_params.json' '214:/galaxy/server/database/objects/6/5/5/dataset_6552eedb-b5a8-4dee-8cc5-2f7af30c460d_files:/galaxy/server/database/objects/6/5/5/dataset_6552eedb-b5a8-4dee-8cc5-2f7af30c460d.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:35:24,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (198) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/198/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/198/galaxy_198.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes ERROR 2025-01-13 01:35:33,007 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://10.100.128.1:443/apis/batch/v1/namespaces/prod-25-01-13-00-39-1/jobs/gxy-5c9j7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 817, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 824, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 107, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-5c9j7": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:35:33,072 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (197/gxy-5c9j7) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-01-13 01:35:33,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 198: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-01-13 01:35:34,241 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 199, 200
tpv.core.entities DEBUG 2025-01-13 01:35:34,278 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:35:34,278 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:35:34,282 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:35:34,295 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:35:34,314 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Working directory for job is: /galaxy/server/database/jobs_directory/000/199
galaxy.jobs.runners DEBUG 2025-01-13 01:35:34,321 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [199] queued (38.968 ms)
galaxy.jobs.handler INFO 2025-01-13 01:35:34,325 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (199) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:35:34,329 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 199
tpv.core.entities DEBUG 2025-01-13 01:35:34,343 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:35:34,344 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:35:34,351 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:35:34,370 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:35:34,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Working directory for job is: /galaxy/server/database/jobs_directory/000/200
galaxy.jobs.runners DEBUG 2025-01-13 01:35:34,410 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [200] queued (58.202 ms)
galaxy.jobs.handler INFO 2025-01-13 01:35:34,416 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (200) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:35:34,421 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 200
galaxy.jobs DEBUG 2025-01-13 01:35:34,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [199] prepared (104.352 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:35:34,485 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/199/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/199/registry.xml' '/galaxy/server/database/jobs_directory/000/199/upload_params.json' '215:/galaxy/server/database/objects/d/9/9/dataset_d999b1ee-4e99-4973-882c-6e4bef840f41_files:/galaxy/server/database/objects/d/9/9/dataset_d999b1ee-4e99-4973-882c-6e4bef840f41.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:35:34,498 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (199) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/199/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/199/galaxy_199.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-13 01:35:34,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [200] prepared (91.080 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:35:34,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/200/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/200/registry.xml' '/galaxy/server/database/jobs_directory/000/200/upload_params.json' '216:/galaxy/server/database/objects/f/8/3/dataset_f8365ce1-94e2-45b4-a126-50985547705d_files:/galaxy/server/database/objects/f/8/3/dataset_f8365ce1-94e2-45b4-a126-50985547705d.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:35:34,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (200) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/200/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/200/galaxy_200.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:35:38,140 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 198 finished
galaxy.model.metadata DEBUG 2025-01-13 01:35:38,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 214
galaxy.jobs INFO 2025-01-13 01:35:38,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 198 in /galaxy/server/database/jobs_directory/000/198
galaxy.jobs DEBUG 2025-01-13 01:35:38,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 198 executed (134.564 ms)
galaxy.jobs.runners DEBUG 2025-01-13 01:35:43,675 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 199: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:35:43,697 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 200: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:35:48,901 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 199 finished
galaxy.jobs.runners DEBUG 2025-01-13 01:35:48,931 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 200 finished
galaxy.model.metadata DEBUG 2025-01-13 01:35:48,964 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 215
galaxy.model.metadata DEBUG 2025-01-13 01:35:49,010 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 216
galaxy.jobs INFO 2025-01-13 01:35:49,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 199 in /galaxy/server/database/jobs_directory/000/199
galaxy.jobs INFO 2025-01-13 01:35:49,052 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 200 in /galaxy/server/database/jobs_directory/000/200
galaxy.jobs DEBUG 2025-01-13 01:35:49,082 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 199 executed (154.329 ms)
galaxy.jobs DEBUG 2025-01-13 01:35:49,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 200 executed (149.422 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:35:49,998 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 201
tpv.core.entities DEBUG 2025-01-13 01:35:50,042 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/.*, abstract=False, cores=1, mem=4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:35:50,043 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:35:50,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:35:50,066 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:35:50,087 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Working directory for job is: /galaxy/server/database/jobs_directory/000/201
galaxy.jobs.runners DEBUG 2025-01-13 01:35:50,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [201] queued (48.551 ms)
galaxy.jobs.handler INFO 2025-01-13 01:35:50,100 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (201) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:35:50,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 201
galaxy.jobs DEBUG 2025-01-13 01:35:50,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [201] prepared (73.932 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:35:50,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:35:50,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:35:50,220 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:35:50,244 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/201/tool_script.sh] for tool command [echo $(R --version | grep version | grep -v GNU)", limma version" $(R --vanilla --slave -e "library(limma); cat(sessionInfo()\$otherPkgs\$limma\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", edgeR version" $(R --vanilla --slave -e "library(edgeR); cat(sessionInfo()\$otherPkgs\$edgeR\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", statmod version" $(R --vanilla --slave -e "library(statmod); cat(sessionInfo()\$otherPkgs\$statmod\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", scales version" $(R --vanilla --slave -e "library(scales); cat(sessionInfo()\$otherPkgs\$scales\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", rjson version" $(R --vanilla --slave -e "library(rjson); cat(sessionInfo()\$otherPkgs\$rjson\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", getopt version" $(R --vanilla --slave -e "library(getopt); cat(sessionInfo()\$otherPkgs\$getopt\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", gplots version" $(R --vanilla --slave -e "library(gplots); cat(sessionInfo()\$otherPkgs\$gplots\$Version)" 2> /dev/null | grep -v -i "WARNING: ")", Glimma version" $(R --vanilla --slave -e "library(Glimma); cat(sessionInfo()\$otherPkgs\$Glimma\$Version)" 2> /dev/null | grep -v -i "WARNING: ") > /galaxy/server/database/jobs_directory/000/201/outputs/COMMAND_VERSION 2>&1;
Rscript '/cvmfs/cloud.galaxyproject.org/tools/toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/119b069fc845/limma_voom/limma_voom.R'  -R '/galaxy/server/database/objects/5/d/8/dataset_5d8bb7e4-99cc-4439-b8f7-3f2b0b85963b.dat' -o '/galaxy/server/database/objects/5/d/8/dataset_5d8bb7e4-99cc-4439-b8f7-3f2b0b85963b_files'  -m '/galaxy/server/database/objects/d/9/9/dataset_d999b1ee-4e99-4973-882c-6e4bef840f41.dat' -i 'Genotype::Mut,Mut,Mut,WT,WT,WT'   -C '/galaxy/server/database/objects/f/8/3/dataset_f8365ce1-94e2-45b4-a126-50985547705d.dat'   -P i       -l '0.0' -p '0.05' -d 'BH' -G '6'   -n 'TMM'  -b  && mkdir ./output_dir  && cp '/galaxy/server/database/objects/5/d/8/dataset_5d8bb7e4-99cc-4439-b8f7-3f2b0b85963b_files'/*tsv output_dir/  && cp -r ./glimma* '/galaxy/server/database/objects/5/d/8/dataset_5d8bb7e4-99cc-4439-b8f7-3f2b0b85963b_files']
galaxy.jobs.runners DEBUG 2025-01-13 01:35:50,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (201) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/201/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/201/galaxy_201.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:35:50,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:35:50,274 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/limma_voom/limma_voom/3.58.1+galaxy0: mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67
galaxy.tool_util.deps.containers INFO 2025-01-13 01:35:50,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3d571fed05a48eb8af17dbc6c8ed632143702ac1:c5763bb5f3402fc75fc6c0fa6c38d545d78e0e67-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:36:14,168 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 201: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:36:19,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 201 finished
galaxy.model.store.discover DEBUG 2025-01-13 01:36:19,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (201) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/201/working/output_dir/limma-voom_Mut-WT-WT-Mut.tsv] with element identifier [limma-voom_Mut-WT-WT-Mut] for output [outTables] (6.896 ms)
galaxy.model.store.discover DEBUG 2025-01-13 01:36:19,111 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (201) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/201/working/output_dir/limma-voom_Mut-WT.tsv] with element identifier [limma-voom_Mut-WT] for output [outTables] (0.640 ms)
galaxy.model.store.discover DEBUG 2025-01-13 01:36:19,112 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (201) Created dynamic collection dataset for path [/galaxy/server/database/jobs_directory/000/201/working/output_dir/limma-voom_WT-Mut.tsv] with element identifier [limma-voom_WT-Mut] for output [outTables] (0.447 ms)
galaxy.model.store.discover DEBUG 2025-01-13 01:36:19,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (201) Add dynamic collection datasets to history for output [outTables] (37.098 ms)
galaxy.model.metadata DEBUG 2025-01-13 01:36:19,195 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 217
galaxy.jobs INFO 2025-01-13 01:36:19,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 201 in /galaxy/server/database/jobs_directory/000/201
galaxy.objectstore CRITICAL 2025-01-13 01:36:19,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] None delete error [Errno 13] Permission denied: 'limma-voom_WT-Mut.tsv'
galaxy.jobs DEBUG 2025-01-13 01:36:19,289 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 201 executed (214.882 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:36:21,804 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 202
tpv.core.entities DEBUG 2025-01-13 01:36:21,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:36:21,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:36:21,848 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:36:21,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:36:21,887 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Working directory for job is: /galaxy/server/database/jobs_directory/000/202
galaxy.jobs.runners DEBUG 2025-01-13 01:36:21,893 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [202] queued (45.489 ms)
galaxy.jobs.handler INFO 2025-01-13 01:36:21,897 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (202) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:36:21,902 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 202
galaxy.jobs DEBUG 2025-01-13 01:36:22,019 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [202] prepared (95.645 ms)
galaxy.jobs.command_factory INFO 2025-01-13 01:36:22,043 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/202/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/202/registry.xml' '/galaxy/server/database/jobs_directory/000/202/upload_params.json' '221:/galaxy/server/database/objects/f/a/8/dataset_fa8bf21e-f419-4fb7-8c24-e00fcc38c80a_files:/galaxy/server/database/objects/f/a/8/dataset_fa8bf21e-f419-4fb7-8c24-e00fcc38c80a.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:36:22,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (202) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/202/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/202/galaxy_202.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-13 01:36:31,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 202: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:36:36,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 202 finished
galaxy.model.metadata DEBUG 2025-01-13 01:36:36,094 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 221
galaxy.jobs INFO 2025-01-13 01:36:36,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 202 in /galaxy/server/database/jobs_directory/000/202
galaxy.jobs DEBUG 2025-01-13 01:36:36,199 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 202 executed (138.881 ms)
galaxy.jobs.handler DEBUG 2025-01-13 01:36:37,243 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 203
tpv.core.entities DEBUG 2025-01-13 01:36:37,285 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-13 01:36:37,286 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-13 01:36:37,292 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-13 01:36:37,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-13 01:36:37,328 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Working directory for job is: /galaxy/server/database/jobs_directory/000/203
galaxy.jobs.runners DEBUG 2025-01-13 01:36:37,337 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [203] queued (44.565 ms)
galaxy.jobs.handler INFO 2025-01-13 01:36:37,342 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (203) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-13 01:36:37,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 203
galaxy.jobs DEBUG 2025-01-13 01:36:37,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [203] prepared (65.006 ms)
galaxy.tool_util.deps.containers INFO 2025-01-13 01:36:37,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:36:37,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_delta_filter/mummer_delta_filter/4.0.0rc1+galaxy3: mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5
galaxy.tool_util.deps.containers INFO 2025-01-13 01:36:37,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-13 01:36:37,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/203/tool_script.sh] for tool command [delta-filter -m -i '0.0' -l '0' -q -u '0.0' -o '100.0' '/galaxy/server/database/objects/f/a/8/dataset_fa8bf21e-f419-4fb7-8c24-e00fcc38c80a.dat' > '/galaxy/server/database/objects/7/e/8/dataset_7e8df703-6e81-4640-bdb6-ba37f78d640a.dat']
galaxy.jobs.runners DEBUG 2025-01-13 01:36:37,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (203) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/203/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/203/galaxy_203.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/203/working/delta-filter.txt" ] ; then cp "/galaxy/server/database/jobs_directory/000/203/working/delta-filter.txt" "/galaxy/server/database/objects/7/e/8/dataset_7e8df703-6e81-4640-bdb6-ba37f78d640a.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-13 01:36:37,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-13 01:36:37,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_delta_filter/mummer_delta_filter/4.0.0rc1+galaxy3: mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5
galaxy.tool_util.deps.containers INFO 2025-01-13 01:36:37,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-94c12e128fff0e4d1d34a43f778cdcdbfaba4960:f01f2bb89bd4c09e7ac156d4371df2eb495534e5-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-13 01:36:47,630 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 203: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-13 01:36:52,346 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 203 finished
galaxy.model.metadata DEBUG 2025-01-13 01:36:52,409 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 222
galaxy.jobs INFO 2025-01-13 01:36:52,452 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 203 in /galaxy/server/database/jobs_directory/000/203
galaxy.jobs DEBUG 2025-01-13 01:36:52,502 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 203 executed (126.271 ms)
