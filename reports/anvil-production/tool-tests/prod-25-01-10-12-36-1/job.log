galaxy.jobs.runners DEBUG 2025-01-10 13:01:27,309 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 33: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:01:32,181 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 33 finished
galaxy.model.metadata DEBUG 2025-01-10 13:01:32,246 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 44
galaxy.model.metadata DEBUG 2025-01-10 13:01:32,258 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 45
galaxy.util WARNING 2025-01-10 13:01:32,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/e/f/dataset_aefbcae6-aa99-4291-9509-24193d3ab73e.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/e/f/dataset_aefbcae6-aa99-4291-9509-24193d3ab73e.dat'
galaxy.util WARNING 2025-01-10 13:01:32,266 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/5/d/3/dataset_5d3ec4b2-773d-4248-8ee1-664b0ff2aec6.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/5/d/3/dataset_5d3ec4b2-773d-4248-8ee1-664b0ff2aec6.dat'
galaxy.jobs INFO 2025-01-10 13:01:32,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 33 in /galaxy/server/database/jobs_directory/000/33
galaxy.objectstore CRITICAL 2025-01-10 13:01:32,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] None delete error [Errno 13] Permission denied: 'dat.test.tab'
galaxy.jobs DEBUG 2025-01-10 13:01:32,321 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 33 executed (111.903 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:01:33,967 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 34
tpv.core.entities DEBUG 2025-01-10 13:01:34,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:01:34,004 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:01:34,010 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:01:34,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:01:34,043 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Working directory for job is: /galaxy/server/database/jobs_directory/000/34
galaxy.jobs.runners DEBUG 2025-01-10 13:01:34,050 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [34] queued (39.839 ms)
galaxy.jobs.handler INFO 2025-01-10 13:01:34,053 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (34) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:01:34,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 34
galaxy.jobs DEBUG 2025-01-10 13:01:34,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [34] prepared (83.159 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:01:34,180 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/34/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/34/registry.xml' '/galaxy/server/database/jobs_directory/000/34/upload_params.json' '46:/galaxy/server/database/objects/5/9/b/dataset_59b18e1f-36dd-4ad1-b08a-532e6e2ed3f0_files:/galaxy/server/database/objects/5/9/b/dataset_59b18e1f-36dd-4ad1-b08a-532e6e2ed3f0.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:01:34,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (34) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/34/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/34/galaxy_34.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:01:42,437 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 34: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:01:47,412 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 34 finished
galaxy.model.metadata DEBUG 2025-01-10 13:01:47,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 46
galaxy.jobs INFO 2025-01-10 13:01:47,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 34 in /galaxy/server/database/jobs_directory/000/34
galaxy.jobs DEBUG 2025-01-10 13:01:47,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 34 executed (114.039 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:01:48,356 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 35
tpv.core.entities DEBUG 2025-01-10 13:01:48,389 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:01:48,390 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:01:48,394 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:01:48,409 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:01:48,436 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Working directory for job is: /galaxy/server/database/jobs_directory/000/35
galaxy.jobs.runners DEBUG 2025-01-10 13:01:48,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [35] queued (50.761 ms)
galaxy.jobs.handler INFO 2025-01-10 13:01:48,450 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (35) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:01:48,454 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 35
galaxy.jobs DEBUG 2025-01-10 13:01:48,544 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [35] prepared (69.420 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:01:48,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:01:48,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-01-10 13:01:48,568 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:01:48,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/35/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/35/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/5/9/b/dataset_59b18e1f-36dd-4ad1-b08a-532e6e2ed3f0.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools cnv  --output-dir cnv_tmp  -c 'test' -s 'test'   -p 0  --aberrant "1.0,1.0" --BAF-weight 1.0 --BAF-dev "0.04,0.04" --LRR-weight 0.2 --LRR-dev "0.2,0.2" --LRR-smooth-win 10 --same-prob 0.5 --err-prob 0.0001 --xy-prob 1e-09             input.vcf.gz   && mv cnv_tmp/cn.*.tab '/galaxy/server/database/objects/6/1/d/dataset_61d0c8f0-dc61-4665-b94a-cfc4857bf678.dat' && mv cnv_tmp/summary.tab '/galaxy/server/database/objects/f/9/a/dataset_f9aa8a5a-3026-4a2f-951f-cfc5648768b6.dat'  && (echo '<html><body><head><title>Copy-number variation plots (bcftools cnv)</title><style type="text/css"> @media print { img { max-width:100% !important; page-break-inside: avoid; } </style>' > /galaxy/server/database/objects/a/1/9/dataset_a1988671-3422-4df4-ae2e-2ecb98798a5a.dat; for plot in cnv_tmp/*.png; do [ -f "$plot" ] || break; echo '<div><img src="data:image/png;base64,' >> /galaxy/server/database/objects/a/1/9/dataset_a1988671-3422-4df4-ae2e-2ecb98798a5a.dat; python -m base64 $plot >> /galaxy/server/database/objects/a/1/9/dataset_a1988671-3422-4df4-ae2e-2ecb98798a5a.dat; echo '" /></div><hr>' >> /galaxy/server/database/objects/a/1/9/dataset_a1988671-3422-4df4-ae2e-2ecb98798a5a.dat; done; echo '</body></html>' >> /galaxy/server/database/objects/a/1/9/dataset_a1988671-3422-4df4-ae2e-2ecb98798a5a.dat;)]
galaxy.jobs.runners DEBUG 2025-01-10 13:01:48,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (35) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/35/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/35/galaxy_35.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:01:48,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:01:48,631 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-01-10 13:01:48,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:01:56,686 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 35: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:02:01,712 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 35 finished
galaxy.model.metadata DEBUG 2025-01-10 13:02:01,782 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 47
galaxy.model.metadata DEBUG 2025-01-10 13:02:01,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 48
galaxy.model.metadata DEBUG 2025-01-10 13:02:01,807 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 49
galaxy.util WARNING 2025-01-10 13:02:01,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/1/d/dataset_61d0c8f0-dc61-4665-b94a-cfc4857bf678.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/1/d/dataset_61d0c8f0-dc61-4665-b94a-cfc4857bf678.dat'
galaxy.util WARNING 2025-01-10 13:02:01,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/f/9/a/dataset_f9aa8a5a-3026-4a2f-951f-cfc5648768b6.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/f/9/a/dataset_f9aa8a5a-3026-4a2f-951f-cfc5648768b6.dat'
galaxy.jobs INFO 2025-01-10 13:02:01,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 35 in /galaxy/server/database/jobs_directory/000/35
galaxy.objectstore CRITICAL 2025-01-10 13:02:01,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:02:01,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 35 executed (111.667 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:02:02,916 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 36
tpv.core.entities DEBUG 2025-01-10 13:02:02,956 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:02:02,957 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:02:02,962 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:02:02,981 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:02:03,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Working directory for job is: /galaxy/server/database/jobs_directory/000/36
galaxy.jobs.runners DEBUG 2025-01-10 13:02:03,009 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [36] queued (46.691 ms)
galaxy.jobs.handler INFO 2025-01-10 13:02:03,013 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (36) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:02:03,017 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 36
galaxy.jobs DEBUG 2025-01-10 13:02:03,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [36] prepared (79.533 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:02:03,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/36/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/36/registry.xml' '/galaxy/server/database/jobs_directory/000/36/upload_params.json' '50:/galaxy/server/database/objects/2/7/1/dataset_27129ff9-6cbc-4ea0-97d9-0a5089f3279b_files:/galaxy/server/database/objects/2/7/1/dataset_27129ff9-6cbc-4ea0-97d9-0a5089f3279b.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:02:03,144 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (36) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/36/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/36/galaxy_36.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:02:11,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 36: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:02:16,858 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 36 finished
galaxy.model.metadata DEBUG 2025-01-10 13:02:16,914 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 50
galaxy.jobs INFO 2025-01-10 13:02:16,956 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 36 in /galaxy/server/database/jobs_directory/000/36
galaxy.jobs DEBUG 2025-01-10 13:02:17,016 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 36 executed (131.477 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:02:18,332 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 37
tpv.core.entities DEBUG 2025-01-10 13:02:18,372 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:02:18,373 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:02:18,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:02:18,395 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:02:18,414 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Working directory for job is: /galaxy/server/database/jobs_directory/000/37
galaxy.jobs.runners DEBUG 2025-01-10 13:02:18,423 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [37] queued (43.751 ms)
galaxy.jobs.handler INFO 2025-01-10 13:02:18,427 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (37) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:02:18,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 37
galaxy.jobs DEBUG 2025-01-10 13:02:18,528 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [37] prepared (69.066 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:02:18,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:02:18,529 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-01-10 13:02:18,550 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:02:18,573 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/37/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/37/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/2/7/1/dataset_27129ff9-6cbc-4ea0-97d9-0a5089f3279b.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools cnv  --output-dir cnv_tmp     --aberrant "1.0,1.0" --BAF-weight 1 --BAF-dev "0.04,0.04" --LRR-weight 0 --same-prob 0.5 --err-prob 0.0001 --xy-prob 1e-09             input.vcf.gz   && mv cnv_tmp/cn.*.tab '/galaxy/server/database/objects/6/6/6/dataset_66641971-4857-4442-9db1-4a7bd1d6f38d.dat' && mv cnv_tmp/summary.*.tab '/galaxy/server/database/objects/2/e/e/dataset_2eeed8d9-e52d-44dd-a2ea-b62a1543e62d.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:02:18,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (37) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/37/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/37/galaxy_37.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:02:18,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:02:18,608 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-01-10 13:02:18,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:02:24,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 37: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:02:28,862 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 37 finished
galaxy.model.metadata DEBUG 2025-01-10 13:02:28,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 51
galaxy.model.metadata DEBUG 2025-01-10 13:02:28,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 52
galaxy.util WARNING 2025-01-10 13:02:28,949 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/6/6/6/dataset_66641971-4857-4442-9db1-4a7bd1d6f38d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/6/6/6/dataset_66641971-4857-4442-9db1-4a7bd1d6f38d.dat'
galaxy.util WARNING 2025-01-10 13:02:28,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/e/e/dataset_2eeed8d9-e52d-44dd-a2ea-b62a1543e62d.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/e/e/dataset_2eeed8d9-e52d-44dd-a2ea-b62a1543e62d.dat'
galaxy.jobs INFO 2025-01-10 13:02:28,982 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 37 in /galaxy/server/database/jobs_directory/000/37
galaxy.objectstore CRITICAL 2025-01-10 13:02:29,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] None delete error [Errno 13] Permission denied: 'dat.test.tab'
galaxy.jobs DEBUG 2025-01-10 13:02:29,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 37 executed (114.866 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:02:30,684 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 38
tpv.core.entities DEBUG 2025-01-10 13:02:30,715 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:02:30,716 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:02:30,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:02:30,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:02:30,746 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Working directory for job is: /galaxy/server/database/jobs_directory/000/38
galaxy.jobs.runners DEBUG 2025-01-10 13:02:30,753 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [38] queued (32.951 ms)
galaxy.jobs.handler INFO 2025-01-10 13:02:30,756 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (38) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:02:30,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 38
galaxy.jobs DEBUG 2025-01-10 13:02:30,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [38] prepared (85.663 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:02:30,885 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/38/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/38/registry.xml' '/galaxy/server/database/jobs_directory/000/38/upload_params.json' '53:/galaxy/server/database/objects/6/d/7/dataset_6d779a74-2dc3-4e1b-be13-5de6e0beaf0f_files:/galaxy/server/database/objects/6/d/7/dataset_6d779a74-2dc3-4e1b-be13-5de6e0beaf0f.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:02:30,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (38) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/38/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/38/galaxy_38.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:02:40,157 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 38: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:02:45,158 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 38 finished
galaxy.model.metadata DEBUG 2025-01-10 13:02:45,212 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 53
galaxy.jobs INFO 2025-01-10 13:02:45,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 38 in /galaxy/server/database/jobs_directory/000/38
galaxy.jobs DEBUG 2025-01-10 13:02:45,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 38 executed (122.651 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:02:46,082 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 39
tpv.core.entities DEBUG 2025-01-10 13:02:46,122 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:02:46,122 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:02:46,129 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:02:46,147 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:02:46,171 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Working directory for job is: /galaxy/server/database/jobs_directory/000/39
galaxy.jobs.runners DEBUG 2025-01-10 13:02:46,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [39] queued (50.188 ms)
galaxy.jobs.handler INFO 2025-01-10 13:02:46,182 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (39) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:02:46,187 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 39
galaxy.jobs DEBUG 2025-01-10 13:02:46,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [39] prepared (59.246 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:02:46,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:02:46,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-01-10 13:02:46,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:02:46,305 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/39/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/39/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/6/d/7/dataset_6d779a74-2dc3-4e1b-be13-5de6e0beaf0f.dat' > input.vcf.gz && bcftools index input.vcf.gz &&            bcftools cnv  --output-dir cnv_tmp    -p 0  --aberrant "1.0,1.0" --BAF-weight 1.0 --BAF-dev "0.04,0.04" --LRR-weight 0.2 --LRR-dev "0.2,0.2" --LRR-smooth-win 10 --same-prob 0.5 --err-prob 0.0001 --xy-prob 1e-09    --regions-overlap 1          input.vcf.gz   && mv cnv_tmp/cn.*.tab '/galaxy/server/database/objects/9/d/9/dataset_9d971696-6f84-4019-a7a9-1e9fc5d857fa.dat' && mv cnv_tmp/summary.*.tab '/galaxy/server/database/objects/9/4/5/dataset_945450cb-4daf-4917-b30e-9cd8a1354dfb.dat'  && (echo '<html><body><head><title>Copy-number variation plots (bcftools cnv)</title><style type="text/css"> @media print { img { max-width:100% !important; page-break-inside: avoid; } </style>' > /galaxy/server/database/objects/e/2/f/dataset_e2f00375-1a46-4597-bec6-e8d204f0f0b2.dat; for plot in cnv_tmp/*.png; do [ -f "$plot" ] || break; echo '<div><img src="data:image/png;base64,' >> /galaxy/server/database/objects/e/2/f/dataset_e2f00375-1a46-4597-bec6-e8d204f0f0b2.dat; python -m base64 $plot >> /galaxy/server/database/objects/e/2/f/dataset_e2f00375-1a46-4597-bec6-e8d204f0f0b2.dat; echo '" /></div><hr>' >> /galaxy/server/database/objects/e/2/f/dataset_e2f00375-1a46-4597-bec6-e8d204f0f0b2.dat; done; echo '</body></html>' >> /galaxy/server/database/objects/e/2/f/dataset_e2f00375-1a46-4597-bec6-e8d204f0f0b2.dat;)]
galaxy.jobs.runners DEBUG 2025-01-10 13:02:46,328 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (39) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/39/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/39/galaxy_39.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:02:46,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:02:46,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_cnv/bcftools_cnv/1.15.1+galaxy4: mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e
galaxy.tool_util.deps.containers INFO 2025-01-10 13:02:46,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-3f0b2099bce3ecb75064dfe6cdabcd4018727aa8:765890586dbdb5583fc21a09dae4bc93bb0eed0e-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:02:54,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 39: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:02:59,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 39 finished
galaxy.model.metadata DEBUG 2025-01-10 13:02:59,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 54
galaxy.model.metadata DEBUG 2025-01-10 13:02:59,363 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 55
galaxy.model.metadata DEBUG 2025-01-10 13:02:59,376 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 56
galaxy.util WARNING 2025-01-10 13:02:59,381 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/d/9/dataset_9d971696-6f84-4019-a7a9-1e9fc5d857fa.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/d/9/dataset_9d971696-6f84-4019-a7a9-1e9fc5d857fa.dat'
galaxy.util WARNING 2025-01-10 13:02:59,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/9/4/5/dataset_945450cb-4daf-4917-b30e-9cd8a1354dfb.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/9/4/5/dataset_945450cb-4daf-4917-b30e-9cd8a1354dfb.dat'
galaxy.jobs INFO 2025-01-10 13:02:59,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 39 in /galaxy/server/database/jobs_directory/000/39
galaxy.objectstore CRITICAL 2025-01-10 13:02:59,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:02:59,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 39 executed (125.188 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:03:02,556 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 40
tpv.core.entities DEBUG 2025-01-10 13:03:02,592 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:03:02,593 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:03:02,597 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:03:02,610 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:03:02,627 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Working directory for job is: /galaxy/server/database/jobs_directory/000/40
galaxy.jobs.runners DEBUG 2025-01-10 13:03:02,634 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [40] queued (36.630 ms)
galaxy.jobs.handler INFO 2025-01-10 13:03:02,637 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (40) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:03:02,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 40
galaxy.jobs DEBUG 2025-01-10 13:03:02,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [40] prepared (74.555 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:03:02,749 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/40/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/40/registry.xml' '/galaxy/server/database/jobs_directory/000/40/upload_params.json' '57:/galaxy/server/database/objects/f/e/0/dataset_fe0b921a-fece-498b-ab8a-dd92bc088076_files:/galaxy/server/database/objects/f/e/0/dataset_fe0b921a-fece-498b-ab8a-dd92bc088076.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:03:02,763 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (40) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/40/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/40/galaxy_40.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:03:11,531 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 40: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:03:16,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 40 finished
galaxy.model.metadata DEBUG 2025-01-10 13:03:16,724 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 57
galaxy.jobs INFO 2025-01-10 13:03:16,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 40 in /galaxy/server/database/jobs_directory/000/40
galaxy.jobs DEBUG 2025-01-10 13:03:16,814 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 40 executed (126.054 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:03:17,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 41
tpv.core.entities DEBUG 2025-01-10 13:03:17,986 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:03:17,986 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:03:17,991 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:03:18,005 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:03:18,020 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Working directory for job is: /galaxy/server/database/jobs_directory/000/41
galaxy.jobs.runners DEBUG 2025-01-10 13:03:18,028 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [41] queued (37.586 ms)
galaxy.jobs.handler INFO 2025-01-10 13:03:18,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (41) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:03:18,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 41
galaxy.jobs DEBUG 2025-01-10 13:03:18,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [41] prepared (77.991 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:03:18,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:03:18,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-10 13:03:18,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:03:18,360 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/41/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/41/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/f/e/0/dataset_fe0b921a-fece-498b-ab8a-dd92bc088076.dat' > input.vcf.gz && bcftools index input.vcf.gz &&                bcftools roh      --AF-dflt "0.4"   --GTs-only "30.0"     --hw-to-az "6.7e-08" --az-to-hw "5e-09"                 input.vcf.gz  > '/galaxy/server/database/objects/0/a/8/dataset_0a845e24-3342-43ef-8d1f-a6794aa9b6a0.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:03:18,373 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (41) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/41/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/41/galaxy_41.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:03:18,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:03:18,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-10 13:03:18,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:03:29,799 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 41: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:03:34,655 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 41 finished
galaxy.model.metadata DEBUG 2025-01-10 13:03:34,715 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 58
galaxy.jobs INFO 2025-01-10 13:03:34,754 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 41 in /galaxy/server/database/jobs_directory/000/41
galaxy.jobs DEBUG 2025-01-10 13:03:34,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 41 executed (117.978 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:03:36,409 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 42
tpv.core.entities DEBUG 2025-01-10 13:03:36,442 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:03:36,442 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:03:36,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:03:36,465 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:03:36,484 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Working directory for job is: /galaxy/server/database/jobs_directory/000/42
galaxy.jobs.runners DEBUG 2025-01-10 13:03:36,491 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [42] queued (42.729 ms)
galaxy.jobs.handler INFO 2025-01-10 13:03:36,495 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (42) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:03:36,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 42
galaxy.jobs DEBUG 2025-01-10 13:03:36,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [42] prepared (82.125 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:03:36,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/42/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/42/registry.xml' '/galaxy/server/database/jobs_directory/000/42/upload_params.json' '59:/galaxy/server/database/objects/6/9/b/dataset_69b5e181-8ba6-43d9-b44a-6c6fb8d1f40d_files:/galaxy/server/database/objects/6/9/b/dataset_69b5e181-8ba6-43d9-b44a-6c6fb8d1f40d.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:03:36,635 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (42) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/42/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/42/galaxy_42.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:03:46,030 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 42: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:03:51,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 42 finished
galaxy.model.metadata DEBUG 2025-01-10 13:03:51,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 59
galaxy.jobs INFO 2025-01-10 13:03:51,121 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 42 in /galaxy/server/database/jobs_directory/000/42
galaxy.jobs DEBUG 2025-01-10 13:03:51,173 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 42 executed (129.222 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:03:51,819 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 43
tpv.core.entities DEBUG 2025-01-10 13:03:51,852 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:03:51,852 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:03:51,857 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:03:51,872 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:03:51,889 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Working directory for job is: /galaxy/server/database/jobs_directory/000/43
galaxy.jobs.runners DEBUG 2025-01-10 13:03:51,898 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [43] queued (41.157 ms)
galaxy.jobs.handler INFO 2025-01-10 13:03:51,903 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (43) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:03:51,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 43
galaxy.jobs DEBUG 2025-01-10 13:03:51,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [43] prepared (53.379 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:03:51,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:03:51,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-10 13:03:52,002 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:03:52,024 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/43/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/43/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/6/9/b/dataset_69b5e181-8ba6-43d9-b44a-6c6fb8d1f40d.dat' > input.vcf.gz && bcftools index input.vcf.gz &&                bcftools roh      --AF-dflt "0.4"   --GTs-only "30.0"     --hw-to-az "6.7e-08" --az-to-hw "5e-09"               --output-type r   input.vcf.gz  > '/galaxy/server/database/objects/e/c/6/dataset_ec66d6a1-3733-4925-8d94-c2dcca872b75.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:03:52,036 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (43) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/43/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/43/galaxy_43.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:03:52,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:03:52,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-10 13:03:52,065 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:03:57,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 43: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:04:01,925 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 43 finished
galaxy.model.metadata DEBUG 2025-01-10 13:04:01,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 60
galaxy.jobs INFO 2025-01-10 13:04:02,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 43 in /galaxy/server/database/jobs_directory/000/43
galaxy.jobs DEBUG 2025-01-10 13:04:02,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 43 executed (126.630 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:04:04,233 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 44
tpv.core.entities DEBUG 2025-01-10 13:04:04,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:04:04,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:04:04,271 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:04:04,285 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:04:04,305 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Working directory for job is: /galaxy/server/database/jobs_directory/000/44
galaxy.jobs.runners DEBUG 2025-01-10 13:04:04,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [44] queued (42.503 ms)
galaxy.jobs.handler INFO 2025-01-10 13:04:04,318 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (44) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:04:04,322 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 44
galaxy.jobs DEBUG 2025-01-10 13:04:04,415 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [44] prepared (75.001 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:04:04,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/44/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/44/registry.xml' '/galaxy/server/database/jobs_directory/000/44/upload_params.json' '61:/galaxy/server/database/objects/7/7/6/dataset_776cd4de-6597-4274-9e6c-381453757f4d_files:/galaxy/server/database/objects/7/7/6/dataset_776cd4de-6597-4274-9e6c-381453757f4d.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:04:04,449 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (44) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/44/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/44/galaxy_44.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:04:12,389 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 44: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:04:17,658 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 44 finished
galaxy.model.metadata DEBUG 2025-01-10 13:04:17,714 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 61
galaxy.jobs INFO 2025-01-10 13:04:17,750 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 44 in /galaxy/server/database/jobs_directory/000/44
galaxy.jobs DEBUG 2025-01-10 13:04:17,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 44 executed (115.123 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:04:18,645 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 45
tpv.core.entities DEBUG 2025-01-10 13:04:18,686 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:04:18,687 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:04:18,692 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:04:18,709 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:04:18,724 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Working directory for job is: /galaxy/server/database/jobs_directory/000/45
galaxy.jobs.runners DEBUG 2025-01-10 13:04:18,731 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [45] queued (38.703 ms)
galaxy.jobs.handler INFO 2025-01-10 13:04:18,736 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (45) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:04:18,738 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 45
galaxy.jobs DEBUG 2025-01-10 13:04:18,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [45] prepared (44.220 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:04:18,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:04:18,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-10 13:04:18,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:04:18,844 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/45/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/45/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/7/7/6/dataset_776cd4de-6597-4274-9e6c-381453757f4d.dat' > input.vcf.gz && bcftools index input.vcf.gz &&                bcftools roh      --AF-dflt "0.4"   --GTs-only "30.0"  --ignore-homref --include-noalt  --hw-to-az "6.7e-08" --az-to-hw "5e-09"               --output-type r   input.vcf.gz  > '/galaxy/server/database/objects/3/0/e/dataset_30e58f3b-89e1-4ff6-a21e-fc7390173fb7.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:04:18,854 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (45) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/45/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/45/galaxy_45.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:04:18,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:04:18,865 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-10 13:04:18,884 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:04:24,552 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 45: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:04:29,627 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 45 finished
galaxy.model.metadata DEBUG 2025-01-10 13:04:29,692 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 62
galaxy.jobs INFO 2025-01-10 13:04:29,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 45 in /galaxy/server/database/jobs_directory/000/45
galaxy.jobs DEBUG 2025-01-10 13:04:29,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 45 executed (124.124 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:04:30,997 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 46
tpv.core.entities DEBUG 2025-01-10 13:04:31,035 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:04:31,035 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:04:31,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:04:31,058 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:04:31,079 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Working directory for job is: /galaxy/server/database/jobs_directory/000/46
galaxy.jobs.runners DEBUG 2025-01-10 13:04:31,088 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [46] queued (47.763 ms)
galaxy.jobs.handler INFO 2025-01-10 13:04:31,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (46) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:04:31,097 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 46
galaxy.jobs DEBUG 2025-01-10 13:04:31,215 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [46] prepared (96.684 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:04:31,236 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/46/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/46/registry.xml' '/galaxy/server/database/jobs_directory/000/46/upload_params.json' '63:/galaxy/server/database/objects/e/7/9/dataset_e79a287b-1227-44d7-99f0-ab223bb1032b_files:/galaxy/server/database/objects/e/7/9/dataset_e79a287b-1227-44d7-99f0-ab223bb1032b.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:04:31,247 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (46) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/46/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/46/galaxy_46.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:04:39,730 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 46: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:04:44,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 46 finished
galaxy.model.metadata DEBUG 2025-01-10 13:04:44,847 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 63
galaxy.jobs INFO 2025-01-10 13:04:44,895 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 46 in /galaxy/server/database/jobs_directory/000/46
galaxy.jobs DEBUG 2025-01-10 13:04:44,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 46 executed (146.897 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:04:45,395 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 47
tpv.core.entities DEBUG 2025-01-10 13:04:45,440 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:04:45,440 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:04:45,447 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:04:45,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:04:45,485 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Working directory for job is: /galaxy/server/database/jobs_directory/000/47
galaxy.jobs.runners DEBUG 2025-01-10 13:04:45,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [47] queued (46.731 ms)
galaxy.jobs.handler INFO 2025-01-10 13:04:45,498 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (47) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:04:45,503 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 47
galaxy.jobs DEBUG 2025-01-10 13:04:45,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [47] prepared (72.627 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:04:45,599 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:04:45,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-10 13:04:45,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:04:45,957 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/47/tool_script.sh] for tool command [bcftools 2>&1 | grep 'Version:' > /galaxy/server/database/jobs_directory/000/47/outputs/COMMAND_VERSION 2>&1;
export BCFTOOLS_PLUGINS=`which bcftools | sed 's,bin/bcftools,libexec/bcftools,'`;     bgzip -c '/galaxy/server/database/objects/e/7/9/dataset_e79a287b-1227-44d7-99f0-ab223bb1032b.dat' > input.vcf.gz && bcftools index input.vcf.gz &&                bcftools roh      --AF-dflt "0.4"   --GTs-only "30.0"     --hw-to-az "6.7e-08" --az-to-hw "5e-09"     --regions-overlap 1             input.vcf.gz  > '/galaxy/server/database/objects/4/9/2/dataset_4924cc7f-87c2-475f-87da-ce940a6bf4d6.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:04:45,974 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (47) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/47/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/47/galaxy_47.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:04:45,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:04:45,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/bcftools_roh/bcftools_roh/1.15.1+galaxy4: mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a
galaxy.tool_util.deps.containers INFO 2025-01-10 13:04:46,008 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-b99184dc2d32592dd62a87fa4a796c61585788e6:07602ea99d759e160674ea07c369efffcc80577a-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:04:51,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 47: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:04:57,243 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 47 finished
galaxy.model.metadata DEBUG 2025-01-10 13:04:57,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 64
galaxy.jobs INFO 2025-01-10 13:04:57,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 47 in /galaxy/server/database/jobs_directory/000/47
galaxy.jobs DEBUG 2025-01-10 13:04:57,401 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 47 executed (132.599 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:04:59,806 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 48, 49
tpv.core.entities DEBUG 2025-01-10 13:04:59,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:04:59,842 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:04:59,847 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:04:59,862 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:04:59,883 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Working directory for job is: /galaxy/server/database/jobs_directory/000/48
galaxy.jobs.runners DEBUG 2025-01-10 13:04:59,892 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [48] queued (45.318 ms)
galaxy.jobs.handler INFO 2025-01-10 13:04:59,896 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (48) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:04:59,904 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 48
tpv.core.entities DEBUG 2025-01-10 13:04:59,921 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:04:59,921 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:04:59,930 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:04:59,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:04:59,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Working directory for job is: /galaxy/server/database/jobs_directory/000/49
galaxy.jobs.runners DEBUG 2025-01-10 13:05:00,002 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [49] queued (71.819 ms)
galaxy.jobs.handler INFO 2025-01-10 13:05:00,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (49) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:05:00,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 49
galaxy.jobs DEBUG 2025-01-10 13:05:00,050 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [48] prepared (117.738 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:05:00,079 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/48/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/48/registry.xml' '/galaxy/server/database/jobs_directory/000/48/upload_params.json' '65:/galaxy/server/database/objects/4/1/8/dataset_4189a066-a458-4d39-a513-f841e59b0621_files:/galaxy/server/database/objects/4/1/8/dataset_4189a066-a458-4d39-a513-f841e59b0621.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:05:00,099 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (48) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/48/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/48/galaxy_48.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-10 13:05:00,137 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [49] prepared (102.365 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:05:00,162 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/49/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/49/registry.xml' '/galaxy/server/database/jobs_directory/000/49/upload_params.json' '66:/galaxy/server/database/objects/3/b/1/dataset_3b1ca1ea-d152-4637-b638-63bf5f507682_files:/galaxy/server/database/objects/3/b/1/dataset_3b1ca1ea-d152-4637-b638-63bf5f507682.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:05:00,176 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (49) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/49/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/49/galaxy_49.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:05:10,273 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 48: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:05:10,290 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 49: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:05:15,412 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 49 finished
galaxy.jobs.runners DEBUG 2025-01-10 13:05:15,419 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 48 finished
galaxy.model.metadata DEBUG 2025-01-10 13:05:15,499 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 66
galaxy.model.metadata DEBUG 2025-01-10 13:05:15,506 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 65
galaxy.jobs INFO 2025-01-10 13:05:15,551 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 49 in /galaxy/server/database/jobs_directory/000/49
galaxy.jobs INFO 2025-01-10 13:05:15,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 48 in /galaxy/server/database/jobs_directory/000/48
galaxy.jobs DEBUG 2025-01-10 13:05:15,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 49 executed (172.912 ms)
galaxy.jobs DEBUG 2025-01-10 13:05:15,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 48 executed (170.011 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:05:16,497 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 50
tpv.core.entities DEBUG 2025-01-10 13:05:16,544 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:05:16,544 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:05:16,551 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:05:16,569 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:05:16,590 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Working directory for job is: /galaxy/server/database/jobs_directory/000/50
galaxy.jobs.runners DEBUG 2025-01-10 13:05:16,600 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [50] queued (49.208 ms)
galaxy.jobs.handler INFO 2025-01-10 13:05:16,605 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (50) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:05:16,610 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 50
galaxy.jobs DEBUG 2025-01-10 13:05:16,688 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [50] prepared (61.389 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:05:16,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:05:16,689 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbedintersect/vcfbedintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-01-10 13:05:16,898 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:05:16,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/50/tool_script.sh] for tool command [vcfintersect -b '/galaxy/server/database/objects/3/b/1/dataset_3b1ca1ea-d152-4637-b638-63bf5f507682.dat'  '/galaxy/server/database/objects/4/1/8/dataset_4189a066-a458-4d39-a513-f841e59b0621.dat' > '/galaxy/server/database/objects/7/8/4/dataset_784d4c74-8a9f-49cd-83fd-f27789feae03.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:05:16,932 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (50) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/50/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/50/galaxy_50.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:05:16,944 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:05:16,945 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbedintersect/vcfbedintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-01-10 13:05:16,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners.kubernetes ERROR 2025-01-10 13:05:28,826 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://10.14.240.1:443/apis/batch/v1/namespaces/prod-25-01-10-12-36-1/jobs/gxy-z8f4g

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 817, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 824, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 107, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-z8f4g": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:05:29,037 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (50/gxy-z8f4g) Terminated at user's request
galaxy.jobs.handler DEBUG 2025-01-10 13:05:30,920 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 51
tpv.core.entities DEBUG 2025-01-10 13:05:30,959 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:05:30,959 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:05:30,964 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:05:30,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:05:31,000 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Working directory for job is: /galaxy/server/database/jobs_directory/000/51
galaxy.jobs.runners DEBUG 2025-01-10 13:05:31,006 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [51] queued (41.366 ms)
galaxy.jobs.handler INFO 2025-01-10 13:05:31,011 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (51) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:05:31,014 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 51
galaxy.jobs DEBUG 2025-01-10 13:05:31,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [51] prepared (83.201 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:05:31,136 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/51/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/51/registry.xml' '/galaxy/server/database/jobs_directory/000/51/upload_params.json' '68:/galaxy/server/database/objects/8/3/5/dataset_835bd624-fb43-4534-9f5c-3911602b112d_files:/galaxy/server/database/objects/8/3/5/dataset_835bd624-fb43-4534-9f5c-3911602b112d.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:05:31,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (51) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/51/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/51/galaxy_51.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:05:39,135 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 51: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:05:43,987 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 51 finished
galaxy.model.metadata DEBUG 2025-01-10 13:05:44,045 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 68
galaxy.jobs INFO 2025-01-10 13:05:44,084 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 51 in /galaxy/server/database/jobs_directory/000/51
galaxy.jobs DEBUG 2025-01-10 13:05:44,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 51 executed (116.115 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:05:45,302 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 52
tpv.core.entities DEBUG 2025-01-10 13:05:45,340 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:05:45,340 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:05:45,346 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:05:45,363 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:05:45,379 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Working directory for job is: /galaxy/server/database/jobs_directory/000/52
galaxy.jobs.runners DEBUG 2025-01-10 13:05:45,387 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [52] queued (40.863 ms)
galaxy.jobs.handler INFO 2025-01-10 13:05:45,391 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (52) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:05:45,395 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 52
galaxy.jobs DEBUG 2025-01-10 13:05:45,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [52] prepared (56.086 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:05:45,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:05:45,469 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbedintersect/vcfbedintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-01-10 13:05:45,491 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:05:45,512 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/52/tool_script.sh] for tool command [vcfintersect -R '20:1-30000'  '/galaxy/server/database/objects/8/3/5/dataset_835bd624-fb43-4534-9f5c-3911602b112d.dat' > '/galaxy/server/database/objects/c/8/c/dataset_c8c607f8-babd-4db9-af6c-f32eb3f78dc6.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:05:45,523 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (52) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/52/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/52/galaxy_52.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:05:45,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:05:45,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfbedintersect/vcfbedintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-01-10 13:05:45,557 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:05:51,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 52: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:05:56,142 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 52 finished
galaxy.model.metadata DEBUG 2025-01-10 13:05:56,201 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 69
galaxy.jobs INFO 2025-01-10 13:05:56,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 52 in /galaxy/server/database/jobs_directory/000/52
galaxy.jobs DEBUG 2025-01-10 13:05:56,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 52 executed (109.518 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:05:58,665 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 54, 53
tpv.core.entities DEBUG 2025-01-10 13:05:58,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:05:58,708 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:05:58,714 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:05:58,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:05:58,755 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Working directory for job is: /galaxy/server/database/jobs_directory/000/53
galaxy.jobs.runners DEBUG 2025-01-10 13:05:58,763 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [53] queued (49.443 ms)
galaxy.jobs.handler INFO 2025-01-10 13:05:58,769 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (53) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:05:58,775 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 53
tpv.core.entities DEBUG 2025-01-10 13:05:58,789 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:05:58,789 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:05:58,797 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:05:58,817 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:05:58,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Working directory for job is: /galaxy/server/database/jobs_directory/000/54
galaxy.jobs.runners DEBUG 2025-01-10 13:05:58,857 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [54] queued (60.492 ms)
galaxy.jobs.handler INFO 2025-01-10 13:05:58,861 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (54) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:05:58,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 54
galaxy.jobs DEBUG 2025-01-10 13:05:58,921 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [53] prepared (120.044 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:05:58,958 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/53/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/53/registry.xml' '/galaxy/server/database/jobs_directory/000/53/upload_params.json' '70:/galaxy/server/database/objects/1/0/2/dataset_102102bf-efb9-47ec-8c2b-d2935c01ac89_files:/galaxy/server/database/objects/1/0/2/dataset_102102bf-efb9-47ec-8c2b-d2935c01ac89.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:05:58,977 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (53) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/53/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/53/galaxy_53.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-10 13:05:59,012 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [54] prepared (120.150 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:05:59,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/54/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/54/registry.xml' '/galaxy/server/database/jobs_directory/000/54/upload_params.json' '71:/galaxy/server/database/objects/b/1/4/dataset_b14165f0-0f05-4fa8-884f-33d1e43fe18b_files:/galaxy/server/database/objects/b/1/4/dataset_b14165f0-0f05-4fa8-884f-33d1e43fe18b.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:05:59,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (54) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/54/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/54/galaxy_54.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-10 13:05:59,864 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 55
tpv.core.entities DEBUG 2025-01-10 13:05:59,895 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:05:59,896 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:05:59,901 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:05:59,921 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:05:59,942 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Working directory for job is: /galaxy/server/database/jobs_directory/000/55
galaxy.jobs.runners DEBUG 2025-01-10 13:05:59,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [55] queued (49.095 ms)
galaxy.jobs.handler INFO 2025-01-10 13:05:59,955 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (55) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:05:59,960 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 55
galaxy.jobs DEBUG 2025-01-10 13:06:00,086 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [55] prepared (103.528 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:06:00,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/55/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/55/registry.xml' '/galaxy/server/database/jobs_directory/000/55/upload_params.json' '72:/galaxy/server/database/objects/f/0/9/dataset_f09173dd-2067-40e2-9ee0-3da20074c758_files:/galaxy/server/database/objects/f/0/9/dataset_f09173dd-2067-40e2-9ee0-3da20074c758.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:06:00,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (55) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/55/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/55/galaxy_55.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:06:08,900 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 53: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:06:08,936 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 54: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:06:08,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 55: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:06:17,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 53 finished
galaxy.jobs.runners DEBUG 2025-01-10 13:06:17,096 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 54 finished
galaxy.model.metadata DEBUG 2025-01-10 13:06:17,110 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 70
galaxy.jobs.runners DEBUG 2025-01-10 13:06:17,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 55 finished
galaxy.jobs INFO 2025-01-10 13:06:17,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 53 in /galaxy/server/database/jobs_directory/000/53
galaxy.model.metadata DEBUG 2025-01-10 13:06:17,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 71
galaxy.model.metadata DEBUG 2025-01-10 13:06:17,217 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 72
galaxy.jobs INFO 2025-01-10 13:06:17,250 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 54 in /galaxy/server/database/jobs_directory/000/54
galaxy.jobs INFO 2025-01-10 13:06:17,269 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 55 in /galaxy/server/database/jobs_directory/000/55
galaxy.jobs DEBUG 2025-01-10 13:06:17,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 53 executed (237.165 ms)
galaxy.jobs DEBUG 2025-01-10 13:06:17,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 54 executed (206.594 ms)
galaxy.jobs DEBUG 2025-01-10 13:06:17,349 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 55 executed (187.024 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:06:18,651 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 56
tpv.core.entities DEBUG 2025-01-10 13:06:18,697 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:06:18,698 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:06:18,705 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:06:18,725 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:06:18,748 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Working directory for job is: /galaxy/server/database/jobs_directory/000/56
galaxy.jobs.runners DEBUG 2025-01-10 13:06:18,758 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [56] queued (52.307 ms)
galaxy.jobs.handler INFO 2025-01-10 13:06:18,762 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (56) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:06:18,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 56
galaxy.jobs DEBUG 2025-01-10 13:06:18,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [56] prepared (70.553 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:06:18,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:06:18,857 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfvcfintersect/vcfvcfintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-01-10 13:06:18,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:06:18,909 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/56/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/f/0/9/dataset_f09173dd-2067-40e2-9ee0-3da20074c758.dat' 'localref.fa' &&  vcfintersect   -r 'localref.fa' -w "30" -i '/galaxy/server/database/objects/1/0/2/dataset_102102bf-efb9-47ec-8c2b-d2935c01ac89.dat' '/galaxy/server/database/objects/b/1/4/dataset_b14165f0-0f05-4fa8-884f-33d1e43fe18b.dat' > '/galaxy/server/database/objects/2/2/4/dataset_224e8ccc-38cf-418a-b98b-93ca9c15c118.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:06:18,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (56) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/56/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/56/galaxy_56.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:06:18,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:06:18,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfvcfintersect/vcfvcfintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-01-10 13:06:18,961 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:06:24,075 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 56: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:06:29,108 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 56 finished
galaxy.model.metadata DEBUG 2025-01-10 13:06:29,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 73
galaxy.jobs INFO 2025-01-10 13:06:29,225 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 56 in /galaxy/server/database/jobs_directory/000/56
galaxy.jobs DEBUG 2025-01-10 13:06:29,293 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 56 executed (157.136 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:06:31,018 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 58, 57
tpv.core.entities DEBUG 2025-01-10 13:06:31,061 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:06:31,061 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:06:31,068 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:06:31,089 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:06:31,112 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Working directory for job is: /galaxy/server/database/jobs_directory/000/57
galaxy.jobs.runners DEBUG 2025-01-10 13:06:31,121 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [57] queued (51.942 ms)
galaxy.jobs.handler INFO 2025-01-10 13:06:31,125 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (57) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:06:31,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 57
tpv.core.entities DEBUG 2025-01-10 13:06:31,148 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:06:31,148 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:06:31,156 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:06:31,178 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:06:31,219 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Working directory for job is: /galaxy/server/database/jobs_directory/000/58
galaxy.jobs.runners DEBUG 2025-01-10 13:06:31,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [58] queued (73.248 ms)
galaxy.jobs.handler INFO 2025-01-10 13:06:31,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (58) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:06:31,242 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 58
galaxy.jobs DEBUG 2025-01-10 13:06:31,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [57] prepared (134.511 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:06:31,335 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/57/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/57/registry.xml' '/galaxy/server/database/jobs_directory/000/57/upload_params.json' '74:/galaxy/server/database/objects/2/4/5/dataset_2454cc87-1893-4208-a4eb-bd43d270ce80_files:/galaxy/server/database/objects/2/4/5/dataset_2454cc87-1893-4208-a4eb-bd43d270ce80.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:06:31,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (57) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/57/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/57/galaxy_57.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-10 13:06:31,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [58] prepared (139.051 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:06:31,432 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/58/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/58/registry.xml' '/galaxy/server/database/jobs_directory/000/58/upload_params.json' '75:/galaxy/server/database/objects/5/e/0/dataset_5e02fb33-a206-4dc9-ae9c-46c8a348838e_files:/galaxy/server/database/objects/5/e/0/dataset_5e02fb33-a206-4dc9-ae9c-46c8a348838e.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:06:31,458 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (58) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/58/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/58/galaxy_58.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-10 13:06:32,239 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 59
tpv.core.entities DEBUG 2025-01-10 13:06:32,278 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:06:32,278 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:06:32,284 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:06:32,304 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:06:32,323 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Working directory for job is: /galaxy/server/database/jobs_directory/000/59
galaxy.jobs.runners DEBUG 2025-01-10 13:06:32,331 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [59] queued (46.396 ms)
galaxy.jobs.handler INFO 2025-01-10 13:06:32,336 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (59) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:06:32,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 59
galaxy.jobs DEBUG 2025-01-10 13:06:32,441 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [59] prepared (85.535 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:06:32,463 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/59/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/59/registry.xml' '/galaxy/server/database/jobs_directory/000/59/upload_params.json' '76:/galaxy/server/database/objects/8/0/7/dataset_807bffd6-79cf-4229-98f3-4ea395b40a27_files:/galaxy/server/database/objects/8/0/7/dataset_807bffd6-79cf-4229-98f3-4ea395b40a27.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:06:32,476 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (59) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/59/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/59/galaxy_59.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:06:40,593 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 57: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:06:40,620 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 58: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:06:42,001 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 59: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:06:48,191 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 58 finished
galaxy.model.metadata DEBUG 2025-01-10 13:06:48,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 75
galaxy.jobs.runners DEBUG 2025-01-10 13:06:48,252 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 57 finished
galaxy.jobs INFO 2025-01-10 13:06:48,314 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 58 in /galaxy/server/database/jobs_directory/000/58
galaxy.model.metadata DEBUG 2025-01-10 13:06:48,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 74
galaxy.jobs INFO 2025-01-10 13:06:48,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 57 in /galaxy/server/database/jobs_directory/000/57
galaxy.jobs DEBUG 2025-01-10 13:06:48,385 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 58 executed (168.399 ms)
galaxy.jobs DEBUG 2025-01-10 13:06:48,445 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 57 executed (140.815 ms)
galaxy.jobs.runners DEBUG 2025-01-10 13:06:49,275 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 59 finished
galaxy.model.metadata DEBUG 2025-01-10 13:06:49,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 76
galaxy.jobs INFO 2025-01-10 13:06:49,364 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 59 in /galaxy/server/database/jobs_directory/000/59
galaxy.jobs DEBUG 2025-01-10 13:06:49,418 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 59 executed (116.422 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:06:50,112 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 60
tpv.core.entities DEBUG 2025-01-10 13:06:50,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=default, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:06:50,158 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:06:50,163 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:06:50,178 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:06:50,193 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Working directory for job is: /galaxy/server/database/jobs_directory/000/60
galaxy.jobs.runners DEBUG 2025-01-10 13:06:50,201 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [60] queued (38.256 ms)
galaxy.jobs.handler INFO 2025-01-10 13:06:50,205 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (60) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:06:50,209 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 60
galaxy.jobs DEBUG 2025-01-10 13:06:50,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [60] prepared (70.087 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:06:50,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:06:50,298 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfvcfintersect/vcfvcfintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-01-10 13:06:50,320 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:06:50,343 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/60/tool_script.sh] for tool command [ln -s '/galaxy/server/database/objects/8/0/7/dataset_807bffd6-79cf-4229-98f3-4ea395b40a27.dat' 'localref.fa' &&  vcfintersect   -r 'localref.fa' -w "30" -u '/galaxy/server/database/objects/2/4/5/dataset_2454cc87-1893-4208-a4eb-bd43d270ce80.dat' '/galaxy/server/database/objects/5/e/0/dataset_5e02fb33-a206-4dc9-ae9c-46c8a348838e.dat' > '/galaxy/server/database/objects/0/b/5/dataset_0b573b24-7e5c-488e-a1d2-c6cc9e98fb69.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:06:50,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (60) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/60/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/60/galaxy_60.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:06:50,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:06:50,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/devteam/vcfvcfintersect/vcfvcfintersect/1.0.0_rc3+galaxy0: vcflib:1.0.0_rc3
galaxy.tool_util.deps.containers INFO 2025-01-10 13:06:50,392 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/vcflib:1.0.0_rc3--py37hc088bd4_0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:06:55,815 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 60: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:07:00,818 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 60 finished
galaxy.model.metadata DEBUG 2025-01-10 13:07:00,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 77
galaxy.jobs INFO 2025-01-10 13:07:00,917 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 60 in /galaxy/server/database/jobs_directory/000/60
galaxy.jobs DEBUG 2025-01-10 13:07:00,971 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 60 executed (127.002 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:07:03,490 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 61
tpv.core.entities DEBUG 2025-01-10 13:07:03,539 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:07:03,540 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:07:03,546 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:07:03,566 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:07:03,586 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Working directory for job is: /galaxy/server/database/jobs_directory/000/61
galaxy.jobs.runners DEBUG 2025-01-10 13:07:03,594 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [61] queued (47.740 ms)
galaxy.jobs.handler INFO 2025-01-10 13:07:03,597 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (61) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:07:03,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 61
galaxy.jobs DEBUG 2025-01-10 13:07:03,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [61] prepared (88.776 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:07:03,728 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/61/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/61/registry.xml' '/galaxy/server/database/jobs_directory/000/61/upload_params.json' '78:/galaxy/server/database/objects/7/e/a/dataset_7eaba3d3-8f00-4bf5-8bb2-e4129a31c24f_files:/galaxy/server/database/objects/7/e/a/dataset_7eaba3d3-8f00-4bf5-8bb2-e4129a31c24f.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:07:03,742 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (61) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/61/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/61/galaxy_61.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:07:11,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 61: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:07:17,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 61 finished
galaxy.model.metadata DEBUG 2025-01-10 13:07:17,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 78
galaxy.jobs INFO 2025-01-10 13:07:17,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 61 in /galaxy/server/database/jobs_directory/000/61
galaxy.jobs DEBUG 2025-01-10 13:07:17,446 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 61 executed (147.922 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:07:17,901 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 62
tpv.core.entities DEBUG 2025-01-10 13:07:17,944 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:07:17,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:07:17,951 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:07:17,966 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:07:17,982 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Working directory for job is: /galaxy/server/database/jobs_directory/000/62
galaxy.jobs.runners DEBUG 2025-01-10 13:07:17,989 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [62] queued (38.136 ms)
galaxy.jobs.handler INFO 2025-01-10 13:07:17,994 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (62) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:07:17,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 62
galaxy.jobs DEBUG 2025-01-10 13:07:18,070 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [62] prepared (55.017 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:07:18,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:07:18,071 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-01-10 13:07:18,276 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:07:18,297 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/62/tool_script.sh] for tool command [export MPLBACKEND="agg" && poretools hist '/galaxy/server/database/objects/7/e/a/dataset_7eaba3d3-8f00-4bf5-8bb2-e4129a31c24f.dat' --saveas histogram.png --min-length 0 --max-length 1000000000 --num-bins 50  && mv histogram.png '/galaxy/server/database/objects/b/3/3/dataset_b3366c23-4dd5-4b72-902d-7fbea72e0c4b.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:07:18,313 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (62) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/62/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/62/galaxy_62.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:07:18,326 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:07:18,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-01-10 13:07:18,345 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:07:50,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 62: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:07:55,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 62 finished
galaxy.model.metadata DEBUG 2025-01-10 13:07:55,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 79
galaxy.util WARNING 2025-01-10 13:07:55,831 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/b/3/3/dataset_b3366c23-4dd5-4b72-902d-7fbea72e0c4b.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/b/3/3/dataset_b3366c23-4dd5-4b72-902d-7fbea72e0c4b.dat'
galaxy.jobs INFO 2025-01-10 13:07:55,867 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 62 in /galaxy/server/database/jobs_directory/000/62
galaxy.objectstore CRITICAL 2025-01-10 13:07:55,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:07:55,905 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 62 executed (147.135 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:07:57,941 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 63
tpv.core.entities DEBUG 2025-01-10 13:07:57,979 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:07:57,980 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:07:57,986 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:07:58,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:07:58,024 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Working directory for job is: /galaxy/server/database/jobs_directory/000/63
galaxy.jobs.runners DEBUG 2025-01-10 13:07:58,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [63] queued (45.105 ms)
galaxy.jobs.handler INFO 2025-01-10 13:07:58,036 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (63) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:07:58,040 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 63
galaxy.jobs DEBUG 2025-01-10 13:07:58,152 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [63] prepared (91.260 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:07:58,175 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/63/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/63/registry.xml' '/galaxy/server/database/jobs_directory/000/63/upload_params.json' '80:/galaxy/server/database/objects/9/6/0/dataset_9601bf4e-dbd4-4589-a382-06293e04e344_files:/galaxy/server/database/objects/9/6/0/dataset_9601bf4e-dbd4-4589-a382-06293e04e344.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:07:58,188 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (63) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/63/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/63/galaxy_63.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:08:08,088 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 63: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:08:13,238 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 63 finished
galaxy.model.metadata DEBUG 2025-01-10 13:08:13,308 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 80
galaxy.jobs INFO 2025-01-10 13:08:13,341 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 63 in /galaxy/server/database/jobs_directory/000/63
galaxy.jobs DEBUG 2025-01-10 13:08:13,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 63 executed (132.128 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:08:14,384 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 64
tpv.core.entities DEBUG 2025-01-10 13:08:14,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:08:14,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:08:14,432 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:08:14,448 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:08:14,467 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Working directory for job is: /galaxy/server/database/jobs_directory/000/64
galaxy.jobs.runners DEBUG 2025-01-10 13:08:14,474 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [64] queued (42.285 ms)
galaxy.jobs.handler INFO 2025-01-10 13:08:14,477 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (64) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:08:14,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 64
galaxy.jobs DEBUG 2025-01-10 13:08:14,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [64] prepared (57.522 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:08:14,555 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:08:14,556 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-01-10 13:08:14,580 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:08:14,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/64/tool_script.sh] for tool command [export MPLBACKEND="agg" && poretools hist '/galaxy/server/database/objects/9/6/0/dataset_9601bf4e-dbd4-4589-a382-06293e04e344.dat' --saveas histogram.pdf --min-length 0 --max-length 1000000000 --num-bins 50  && mv histogram.pdf '/galaxy/server/database/objects/e/f/a/dataset_efab2f5d-d968-40e1-ad99-ed71ee10b2ba.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:08:14,619 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (64) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/64/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/64/galaxy_64.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:08:14,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:08:14,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-01-10 13:08:14,653 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:08:21,254 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 64: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:08:26,306 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 64 finished
galaxy.model.metadata DEBUG 2025-01-10 13:08:26,370 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 81
galaxy.util WARNING 2025-01-10 13:08:26,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/f/a/dataset_efab2f5d-d968-40e1-ad99-ed71ee10b2ba.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/f/a/dataset_efab2f5d-d968-40e1-ad99-ed71ee10b2ba.dat'
galaxy.jobs INFO 2025-01-10 13:08:26,410 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 64 in /galaxy/server/database/jobs_directory/000/64
galaxy.objectstore CRITICAL 2025-01-10 13:08:26,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:08:26,436 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 64 executed (99.901 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:08:28,773 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 65
tpv.core.entities DEBUG 2025-01-10 13:08:28,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:08:28,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:08:28,813 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:08:28,831 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:08:28,849 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Working directory for job is: /galaxy/server/database/jobs_directory/000/65
galaxy.jobs.runners DEBUG 2025-01-10 13:08:28,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [65] queued (42.318 ms)
galaxy.jobs.handler INFO 2025-01-10 13:08:28,860 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (65) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:08:28,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 65
galaxy.jobs DEBUG 2025-01-10 13:08:28,999 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [65] prepared (110.695 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:08:29,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/65/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/65/registry.xml' '/galaxy/server/database/jobs_directory/000/65/upload_params.json' '82:/galaxy/server/database/objects/f/1/0/dataset_f1041d8f-aac9-4dae-b155-3495f311c3d9_files:/galaxy/server/database/objects/f/1/0/dataset_f1041d8f-aac9-4dae-b155-3495f311c3d9.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:08:29,042 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (65) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/65/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/65/galaxy_65.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:08:37,444 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 65: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:08:42,475 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 65 finished
galaxy.model.metadata DEBUG 2025-01-10 13:08:42,538 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 82
galaxy.jobs INFO 2025-01-10 13:08:42,575 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 65 in /galaxy/server/database/jobs_directory/000/65
galaxy.jobs DEBUG 2025-01-10 13:08:42,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 65 executed (127.783 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:08:43,164 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 66
tpv.core.entities DEBUG 2025-01-10 13:08:43,205 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:08:43,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:08:43,212 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:08:43,229 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:08:43,247 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Working directory for job is: /galaxy/server/database/jobs_directory/000/66
galaxy.jobs.runners DEBUG 2025-01-10 13:08:43,256 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [66] queued (44.219 ms)
galaxy.jobs.handler INFO 2025-01-10 13:08:43,260 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (66) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:08:43,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 66
galaxy.jobs DEBUG 2025-01-10 13:08:43,338 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [66] prepared (58.538 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:08:43,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:08:43,339 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-01-10 13:08:43,361 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:08:43,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/66/tool_script.sh] for tool command [export MPLBACKEND="agg" && poretools hist '/galaxy/server/database/objects/f/1/0/dataset_f1041d8f-aac9-4dae-b155-3495f311c3d9.dat' --saveas histogram.svg --min-length 0 --max-length 1000000000 --num-bins 50  && mv histogram.svg '/galaxy/server/database/objects/0/6/3/dataset_0632b383-3d7f-4bd7-9db9-5c55902b4923.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:08:43,404 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (66) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/66/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/66/galaxy_66.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:08:43,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:08:43,422 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/poretools_hist/poretools_hist/0.6.1a1.1: poretools:0.6.1a1
galaxy.tool_util.deps.containers INFO 2025-01-10 13:08:43,448 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/poretools:0.6.1a1--py_8,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:08:50,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 66: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:08:55,571 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 66 finished
galaxy.model.metadata DEBUG 2025-01-10 13:08:55,641 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 83
galaxy.util WARNING 2025-01-10 13:08:55,648 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/6/3/dataset_0632b383-3d7f-4bd7-9db9-5c55902b4923.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/6/3/dataset_0632b383-3d7f-4bd7-9db9-5c55902b4923.dat'
galaxy.jobs INFO 2025-01-10 13:08:55,707 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 66 in /galaxy/server/database/jobs_directory/000/66
galaxy.objectstore CRITICAL 2025-01-10 13:08:55,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:08:55,732 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 66 executed (126.687 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:08:58,618 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 67, 68
tpv.core.entities DEBUG 2025-01-10 13:08:58,663 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:08:58,663 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:08:58,670 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:08:58,691 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:08:58,710 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Working directory for job is: /galaxy/server/database/jobs_directory/000/67
galaxy.jobs.runners DEBUG 2025-01-10 13:08:58,717 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [67] queued (47.227 ms)
galaxy.jobs.handler INFO 2025-01-10 13:08:58,722 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (67) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:08:58,727 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 67
tpv.core.entities DEBUG 2025-01-10 13:08:58,744 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:08:58,744 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:08:58,752 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:08:58,771 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:08:58,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Working directory for job is: /galaxy/server/database/jobs_directory/000/68
galaxy.jobs.runners DEBUG 2025-01-10 13:08:58,808 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [68] queued (55.588 ms)
galaxy.jobs.handler INFO 2025-01-10 13:08:58,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (68) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:08:58,819 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 68
galaxy.jobs DEBUG 2025-01-10 13:08:58,872 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [67] prepared (116.198 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:08:58,903 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/67/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/67/registry.xml' '/galaxy/server/database/jobs_directory/000/67/upload_params.json' '84:/galaxy/server/database/objects/b/f/5/dataset_bf5fd1a7-49e9-4b55-959d-88c60798e9cd_files:/galaxy/server/database/objects/b/f/5/dataset_bf5fd1a7-49e9-4b55-959d-88c60798e9cd.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:08:58,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (67) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/67/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/67/galaxy_67.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-10 13:08:58,959 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [68] prepared (118.400 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:08:58,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/68/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/68/registry.xml' '/galaxy/server/database/jobs_directory/000/68/upload_params.json' '85:/galaxy/server/database/objects/b/8/f/dataset_b8f63b79-cb26-4c42-8629-9a862f89f1cf_files:/galaxy/server/database/objects/b/8/f/dataset_b8f63b79-cb26-4c42-8629-9a862f89f1cf.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:08:58,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (68) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/68/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/68/galaxy_68.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:09:09,064 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 67: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:09:09,163 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 68: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:09:14,268 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 67 finished
galaxy.jobs.runners DEBUG 2025-01-10 13:09:14,299 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 68 finished
galaxy.model.metadata DEBUG 2025-01-10 13:09:14,348 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 84
galaxy.model.metadata DEBUG 2025-01-10 13:09:14,382 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 85
galaxy.jobs INFO 2025-01-10 13:09:14,403 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 67 in /galaxy/server/database/jobs_directory/000/67
galaxy.jobs INFO 2025-01-10 13:09:14,423 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 68 in /galaxy/server/database/jobs_directory/000/68
galaxy.jobs DEBUG 2025-01-10 13:09:14,465 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 67 executed (161.898 ms)
galaxy.jobs DEBUG 2025-01-10 13:09:14,486 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 68 executed (152.041 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:09:15,211 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 69
tpv.core.entities DEBUG 2025-01-10 13:09:15,251 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_matrix/deeptools_compute_matrix/.*, abstract=False, cores=10, mem=30, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:09:15,251 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:09:15,256 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:09:15,274 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:09:15,293 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Working directory for job is: /galaxy/server/database/jobs_directory/000/69
galaxy.jobs.runners DEBUG 2025-01-10 13:09:15,303 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [69] queued (46.508 ms)
galaxy.jobs.handler INFO 2025-01-10 13:09:15,307 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (69) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:09:15,311 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 69
galaxy.jobs DEBUG 2025-01-10 13:09:15,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [69] prepared (93.835 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:09:15,425 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:09:15,426 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_matrix/deeptools_compute_matrix/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:09:15,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:09:15,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/69/tool_script.sh] for tool command [computeMatrix --version > /galaxy/server/database/jobs_directory/000/69/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/b/8/f/dataset_b8f63b79-cb26-4c42-8629-9a862f89f1cf.dat' 'bamCoverage_result4.bw_0.bw' &&    ln -f -s '/galaxy/server/database/objects/b/f/5/dataset_bf5fd1a7-49e9-4b55-959d-88c60798e9cd.dat' 'computeMatrix1_bed_0.bed' &&  computeMatrix  reference-point --regionsFileName 'computeMatrix1_bed_0.bed'  --scoreFileName 'bamCoverage_result4.bw_0.bw'  --outFileName '/galaxy/server/database/objects/8/a/2/dataset_8a248aa5-629f-4ee5-98bb-a6f24425038c.dat' --samplesLabel 'bamCoverage_result4.bw'  --numberOfProcessors "${GALAXY_SLOTS:-4}"   --referencePoint TSS  --beforeRegionStartLength 10 --afterRegionStartLength 10  --sortRegions 'keep' --sortUsing 'sum' --averageTypeBins 'sum'   --binSize 10     --transcriptID transcript --exonID exon --transcript_id_designator transcript_id]
galaxy.jobs.runners DEBUG 2025-01-10 13:09:15,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (69) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/69/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/69/galaxy_69.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:09:15,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:09:15,671 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_matrix/deeptools_compute_matrix/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:09:15,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:09:41,618 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 69: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:09:46,698 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 69 finished
galaxy.model.metadata DEBUG 2025-01-10 13:09:46,761 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 86
galaxy.util WARNING 2025-01-10 13:09:46,766 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/8/a/2/dataset_8a248aa5-629f-4ee5-98bb-a6f24425038c.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/8/a/2/dataset_8a248aa5-629f-4ee5-98bb-a6f24425038c.dat'
galaxy.jobs INFO 2025-01-10 13:09:46,793 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 69 in /galaxy/server/database/jobs_directory/000/69
galaxy.objectstore CRITICAL 2025-01-10 13:09:46,812 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:09:46,813 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 69 executed (83.654 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:09:48,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 70
tpv.core.entities DEBUG 2025-01-10 13:09:48,065 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:09:48,066 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:09:48,072 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:09:48,090 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:09:48,107 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Working directory for job is: /galaxy/server/database/jobs_directory/000/70
galaxy.jobs.runners DEBUG 2025-01-10 13:09:48,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [70] queued (42.338 ms)
galaxy.jobs.handler INFO 2025-01-10 13:09:48,119 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (70) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:09:48,122 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 70
galaxy.jobs DEBUG 2025-01-10 13:09:48,230 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [70] prepared (90.640 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:09:48,253 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/70/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/70/registry.xml' '/galaxy/server/database/jobs_directory/000/70/upload_params.json' '87:/galaxy/server/database/objects/2/6/d/dataset_26d1dfe4-4e68-4b3f-86cd-df6c65d1dcab_files:/galaxy/server/database/objects/2/6/d/dataset_26d1dfe4-4e68-4b3f-86cd-df6c65d1dcab.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:09:48,267 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (70) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/70/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/70/galaxy_70.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-10 13:09:49,121 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 71
tpv.core.entities DEBUG 2025-01-10 13:09:49,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:09:49,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:09:49,161 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:09:49,175 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:09:49,196 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Working directory for job is: /galaxy/server/database/jobs_directory/000/71
galaxy.jobs.runners DEBUG 2025-01-10 13:09:49,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [71] queued (42.693 ms)
galaxy.jobs.handler INFO 2025-01-10 13:09:49,208 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (71) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:09:49,214 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 71
galaxy.jobs DEBUG 2025-01-10 13:09:49,336 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [71] prepared (95.946 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:09:49,357 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/71/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/71/registry.xml' '/galaxy/server/database/jobs_directory/000/71/upload_params.json' '88:/galaxy/server/database/objects/2/8/c/dataset_28c05db6-2fb8-4bc8-b469-57b240def629_files:/galaxy/server/database/objects/2/8/c/dataset_28c05db6-2fb8-4bc8-b469-57b240def629.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:09:49,371 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (71) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/71/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/71/galaxy_71.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:09:57,970 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 70: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:09:59,133 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 71: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:10:03,390 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 70 finished
galaxy.model.metadata DEBUG 2025-01-10 13:10:03,483 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 87
galaxy.jobs INFO 2025-01-10 13:10:03,545 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 70 in /galaxy/server/database/jobs_directory/000/70
galaxy.jobs DEBUG 2025-01-10 13:10:03,614 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 70 executed (180.476 ms)
galaxy.jobs.runners DEBUG 2025-01-10 13:10:04,324 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 71 finished
galaxy.model.metadata DEBUG 2025-01-10 13:10:04,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 88
galaxy.jobs INFO 2025-01-10 13:10:04,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 71 in /galaxy/server/database/jobs_directory/000/71
galaxy.jobs DEBUG 2025-01-10 13:10:04,471 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 71 executed (122.762 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:10:05,572 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 72
tpv.core.entities DEBUG 2025-01-10 13:10:05,614 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_matrix/deeptools_compute_matrix/.*, abstract=False, cores=10, mem=30, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:10:05,615 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:10:05,619 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:10:05,635 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:10:05,657 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Working directory for job is: /galaxy/server/database/jobs_directory/000/72
galaxy.jobs.runners DEBUG 2025-01-10 13:10:05,668 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [72] queued (47.945 ms)
galaxy.jobs.handler INFO 2025-01-10 13:10:05,672 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (72) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:10:05,691 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 72
galaxy.jobs DEBUG 2025-01-10 13:10:05,823 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [72] prepared (109.736 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:10:05,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:10:05,824 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_matrix/deeptools_compute_matrix/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:10:06,006 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:10:06,032 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/72/tool_script.sh] for tool command [computeMatrix --version > /galaxy/server/database/jobs_directory/000/72/outputs/COMMAND_VERSION 2>&1;
ln -f -s '/galaxy/server/database/objects/2/8/c/dataset_28c05db6-2fb8-4bc8-b469-57b240def629.dat' 'computeMatrix2.bw_0.bw' &&    ln -f -s '/galaxy/server/database/objects/2/6/d/dataset_26d1dfe4-4e68-4b3f-86cd-df6c65d1dcab.dat' 'computeMatrix2_bed_0.bed' &&  computeMatrix  reference-point --regionsFileName 'computeMatrix2_bed_0.bed'  --scoreFileName 'computeMatrix2.bw_0.bw'  --outFileName '/galaxy/server/database/objects/7/b/4/dataset_7b4ba3cf-8515-42e5-9c52-5a2b2dea4b89.dat' --samplesLabel 'computeMatrix2.bw'  --numberOfProcessors "${GALAXY_SLOTS:-4}"   --referencePoint TSS  --beforeRegionStartLength 10 --afterRegionStartLength 10  --sortRegions 'keep' --sortUsing 'mean' --averageTypeBins 'mean'   --binSize 10     --transcriptID transcript --exonID exon --transcript_id_designator transcript_id]
galaxy.jobs.runners DEBUG 2025-01-10 13:10:06,046 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (72) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/72/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/72/galaxy_72.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:10:06,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:10:06,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_compute_matrix/deeptools_compute_matrix/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:10:06,081 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:10:11,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 72: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:10:16,542 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 72 finished
galaxy.model.metadata DEBUG 2025-01-10 13:10:16,605 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 89
galaxy.util WARNING 2025-01-10 13:10:16,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/7/b/4/dataset_7b4ba3cf-8515-42e5-9c52-5a2b2dea4b89.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/7/b/4/dataset_7b4ba3cf-8515-42e5-9c52-5a2b2dea4b89.dat'
galaxy.jobs INFO 2025-01-10 13:10:16,642 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 72 in /galaxy/server/database/jobs_directory/000/72
galaxy.objectstore CRITICAL 2025-01-10 13:10:16,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:10:16,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 72 executed (92.824 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:10:17,970 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 73
tpv.core.entities DEBUG 2025-01-10 13:10:18,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:10:18,008 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:10:18,014 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:10:18,030 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:10:18,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Working directory for job is: /galaxy/server/database/jobs_directory/000/73
galaxy.jobs.runners DEBUG 2025-01-10 13:10:18,054 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [73] queued (40.192 ms)
galaxy.jobs.handler INFO 2025-01-10 13:10:18,058 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (73) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:10:18,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 73
galaxy.jobs DEBUG 2025-01-10 13:10:18,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [73] prepared (88.776 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:10:18,194 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/73/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/73/registry.xml' '/galaxy/server/database/jobs_directory/000/73/upload_params.json' '90:/galaxy/server/database/objects/b/e/4/dataset_be448ace-1ef3-41ee-acae-f9b7b1403eb8_files:/galaxy/server/database/objects/b/e/4/dataset_be448ace-1ef3-41ee-acae-f9b7b1403eb8.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:10:18,207 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (73) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/73/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/73/galaxy_73.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-10 13:10:19,061 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 74
tpv.core.entities DEBUG 2025-01-10 13:10:19,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:10:19,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:10:19,101 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:10:19,114 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:10:19,134 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Working directory for job is: /galaxy/server/database/jobs_directory/000/74
galaxy.jobs.runners DEBUG 2025-01-10 13:10:19,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [74] queued (42.619 ms)
galaxy.jobs.handler INFO 2025-01-10 13:10:19,147 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (74) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:10:19,151 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 74
galaxy.jobs DEBUG 2025-01-10 13:10:19,262 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [74] prepared (87.929 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:10:19,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/74/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/74/registry.xml' '/galaxy/server/database/jobs_directory/000/74/upload_params.json' '91:/galaxy/server/database/objects/2/5/5/dataset_25587b7c-32dd-42ef-8719-e1758de633df_files:/galaxy/server/database/objects/2/5/5/dataset_25587b7c-32dd-42ef-8719-e1758de633df.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:10:19,296 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (74) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/74/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/74/galaxy_74.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes ERROR 2025-01-10 13:10:27,764 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://10.14.240.1:443/apis/batch/v1/namespaces/prod-25-01-10-12-36-1/jobs/gxy-lxt7f

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 817, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 824, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 107, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-lxt7f": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:10:27,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (74/gxy-lxt7f) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-01-10 13:10:28,951 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 73: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:10:33,780 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 73 finished
galaxy.model.metadata DEBUG 2025-01-10 13:10:33,845 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 90
galaxy.jobs INFO 2025-01-10 13:10:33,891 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 73 in /galaxy/server/database/jobs_directory/000/73
galaxy.jobs DEBUG 2025-01-10 13:10:33,950 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 73 executed (142.162 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:10:36,537 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 75
tpv.core.entities DEBUG 2025-01-10 13:10:36,577 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:10:36,578 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:10:36,584 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:10:36,601 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:10:36,625 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Working directory for job is: /galaxy/server/database/jobs_directory/000/75
galaxy.jobs.runners DEBUG 2025-01-10 13:10:36,634 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [75] queued (49.749 ms)
galaxy.jobs.handler INFO 2025-01-10 13:10:36,638 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (75) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:10:36,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 75
galaxy.jobs DEBUG 2025-01-10 13:10:36,767 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [75] prepared (102.385 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:10:36,790 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/75/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/75/registry.xml' '/galaxy/server/database/jobs_directory/000/75/upload_params.json' '92:/galaxy/server/database/objects/d/c/e/dataset_dce87147-105b-4851-8f01-5eeeeddaa818_files:/galaxy/server/database/objects/d/c/e/dataset_dce87147-105b-4851-8f01-5eeeeddaa818.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:10:36,801 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (75) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/75/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/75/galaxy_75.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:10:46,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 75: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:10:51,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 75 finished
galaxy.model.metadata DEBUG 2025-01-10 13:10:51,271 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 92
galaxy.jobs INFO 2025-01-10 13:10:51,359 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 75 in /galaxy/server/database/jobs_directory/000/75
galaxy.jobs DEBUG 2025-01-10 13:10:51,412 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 75 executed (175.238 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:10:51,980 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 76
tpv.core.entities DEBUG 2025-01-10 13:10:52,026 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:10:52,027 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:10:52,032 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:10:52,050 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:10:52,071 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Working directory for job is: /galaxy/server/database/jobs_directory/000/76
galaxy.jobs.runners DEBUG 2025-01-10 13:10:52,081 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [76] queued (49.063 ms)
galaxy.jobs.handler INFO 2025-01-10 13:10:52,086 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (76) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:10:52,090 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 76
galaxy.jobs DEBUG 2025-01-10 13:10:52,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [76] prepared (76.318 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:10:52,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:10:52,190 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:10:52,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:10:52,233 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/76/tool_script.sh] for tool command [bamCoverage --version > /galaxy/server/database/jobs_directory/000/76/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/d/c/e/dataset_dce87147-105b-4851-8f01-5eeeeddaa818.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/4/9/1/metadata_491aa312-7bf5-4b9e-80be-fa0b2dc90496.dat' one.bam.bai &&  bamCoverage --numberOfProcessors "${GALAXY_SLOTS:-4}"  --bam one.bam --outFileName '/galaxy/server/database/objects/1/a/c/dataset_1aca232f-c045-4291-8a0f-da1cbd7cdfd6.dat' --outFileFormat 'bigwig'  --binSize 10]
galaxy.jobs.runners DEBUG 2025-01-10 13:10:52,248 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (76) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/76/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/76/galaxy_76.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:10:52,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:10:52,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:10:52,288 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:10:57,356 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 76: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:11:02,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 76 finished
galaxy.model.metadata DEBUG 2025-01-10 13:11:02,585 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 93
galaxy.util WARNING 2025-01-10 13:11:02,590 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/a/c/dataset_1aca232f-c045-4291-8a0f-da1cbd7cdfd6.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/a/c/dataset_1aca232f-c045-4291-8a0f-da1cbd7cdfd6.dat'
galaxy.jobs INFO 2025-01-10 13:11:02,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 76 in /galaxy/server/database/jobs_directory/000/76
galaxy.objectstore CRITICAL 2025-01-10 13:11:02,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:11:02,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 76 executed (103.744 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:11:04,371 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 77
tpv.core.entities DEBUG 2025-01-10 13:11:04,409 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:11:04,409 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:11:04,415 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:11:04,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:11:04,459 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Working directory for job is: /galaxy/server/database/jobs_directory/000/77
galaxy.jobs.runners DEBUG 2025-01-10 13:11:04,468 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [77] queued (53.227 ms)
galaxy.jobs.handler INFO 2025-01-10 13:11:04,472 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (77) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:11:04,477 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 77
galaxy.jobs DEBUG 2025-01-10 13:11:04,607 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [77] prepared (106.593 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:11:04,629 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/77/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/77/registry.xml' '/galaxy/server/database/jobs_directory/000/77/upload_params.json' '94:/galaxy/server/database/objects/7/d/0/dataset_7d0af6ec-bd68-4cfc-9774-524bdca6204d_files:/galaxy/server/database/objects/7/d/0/dataset_7d0af6ec-bd68-4cfc-9774-524bdca6204d.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:11:04,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (77) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/77/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/77/galaxy_77.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:11:13,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 77: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:11:19,113 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 77 finished
galaxy.model.metadata DEBUG 2025-01-10 13:11:19,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 94
galaxy.jobs INFO 2025-01-10 13:11:19,270 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 77 in /galaxy/server/database/jobs_directory/000/77
galaxy.jobs DEBUG 2025-01-10 13:11:19,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 77 executed (191.950 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:11:19,876 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 78
tpv.core.entities DEBUG 2025-01-10 13:11:19,923 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:11:19,923 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:11:19,931 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:11:19,954 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:11:19,980 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Working directory for job is: /galaxy/server/database/jobs_directory/000/78
galaxy.jobs.runners DEBUG 2025-01-10 13:11:19,992 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [78] queued (60.446 ms)
galaxy.jobs.handler INFO 2025-01-10 13:11:19,997 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (78) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:11:20,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 78
galaxy.jobs DEBUG 2025-01-10 13:11:20,103 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [78] prepared (71.976 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:11:20,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:11:20,104 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:11:20,132 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:11:20,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/78/tool_script.sh] for tool command [bamCoverage --version > /galaxy/server/database/jobs_directory/000/78/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/7/d/0/dataset_7d0af6ec-bd68-4cfc-9774-524bdca6204d.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/9/7/b/metadata_97b18ed3-ab0b-49ca-9b79-22dae2b1aeca.dat' one.bam.bai &&  bamCoverage --numberOfProcessors "${GALAXY_SLOTS:-4}"  --bam one.bam --outFileName '/galaxy/server/database/objects/1/4/f/dataset_14f11d37-9de1-4e03-830a-df0fa3e6ef86.dat' --outFileFormat 'bigwig'  --binSize 10  --normalizeUsing RPGC --effectiveGenomeSize 2451960000]
galaxy.jobs.runners DEBUG 2025-01-10 13:11:20,183 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (78) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/78/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/78/galaxy_78.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:11:20,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:11:20,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:11:20,219 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:11:25,946 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 78: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:11:31,058 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 78 finished
galaxy.model.metadata DEBUG 2025-01-10 13:11:31,128 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 95
galaxy.util WARNING 2025-01-10 13:11:31,134 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/4/f/dataset_14f11d37-9de1-4e03-830a-df0fa3e6ef86.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/4/f/dataset_14f11d37-9de1-4e03-830a-df0fa3e6ef86.dat'
galaxy.jobs INFO 2025-01-10 13:11:31,171 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 78 in /galaxy/server/database/jobs_directory/000/78
galaxy.objectstore CRITICAL 2025-01-10 13:11:31,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:11:31,197 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 78 executed (104.713 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:11:33,313 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 79
tpv.core.entities DEBUG 2025-01-10 13:11:33,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:11:33,357 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:11:33,364 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:11:33,387 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:11:33,411 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Working directory for job is: /galaxy/server/database/jobs_directory/000/79
galaxy.jobs.runners DEBUG 2025-01-10 13:11:33,421 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [79] queued (57.001 ms)
galaxy.jobs.handler INFO 2025-01-10 13:11:33,426 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (79) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:11:33,431 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 79
galaxy.jobs DEBUG 2025-01-10 13:11:33,558 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [79] prepared (103.417 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:11:33,587 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/79/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/79/registry.xml' '/galaxy/server/database/jobs_directory/000/79/upload_params.json' '96:/galaxy/server/database/objects/f/0/a/dataset_f0abdcc8-9d67-4716-8f3e-0644e7aeca46_files:/galaxy/server/database/objects/f/0/a/dataset_f0abdcc8-9d67-4716-8f3e-0644e7aeca46.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:11:33,603 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (79) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/79/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/79/galaxy_79.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:11:42,093 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 79: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:11:47,257 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 79 finished
galaxy.model.metadata DEBUG 2025-01-10 13:11:47,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 96
galaxy.jobs INFO 2025-01-10 13:11:47,394 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 79 in /galaxy/server/database/jobs_directory/000/79
galaxy.jobs DEBUG 2025-01-10 13:11:47,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 79 executed (170.305 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:11:47,751 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 80
tpv.core.entities DEBUG 2025-01-10 13:11:47,790 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:11:47,791 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:11:47,796 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:11:47,812 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:11:47,827 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Working directory for job is: /galaxy/server/database/jobs_directory/000/80
galaxy.jobs.runners DEBUG 2025-01-10 13:11:47,834 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [80] queued (37.678 ms)
galaxy.jobs.handler INFO 2025-01-10 13:11:47,838 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (80) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:11:47,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 80
galaxy.jobs DEBUG 2025-01-10 13:11:47,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [80] prepared (60.789 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:11:47,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:11:47,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:11:47,943 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:11:47,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/80/tool_script.sh] for tool command [bamCoverage --version > /galaxy/server/database/jobs_directory/000/80/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/f/0/a/dataset_f0abdcc8-9d67-4716-8f3e-0644e7aeca46.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/f/c/c/metadata_fcc70292-b187-47a9-96e1-7eb27d8abae7.dat' one.bam.bai &&  bamCoverage --numberOfProcessors "${GALAXY_SLOTS:-4}"  --bam one.bam --outFileName '/galaxy/server/database/objects/0/7/6/dataset_0765590b-7243-46e4-8d3f-4f9d697c1cce.dat' --outFileFormat 'bedgraph'  --binSize 10  --normalizeUsing RPGC --effectiveGenomeSize 2451960000]
galaxy.jobs.runners DEBUG 2025-01-10 13:11:47,981 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (80) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/80/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/80/galaxy_80.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:11:47,996 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:11:47,997 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:11:48,018 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:11:54,224 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 80: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:11:59,285 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 80 finished
galaxy.model.metadata DEBUG 2025-01-10 13:11:59,337 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 97
galaxy.util WARNING 2025-01-10 13:11:59,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/0/7/6/dataset_0765590b-7243-46e4-8d3f-4f9d697c1cce.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/0/7/6/dataset_0765590b-7243-46e4-8d3f-4f9d697c1cce.dat'
galaxy.jobs INFO 2025-01-10 13:11:59,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 80 in /galaxy/server/database/jobs_directory/000/80
galaxy.objectstore CRITICAL 2025-01-10 13:11:59,398 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:11:59,399 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 80 executed (85.684 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:12:01,128 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 81
tpv.core.entities DEBUG 2025-01-10 13:12:01,160 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:12:01,161 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:12:01,165 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:12:01,180 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:12:01,194 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Working directory for job is: /galaxy/server/database/jobs_directory/000/81
galaxy.jobs.runners DEBUG 2025-01-10 13:12:01,201 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [81] queued (35.759 ms)
galaxy.jobs.handler INFO 2025-01-10 13:12:01,206 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (81) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:12:01,208 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 81
galaxy.jobs DEBUG 2025-01-10 13:12:01,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [81] prepared (96.939 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:12:01,340 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/81/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/81/registry.xml' '/galaxy/server/database/jobs_directory/000/81/upload_params.json' '98:/galaxy/server/database/objects/e/1/f/dataset_e1fe6191-ced2-4063-9aeb-c6cf90ad7a0c_files:/galaxy/server/database/objects/e/1/f/dataset_e1fe6191-ced2-4063-9aeb-c6cf90ad7a0c.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:12:01,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (81) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/81/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/81/galaxy_81.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:12:10,383 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 81: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:12:15,428 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 81 finished
galaxy.model.metadata DEBUG 2025-01-10 13:12:15,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 98
galaxy.jobs INFO 2025-01-10 13:12:15,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 81 in /galaxy/server/database/jobs_directory/000/81
galaxy.jobs DEBUG 2025-01-10 13:12:15,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 81 executed (168.198 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:12:16,606 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 82
tpv.core.entities DEBUG 2025-01-10 13:12:16,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:12:16,644 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:12:16,651 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:12:16,669 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:12:16,693 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Working directory for job is: /galaxy/server/database/jobs_directory/000/82
galaxy.jobs.runners DEBUG 2025-01-10 13:12:16,702 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [82] queued (50.743 ms)
galaxy.jobs.handler INFO 2025-01-10 13:12:16,707 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (82) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:12:16,710 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 82
galaxy.jobs DEBUG 2025-01-10 13:12:16,794 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [82] prepared (66.347 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:12:16,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:12:16,795 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:12:16,820 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:12:16,842 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/82/tool_script.sh] for tool command [bamCoverage --version > /galaxy/server/database/jobs_directory/000/82/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/e/1/f/dataset_e1fe6191-ced2-4063-9aeb-c6cf90ad7a0c.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/6/6/b/metadata_66b0790b-3997-4499-8a27-6d03810d3710.dat' one.bam.bai &&  bamCoverage --numberOfProcessors "${GALAXY_SLOTS:-4}"  --bam one.bam --outFileName '/galaxy/server/database/objects/3/a/f/dataset_3af993fc-255b-4744-b571-e99331853f3c.dat' --outFileFormat 'bigwig'  --binSize 10  --normalizeUsing RPGC --effectiveGenomeSize 2451960000]
galaxy.jobs.runners DEBUG 2025-01-10 13:12:16,853 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (82) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/82/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/82/galaxy_82.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:12:16,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:12:16,868 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:12:16,893 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:12:22,559 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 82: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:12:27,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 82 finished
galaxy.model.metadata DEBUG 2025-01-10 13:12:27,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 99
galaxy.util WARNING 2025-01-10 13:12:27,739 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/3/a/f/dataset_3af993fc-255b-4744-b571-e99331853f3c.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/3/a/f/dataset_3af993fc-255b-4744-b571-e99331853f3c.dat'
galaxy.jobs INFO 2025-01-10 13:12:27,771 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 82 in /galaxy/server/database/jobs_directory/000/82
galaxy.objectstore CRITICAL 2025-01-10 13:12:27,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:12:27,792 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 82 executed (100.698 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:12:30,001 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 83
tpv.core.entities DEBUG 2025-01-10 13:12:30,044 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:12:30,045 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:12:30,049 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:12:30,065 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:12:30,084 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Working directory for job is: /galaxy/server/database/jobs_directory/000/83
galaxy.jobs.runners DEBUG 2025-01-10 13:12:30,092 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [83] queued (42.348 ms)
galaxy.jobs.handler INFO 2025-01-10 13:12:30,098 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (83) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:12:30,100 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 83
galaxy.jobs DEBUG 2025-01-10 13:12:30,216 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [83] prepared (94.637 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:12:30,237 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/83/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/83/registry.xml' '/galaxy/server/database/jobs_directory/000/83/upload_params.json' '100:/galaxy/server/database/objects/8/0/0/dataset_800e64bc-2698-4511-82bc-5677354395b3_files:/galaxy/server/database/objects/8/0/0/dataset_800e64bc-2698-4511-82bc-5677354395b3.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:12:30,249 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (83) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/83/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/83/galaxy_83.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:12:38,746 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 83: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:12:43,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 83 finished
galaxy.model.metadata DEBUG 2025-01-10 13:12:43,800 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 100
galaxy.jobs INFO 2025-01-10 13:12:43,883 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 83 in /galaxy/server/database/jobs_directory/000/83
galaxy.jobs DEBUG 2025-01-10 13:12:43,939 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 83 executed (176.646 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:12:44,399 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 84
tpv.core.entities DEBUG 2025-01-10 13:12:44,437 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:12:44,438 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:12:44,442 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:12:44,459 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:12:44,478 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Working directory for job is: /galaxy/server/database/jobs_directory/000/84
galaxy.jobs.runners DEBUG 2025-01-10 13:12:44,487 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [84] queued (44.521 ms)
galaxy.jobs.handler INFO 2025-01-10 13:12:44,491 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (84) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:12:44,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 84
galaxy.jobs DEBUG 2025-01-10 13:12:44,591 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [84] prepared (76.551 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:12:44,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:12:44,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:12:44,612 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:12:44,632 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/84/tool_script.sh] for tool command [bamCoverage --version > /galaxy/server/database/jobs_directory/000/84/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/8/0/0/dataset_800e64bc-2698-4511-82bc-5677354395b3.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/8/a/a/metadata_8aa5977c-6f9c-4d51-a190-6a593444e874.dat' one.bam.bai &&  bamCoverage --numberOfProcessors "${GALAXY_SLOTS:-4}"  --bam one.bam --outFileName '/galaxy/server/database/objects/a/6/6/dataset_a668fe81-cef7-48ff-a11d-938fd93b1e4f.dat' --outFileFormat 'bedgraph'  --binSize 10  --normalizeUsing RPGC --effectiveGenomeSize 2451960000    --scaleFactor 1.0      --minMappingQuality '1']
galaxy.jobs.runners DEBUG 2025-01-10 13:12:44,644 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (84) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/84/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/84/galaxy_84.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:12:44,656 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:12:44,657 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:12:44,674 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:12:50,866 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 84: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:12:55,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 84 finished
galaxy.model.metadata DEBUG 2025-01-10 13:12:55,863 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 101
galaxy.util WARNING 2025-01-10 13:12:55,870 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/a/6/6/dataset_a668fe81-cef7-48ff-a11d-938fd93b1e4f.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/a/6/6/dataset_a668fe81-cef7-48ff-a11d-938fd93b1e4f.dat'
galaxy.jobs INFO 2025-01-10 13:12:55,899 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 84 in /galaxy/server/database/jobs_directory/000/84
galaxy.objectstore CRITICAL 2025-01-10 13:12:55,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:12:55,919 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 84 executed (91.417 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:12:57,820 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 85
tpv.core.entities DEBUG 2025-01-10 13:12:57,854 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:12:57,855 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:12:57,861 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:12:57,879 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:12:57,901 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Working directory for job is: /galaxy/server/database/jobs_directory/000/85
galaxy.jobs.runners DEBUG 2025-01-10 13:12:57,908 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [85] queued (47.055 ms)
galaxy.jobs.handler INFO 2025-01-10 13:12:57,911 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (85) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:12:57,915 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 85
galaxy.jobs DEBUG 2025-01-10 13:12:58,026 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [85] prepared (89.882 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:12:58,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/85/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/85/registry.xml' '/galaxy/server/database/jobs_directory/000/85/upload_params.json' '102:/galaxy/server/database/objects/1/9/5/dataset_1950d8de-13d4-46f3-9d13-4b8623119102_files:/galaxy/server/database/objects/1/9/5/dataset_1950d8de-13d4-46f3-9d13-4b8623119102.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:12:58,062 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (85) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/85/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/85/galaxy_85.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:13:07,235 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 85: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:13:12,146 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 85 finished
galaxy.model.metadata DEBUG 2025-01-10 13:13:12,206 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 102
galaxy.jobs INFO 2025-01-10 13:13:12,286 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 85 in /galaxy/server/database/jobs_directory/000/85
galaxy.jobs DEBUG 2025-01-10 13:13:12,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 85 executed (173.251 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:13:13,404 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 86
tpv.core.entities DEBUG 2025-01-10 13:13:13,444 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:13:13,445 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:13:13,451 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:13:13,470 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:13:13,494 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Working directory for job is: /galaxy/server/database/jobs_directory/000/86
galaxy.jobs.runners DEBUG 2025-01-10 13:13:13,509 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [86] queued (58.128 ms)
galaxy.jobs.handler INFO 2025-01-10 13:13:13,512 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (86) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:13:13,516 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 86
galaxy.jobs DEBUG 2025-01-10 13:13:13,600 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [86] prepared (67.803 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:13:13,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:13:13,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:13:13,624 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:13:13,650 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/86/tool_script.sh] for tool command [bamCoverage --version > /galaxy/server/database/jobs_directory/000/86/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/1/9/5/dataset_1950d8de-13d4-46f3-9d13-4b8623119102.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/4/f/f/metadata_4ff4b40f-3673-45ba-99da-605e6e87f6b3.dat' one.bam.bai &&  bamCoverage --numberOfProcessors "${GALAXY_SLOTS:-4}"  --bam one.bam --outFileName '/galaxy/server/database/objects/2/5/2/dataset_25238a61-8186-47a6-92b2-d61dd6a74256.dat' --outFileFormat 'bigwig'  --binSize 10  --normalizeUsing RPGC --effectiveGenomeSize 2451960000    --scaleFactor 1.0      --minMappingQuality '1'     --filterRNAstrand 'reverse']
galaxy.jobs.runners DEBUG 2025-01-10 13:13:13,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (86) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/86/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/86/galaxy_86.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:13:13,680 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:13:13,681 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:13:13,703 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:13:19,519 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 86: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:13:24,497 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 86 finished
galaxy.model.metadata DEBUG 2025-01-10 13:13:24,562 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 103
galaxy.util WARNING 2025-01-10 13:13:24,569 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/2/5/2/dataset_25238a61-8186-47a6-92b2-d61dd6a74256.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/2/5/2/dataset_25238a61-8186-47a6-92b2-d61dd6a74256.dat'
galaxy.jobs INFO 2025-01-10 13:13:24,602 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 86 in /galaxy/server/database/jobs_directory/000/86
galaxy.objectstore CRITICAL 2025-01-10 13:13:24,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:13:24,623 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 86 executed (95.636 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:13:26,808 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 87
tpv.core.entities DEBUG 2025-01-10 13:13:26,843 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:13:26,844 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:13:26,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:13:26,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:13:26,891 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Working directory for job is: /galaxy/server/database/jobs_directory/000/87
galaxy.jobs.runners DEBUG 2025-01-10 13:13:26,899 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [87] queued (48.296 ms)
galaxy.jobs.handler INFO 2025-01-10 13:13:26,902 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (87) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:13:26,906 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 87
galaxy.jobs DEBUG 2025-01-10 13:13:27,015 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [87] prepared (91.178 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:13:27,041 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/87/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/87/registry.xml' '/galaxy/server/database/jobs_directory/000/87/upload_params.json' '104:/galaxy/server/database/objects/c/b/6/dataset_cb629882-1ba9-4831-a90d-63adeae9e50c_files:/galaxy/server/database/objects/c/b/6/dataset_cb629882-1ba9-4831-a90d-63adeae9e50c.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:13:27,054 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (87) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/87/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/87/galaxy_87.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:13:36,827 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 87: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:13:41,783 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 87 finished
galaxy.model.metadata DEBUG 2025-01-10 13:13:41,846 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 104
galaxy.jobs INFO 2025-01-10 13:13:41,920 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 87 in /galaxy/server/database/jobs_directory/000/87
galaxy.jobs DEBUG 2025-01-10 13:13:41,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 87 executed (165.649 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:13:42,226 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 88
tpv.core.entities DEBUG 2025-01-10 13:13:42,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/.*, abstract=False, cores=10, mem=24, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:13:42,266 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:13:42,272 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:13:42,289 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:13:42,309 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Working directory for job is: /galaxy/server/database/jobs_directory/000/88
galaxy.jobs.runners DEBUG 2025-01-10 13:13:42,319 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [88] queued (46.744 ms)
galaxy.jobs.handler INFO 2025-01-10 13:13:42,323 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (88) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:13:42,327 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 88
galaxy.jobs DEBUG 2025-01-10 13:13:42,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [88] prepared (63.641 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:13:42,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:13:42,408 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:13:42,432 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:13:42,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/88/tool_script.sh] for tool command [bamCoverage --version > /galaxy/server/database/jobs_directory/000/88/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/c/b/6/dataset_cb629882-1ba9-4831-a90d-63adeae9e50c.dat' one.bam && ln -s '/galaxy/server/database/objects/_metadata_files/2/5/5/metadata_255b9858-c139-4c53-a255-73183906275b.dat' one.bam.bai &&  bamCoverage --numberOfProcessors "${GALAXY_SLOTS:-4}"  --bam one.bam --outFileName '/galaxy/server/database/objects/c/e/0/dataset_ce0aca7d-47af-4557-88f6-4070a83ecbab.dat' --outFileFormat 'bigwig'  --binSize 10     --scaleFactor 1.0    --extendReads        --Offset -4 -1]
galaxy.jobs.runners DEBUG 2025-01-10 13:13:42,468 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (88) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/88/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/88/galaxy_88.ec; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:13:42,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:13:42,481 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0: mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18
galaxy.tool_util.deps.containers INFO 2025-01-10 13:13:42,500 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-eb9e7907c7a753917c1e4d7a64384c047429618a:bcea566aaf2a8cd09765df369b45c50e0b7e9f18-1,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:13:48,044 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 88: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:13:52,986 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 88 finished
galaxy.model.metadata DEBUG 2025-01-10 13:13:53,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 105
galaxy.util WARNING 2025-01-10 13:13:53,068 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/c/e/0/dataset_ce0aca7d-47af-4557-88f6-4070a83ecbab.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/c/e/0/dataset_ce0aca7d-47af-4557-88f6-4070a83ecbab.dat'
galaxy.jobs INFO 2025-01-10 13:13:53,107 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 88 in /galaxy/server/database/jobs_directory/000/88
galaxy.objectstore CRITICAL 2025-01-10 13:13:53,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] None delete error [Errno 13] Permission denied: 'matplotlib'
galaxy.jobs DEBUG 2025-01-10 13:13:53,131 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 88 executed (112.512 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:13:55,623 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 89
tpv.core.entities DEBUG 2025-01-10 13:13:55,663 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:13:55,663 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:13:55,669 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:13:55,701 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:13:55,720 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Working directory for job is: /galaxy/server/database/jobs_directory/000/89
galaxy.jobs.runners DEBUG 2025-01-10 13:13:55,728 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [89] queued (59.064 ms)
galaxy.jobs.handler INFO 2025-01-10 13:13:55,732 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (89) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:13:55,737 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 89
galaxy.jobs DEBUG 2025-01-10 13:13:55,860 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [89] prepared (101.302 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:13:55,880 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/89/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/89/registry.xml' '/galaxy/server/database/jobs_directory/000/89/upload_params.json' '106:/galaxy/server/database/objects/8/1/2/dataset_812500b8-0220-4243-8c29-834a806c4313_files:/galaxy/server/database/objects/8/1/2/dataset_812500b8-0220-4243-8c29-834a806c4313.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:13:55,894 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (89) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/89/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/89/galaxy_89.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:14:06,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 89: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:14:11,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 89 finished
galaxy.model.metadata DEBUG 2025-01-10 13:14:11,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 106
galaxy.jobs INFO 2025-01-10 13:14:11,676 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 89 in /galaxy/server/database/jobs_directory/000/89
galaxy.jobs DEBUG 2025-01-10 13:14:11,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 89 executed (191.180 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:14:12,102 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 90
tpv.core.entities DEBUG 2025-01-10 13:14:12,142 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:14:12,143 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:14:12,149 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:14:12,165 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:14:12,181 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Working directory for job is: /galaxy/server/database/jobs_directory/000/90
galaxy.jobs.runners DEBUG 2025-01-10 13:14:12,191 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [90] queued (42.507 ms)
galaxy.jobs.handler INFO 2025-01-10 13:14:12,196 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (90) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:14:12,200 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 90
galaxy.jobs DEBUG 2025-01-10 13:14:12,294 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [90] prepared (74.010 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:14:12,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:14:12,295 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-01-10 13:14:12,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:14:12,548 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/90/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/90/outputs/COMMAND_VERSION 2>&1;
lofreq indelqual --uniform 20,30 -o output.bam /galaxy/server/database/objects/8/1/2/dataset_812500b8-0220-4243-8c29-834a806c4313.dat]
galaxy.jobs.runners DEBUG 2025-01-10 13:14:12,565 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (90) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/90/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/90/galaxy_90.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/90/working/output.bam" ] ; then cp "/galaxy/server/database/jobs_directory/000/90/working/output.bam" "/galaxy/server/database/objects/1/0/c/dataset_10c5909b-f181-4a8a-8fad-f3660edd6fd1.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:14:12,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:14:12,576 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-01-10 13:14:12,598 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:14:26,636 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 90: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:14:31,586 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 90 finished
galaxy.model.metadata DEBUG 2025-01-10 13:14:31,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 107
galaxy.util WARNING 2025-01-10 13:14:31,683 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/1/0/c/dataset_10c5909b-f181-4a8a-8fad-f3660edd6fd1.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/1/0/c/dataset_10c5909b-f181-4a8a-8fad-f3660edd6fd1.dat'
galaxy.jobs INFO 2025-01-10 13:14:31,708 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 90 in /galaxy/server/database/jobs_directory/000/90
galaxy.jobs DEBUG 2025-01-10 13:14:31,753 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 90 executed (139.021 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:14:33,804 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 92, 91
tpv.core.entities DEBUG 2025-01-10 13:14:33,832 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:14:33,833 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:14:33,837 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:14:33,850 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:14:33,869 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Working directory for job is: /galaxy/server/database/jobs_directory/000/91
galaxy.jobs.runners DEBUG 2025-01-10 13:14:33,877 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [91] queued (40.343 ms)
galaxy.jobs.handler INFO 2025-01-10 13:14:33,881 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (91) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:14:33,887 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 91
tpv.core.entities DEBUG 2025-01-10 13:14:33,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:14:33,900 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:14:33,908 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:14:33,928 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:14:33,964 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Working directory for job is: /galaxy/server/database/jobs_directory/000/92
galaxy.jobs.runners DEBUG 2025-01-10 13:14:33,973 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [92] queued (65.510 ms)
galaxy.jobs.handler INFO 2025-01-10 13:14:33,978 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (92) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:14:33,985 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 92
galaxy.jobs DEBUG 2025-01-10 13:14:34,027 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [91] prepared (114.480 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:14:34,059 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/91/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/91/registry.xml' '/galaxy/server/database/jobs_directory/000/91/upload_params.json' '108:/galaxy/server/database/objects/6/7/4/dataset_67426a48-69db-44e6-b916-0212bfc87523_files:/galaxy/server/database/objects/6/7/4/dataset_67426a48-69db-44e6-b916-0212bfc87523.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:14:34,091 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (91) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/91/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/91/galaxy_91.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-10 13:14:34,119 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [92] prepared (107.930 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:14:34,147 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/92/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/92/registry.xml' '/galaxy/server/database/jobs_directory/000/92/upload_params.json' '109:/galaxy/server/database/objects/f/2/9/dataset_f29038ed-9112-4d1c-9f7a-9e0c8f65348d_files:/galaxy/server/database/objects/f/2/9/dataset_f29038ed-9112-4d1c-9f7a-9e0c8f65348d.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:14:34,166 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (92) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/92/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/92/galaxy_92.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:14:42,967 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 91: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:14:43,001 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 92: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:14:48,526 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 91 finished
galaxy.model.metadata DEBUG 2025-01-10 13:14:48,583 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 108
galaxy.jobs.runners DEBUG 2025-01-10 13:14:48,594 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 92 finished
galaxy.jobs INFO 2025-01-10 13:14:48,663 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 91 in /galaxy/server/database/jobs_directory/000/91
galaxy.model.metadata DEBUG 2025-01-10 13:14:48,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 109
galaxy.jobs INFO 2025-01-10 13:14:48,706 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 92 in /galaxy/server/database/jobs_directory/000/92
galaxy.jobs DEBUG 2025-01-10 13:14:48,729 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 91 executed (175.712 ms)
galaxy.jobs DEBUG 2025-01-10 13:14:48,769 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 92 executed (144.556 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:14:49,401 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 93
tpv.core.entities DEBUG 2025-01-10 13:14:49,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:14:49,441 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:14:49,446 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:14:49,461 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:14:49,479 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Working directory for job is: /galaxy/server/database/jobs_directory/000/93
galaxy.jobs.runners DEBUG 2025-01-10 13:14:49,487 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [93] queued (40.793 ms)
galaxy.jobs.handler INFO 2025-01-10 13:14:49,490 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (93) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:14:49,494 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 93
galaxy.jobs DEBUG 2025-01-10 13:14:49,566 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [93] prepared (52.687 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:14:49,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:14:49,567 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-01-10 13:14:49,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:14:49,617 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/93/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/93/outputs/COMMAND_VERSION 2>&1;
ln -s '/galaxy/server/database/objects/f/2/9/dataset_f29038ed-9112-4d1c-9f7a-9e0c8f65348d.dat' reference.fa && lofreq faidx reference.fa 2>&1 || echo "Error running samtools faidx for indexing fasta reference for lofreq" >&2 &&  lofreq indelqual --dindel --ref reference.fa -o output.bam /galaxy/server/database/objects/6/7/4/dataset_67426a48-69db-44e6-b916-0212bfc87523.dat]
galaxy.jobs.runners DEBUG 2025-01-10 13:14:49,633 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (93) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/93/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/93/galaxy_93.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/93/working/output.bam" ] ; then cp "/galaxy/server/database/jobs_directory/000/93/working/output.bam" "/galaxy/server/database/objects/e/f/0/dataset_ef02ece8-1f5c-47a4-a337-20fd485ba626.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:14:49,645 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:14:49,646 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-01-10 13:14:49,665 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:14:55,117 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 93: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:15:00,106 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 93 finished
galaxy.model.metadata DEBUG 2025-01-10 13:15:00,160 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 110
galaxy.util WARNING 2025-01-10 13:15:00,204 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/e/f/0/dataset_ef02ece8-1f5c-47a4-a337-20fd485ba626.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/e/f/0/dataset_ef02ece8-1f5c-47a4-a337-20fd485ba626.dat'
galaxy.jobs INFO 2025-01-10 13:15:00,228 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 93 in /galaxy/server/database/jobs_directory/000/93
galaxy.jobs DEBUG 2025-01-10 13:15:00,279 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 93 executed (145.872 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:15:02,769 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 94
tpv.core.entities DEBUG 2025-01-10 13:15:02,809 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:15:02,810 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:15:02,815 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:15:02,831 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:15:02,849 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Working directory for job is: /galaxy/server/database/jobs_directory/000/94
galaxy.jobs.runners DEBUG 2025-01-10 13:15:02,856 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [94] queued (40.701 ms)
galaxy.jobs.handler INFO 2025-01-10 13:15:02,860 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (94) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:15:02,864 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 94
galaxy.jobs DEBUG 2025-01-10 13:15:02,980 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [94] prepared (98.850 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:15:03,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/94/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/94/registry.xml' '/galaxy/server/database/jobs_directory/000/94/upload_params.json' '111:/galaxy/server/database/objects/6/a/e/dataset_6aeca283-e93a-4831-8d92-4ea8eb98c6ee_files:/galaxy/server/database/objects/6/a/e/dataset_6aeca283-e93a-4831-8d92-4ea8eb98c6ee.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:15:03,022 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (94) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/94/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/94/galaxy_94.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:15:12,315 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 94: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:15:17,533 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 94 finished
galaxy.model.metadata DEBUG 2025-01-10 13:15:17,589 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 111
galaxy.jobs INFO 2025-01-10 13:15:17,677 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 94 in /galaxy/server/database/jobs_directory/000/94
galaxy.jobs DEBUG 2025-01-10 13:15:17,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 94 executed (172.946 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:15:18,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 95
tpv.core.entities DEBUG 2025-01-10 13:15:18,245 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/.*, abstract=False, cores=1, mem=11.4, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:15:18,245 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:15:18,250 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:15:18,267 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:15:18,285 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Working directory for job is: /galaxy/server/database/jobs_directory/000/95
galaxy.jobs.runners DEBUG 2025-01-10 13:15:18,293 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [95] queued (43.306 ms)
galaxy.jobs.handler INFO 2025-01-10 13:15:18,297 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (95) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:15:18,302 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 95
galaxy.jobs DEBUG 2025-01-10 13:15:18,377 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [95] prepared (55.749 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:15:18,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:15:18,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-01-10 13:15:18,668 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:15:18,690 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/95/tool_script.sh] for tool command [lofreq version | grep version | cut -d ' ' -f 2 > /galaxy/server/database/jobs_directory/000/95/outputs/COMMAND_VERSION 2>&1;
lofreq indelqual --uniform 20 -o output.bam /galaxy/server/database/objects/6/a/e/dataset_6aeca283-e93a-4831-8d92-4ea8eb98c6ee.dat]
galaxy.jobs.runners DEBUG 2025-01-10 13:15:18,705 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (95) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/95/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/95/galaxy_95.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/95/working/output.bam" ] ; then cp "/galaxy/server/database/jobs_directory/000/95/working/output.bam" "/galaxy/server/database/objects/d/9/6/dataset_d969a15d-ced3-42d3-9f83-542f2c9c0874.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:15:18,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:15:18,717 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1: lofreq:2.1.5
galaxy.tool_util.deps.containers INFO 2025-01-10 13:15:18,736 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/lofreq:2.1.5--py312ha5e83ca_15,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:15:24,459 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 95: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:15:29,592 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 95 finished
galaxy.model.metadata DEBUG 2025-01-10 13:15:29,664 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 112
galaxy.util WARNING 2025-01-10 13:15:29,719 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Unable to honor primary group (grp.struct_group(gr_name='galaxy', gr_passwd='x', gr_gid=101, gr_mem=[])) for /galaxy/server/database/objects/d/9/6/dataset_d969a15d-ced3-42d3-9f83-542f2c9c0874.dat, group remains grp.struct_group(gr_name='root', gr_passwd='x', gr_gid=0, gr_mem=[]), error was: [Errno 1] Operation not permitted: '/galaxy/server/database/objects/d/9/6/dataset_d969a15d-ced3-42d3-9f83-542f2c9c0874.dat'
galaxy.jobs INFO 2025-01-10 13:15:29,747 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 95 in /galaxy/server/database/jobs_directory/000/95
galaxy.jobs DEBUG 2025-01-10 13:15:29,798 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 95 executed (168.618 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:15:33,696 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 97, 96
tpv.core.entities DEBUG 2025-01-10 13:15:33,727 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:15:33,727 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:15:33,733 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:15:33,750 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:15:33,769 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Working directory for job is: /galaxy/server/database/jobs_directory/000/96
galaxy.jobs.runners DEBUG 2025-01-10 13:15:33,775 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [96] queued (42.259 ms)
galaxy.jobs.handler INFO 2025-01-10 13:15:33,780 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (96) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:15:33,785 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 96
tpv.core.entities DEBUG 2025-01-10 13:15:33,800 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:15:33,801 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:15:33,807 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:15:33,826 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:15:33,859 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Working directory for job is: /galaxy/server/database/jobs_directory/000/97
galaxy.jobs.runners DEBUG 2025-01-10 13:15:33,867 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [97] queued (59.458 ms)
galaxy.jobs.handler INFO 2025-01-10 13:15:33,871 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (97) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:15:33,877 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 97
galaxy.jobs DEBUG 2025-01-10 13:15:33,924 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [96] prepared (115.482 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:15:33,962 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/96/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/96/registry.xml' '/galaxy/server/database/jobs_directory/000/96/upload_params.json' '113:/galaxy/server/database/objects/0/7/f/dataset_07ff2d98-8a69-4f24-ac20-21bca092d811_files:/galaxy/server/database/objects/0/7/f/dataset_07ff2d98-8a69-4f24-ac20-21bca092d811.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:15:33,979 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (96) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/96/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/96/galaxy_96.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-10 13:15:34,023 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [97] prepared (121.951 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:15:34,047 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/97/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/97/registry.xml' '/galaxy/server/database/jobs_directory/000/97/upload_params.json' '114:/galaxy/server/database/objects/c/d/9/dataset_cd986961-cd8c-4ed5-a21f-cf7dd19e7100_files:/galaxy/server/database/objects/c/d/9/dataset_cd986961-cd8c-4ed5-a21f-cf7dd19e7100.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:15:34,060 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (97) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/97/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/97/galaxy_97.ec; sh -c "exit $return_code"
galaxy.jobs.runners.kubernetes ERROR 2025-01-10 13:15:41,693 [pN:job_handler_0,p:8,tN:KubernetesRunner.monitor_thread] Could not clean up k8s batch job. Ignoring...
Traceback (most recent call last):
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 437, in raise_for_status
    resp.raise_for_status()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://10.14.240.1:443/apis/batch/v1/namespaces/prod-25-01-10-12-36-1/jobs/gxy-mr42m

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 817, in _handle_job_failure
    self.__cleanup_k8s_job(job)
  File "/galaxy/server/lib/galaxy/jobs/runners/kubernetes.py", line 824, in __cleanup_k8s_job
    delete_job(job, k8s_cleanup_job)
  File "/galaxy/server/lib/galaxy/jobs/runners/util/pykube_util.py", line 107, in delete_job
    job.scale(replicas=0)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/mixins.py", line 30, in scale
    self.update()
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 165, in update
    self.patch(self.obj, subresource=subresource)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/objects.py", line 157, in patch
    self.api.raise_for_status(r)
  File "/galaxy/server/.venv/lib/python3.10/site-packages/pykube/http.py", line 444, in raise_for_status
    raise HTTPError(resp.status_code, payload["message"])
pykube.exceptions.HTTPError: Operation cannot be fulfilled on jobs.batch "gxy-mr42m": the object has been modified; please apply your changes to the latest version and try again
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:15:41,731 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (96/gxy-mr42m) Terminated at user's request
galaxy.jobs.runners DEBUG 2025-01-10 13:15:41,882 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 97: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.handler DEBUG 2025-01-10 13:15:43,078 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 98, 99
tpv.core.entities DEBUG 2025-01-10 13:15:43,120 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:15:43,121 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:15:43,126 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:15:43,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:15:43,166 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Working directory for job is: /galaxy/server/database/jobs_directory/000/98
galaxy.jobs.runners DEBUG 2025-01-10 13:15:43,175 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [98] queued (47.997 ms)
galaxy.jobs.handler INFO 2025-01-10 13:15:43,179 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (98) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:15:43,186 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 98
tpv.core.entities DEBUG 2025-01-10 13:15:43,203 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:15:43,204 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:15:43,213 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:15:43,236 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:15:43,279 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Working directory for job is: /galaxy/server/database/jobs_directory/000/99
galaxy.jobs.runners DEBUG 2025-01-10 13:15:43,289 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [99] queued (76.328 ms)
galaxy.jobs.handler INFO 2025-01-10 13:15:43,296 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (99) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:15:43,301 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Starting queue_job for job 99
galaxy.jobs DEBUG 2025-01-10 13:15:43,347 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [98] prepared (131.752 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:15:43,388 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/98/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/98/registry.xml' '/galaxy/server/database/jobs_directory/000/98/upload_params.json' '115:/galaxy/server/database/objects/1/2/f/dataset_12fdd6db-9d51-47d1-b909-66230b54ab4f_files:/galaxy/server/database/objects/1/2/f/dataset_12fdd6db-9d51-47d1-b909-66230b54ab4f.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:15:43,429 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (98) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/98/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/98/galaxy_98.ec; sh -c "exit $return_code"
galaxy.jobs DEBUG 2025-01-10 13:15:43,455 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Job wrapper for Job [99] prepared (129.431 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:15:43,489 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Built script [/galaxy/server/database/jobs_directory/000/99/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/99/registry.xml' '/galaxy/server/database/jobs_directory/000/99/upload_params.json' '116:/galaxy/server/database/objects/5/b/f/dataset_5bf752b9-0e0b-4169-97a2-638eec86533c_files:/galaxy/server/database/objects/5/b/f/dataset_5bf752b9-0e0b-4169-97a2-638eec86533c.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:15:43,508 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] (99) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/99/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/99/galaxy_99.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:15:47,221 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 97 finished
galaxy.model.metadata DEBUG 2025-01-10 13:15:47,282 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 114
galaxy.jobs INFO 2025-01-10 13:15:47,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 97 in /galaxy/server/database/jobs_directory/000/97
galaxy.jobs DEBUG 2025-01-10 13:15:47,380 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 97 executed (129.932 ms)
galaxy.jobs.runners DEBUG 2025-01-10 13:15:52,427 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] executing external set_meta script for job 98: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:15:52,443 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 99: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:15:58,085 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] execution of external set_meta for job 98 finished
galaxy.model.metadata DEBUG 2025-01-10 13:15:58,149 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] loading metadata from file for: HistoryDatasetAssociation 115
galaxy.jobs INFO 2025-01-10 13:15:58,193 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Collecting metrics for Job 98 in /galaxy/server/database/jobs_directory/000/98
galaxy.jobs.runners DEBUG 2025-01-10 13:15:58,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 99 finished
galaxy.jobs DEBUG 2025-01-10 13:15:58,259 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] job_wrapper.finish for job 98 executed (143.830 ms)
galaxy.model.metadata DEBUG 2025-01-10 13:15:58,281 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 116
galaxy.jobs INFO 2025-01-10 13:15:58,319 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 99 in /galaxy/server/database/jobs_directory/000/99
galaxy.jobs DEBUG 2025-01-10 13:15:58,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 99 executed (135.845 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:15:58,945 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 100
tpv.core.entities DEBUG 2025-01-10 13:15:59,001 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/.*, abstract=False, cores=5, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:15:59,002 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:15:59,007 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:15:59,026 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:15:59,046 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Working directory for job is: /galaxy/server/database/jobs_directory/000/100
galaxy.jobs.runners DEBUG 2025-01-10 13:15:59,056 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [100] queued (49.057 ms)
galaxy.jobs.handler INFO 2025-01-10 13:15:59,062 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (100) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:15:59,067 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Starting queue_job for job 100
galaxy.jobs DEBUG 2025-01-10 13:15:59,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Job wrapper for Job [100] prepared (73.960 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:15:59,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:15:59,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/4.0.0rc1+galaxy3: mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003
galaxy.tool_util.deps.containers INFO 2025-01-10 13:15:59,352 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:15:59,378 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Built script [/galaxy/server/database/jobs_directory/000/100/tool_script.sh] for tool command [ln -s /galaxy/server/database/objects/1/2/f/dataset_12fdd6db-9d51-47d1-b909-66230b54ab4f.dat reference.fa && ln -s /galaxy/server/database/objects/5/b/f/dataset_5bf752b9-0e0b-4169-97a2-638eec86533c.dat query.fa && nucmer  --sam-long=outsam.sam -b '200' -c '65' -D '5' -d '0.12'   -g '90' -l '20' -L '0'   --threads "${GALAXY_SLOTS:-1}" 'reference.fa' 'query.fa' && samtools dict reference.fa > outsamhead && tail -n +3 outsam.sam >> outsamhead && samtools sort  -@ ${GALAXY_SLOTS:-1} -T "${TMPDIR:-.}" outsamhead | samtools calmd -b --threads {GALAXY_SLOTS:-1} - reference.fa > outsam]
galaxy.jobs.runners DEBUG 2025-01-10 13:15:59,393 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] (100) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/100/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/100/galaxy_100.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/100/working/outsam" ] ; then cp "/galaxy/server/database/jobs_directory/000/100/working/outsam" "/galaxy/server/database/objects/3/b/e/dataset_3bebc6a9-f0cb-4d04-bb5e-6470ce708b5b.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:15:59,406 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:15:59,407 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/4.0.0rc1+galaxy3: mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003
galaxy.tool_util.deps.containers INFO 2025-01-10 13:15:59,430 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:16:29,189 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 100: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:16:34,358 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 100 finished
galaxy.model.metadata DEBUG 2025-01-10 13:16:34,439 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 117
galaxy.jobs INFO 2025-01-10 13:16:34,535 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 100 in /galaxy/server/database/jobs_directory/000/100
galaxy.jobs DEBUG 2025-01-10 13:16:34,601 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 100 executed (211.934 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:16:35,960 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 101
tpv.core.entities DEBUG 2025-01-10 13:16:35,996 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:16:35,996 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:16:36,003 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:16:36,021 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:16:36,040 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Working directory for job is: /galaxy/server/database/jobs_directory/000/101
galaxy.jobs.runners DEBUG 2025-01-10 13:16:36,047 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [101] queued (44.245 ms)
galaxy.jobs.handler INFO 2025-01-10 13:16:36,052 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (101) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:16:36,057 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 101
galaxy.jobs DEBUG 2025-01-10 13:16:36,189 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [101] prepared (110.425 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:16:36,213 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/101/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/101/registry.xml' '/galaxy/server/database/jobs_directory/000/101/upload_params.json' '118:/galaxy/server/database/objects/6/a/5/dataset_6a5b56a7-a868-4ca7-b79c-b328d191fee2_files:/galaxy/server/database/objects/6/a/5/dataset_6a5b56a7-a868-4ca7-b79c-b328d191fee2.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:16:36,226 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (101) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/101/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/101/galaxy_101.ec; sh -c "exit $return_code"
galaxy.jobs.handler DEBUG 2025-01-10 13:16:37,055 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 102
tpv.core.entities DEBUG 2025-01-10 13:16:37,095 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=force_default_container_for_built_in_tools, Rule: force_default_container_for_built_in_tools, abstract=False, cores=1, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true', 'docker_container_id_override': 'quay.io/galaxyproject/galaxy-anvil:23.1'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:16:37,096 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:16:37,101 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:16:37,121 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:16:37,145 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Working directory for job is: /galaxy/server/database/jobs_directory/000/102
galaxy.jobs.runners DEBUG 2025-01-10 13:16:37,153 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [102] queued (51.898 ms)
galaxy.jobs.handler INFO 2025-01-10 13:16:37,157 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (102) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:16:37,161 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Starting queue_job for job 102
galaxy.jobs DEBUG 2025-01-10 13:16:37,280 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Job wrapper for Job [102] prepared (99.890 ms)
galaxy.jobs.command_factory INFO 2025-01-10 13:16:37,303 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] Built script [/galaxy/server/database/jobs_directory/000/102/tool_script.sh] for tool command [python '/galaxy/server/database/tools/data_source/upload.py' '/galaxy/server' '/galaxy/server/database/jobs_directory/000/102/registry.xml' '/galaxy/server/database/jobs_directory/000/102/upload_params.json' '119:/galaxy/server/database/objects/9/3/4/dataset_93424f1c-2d3f-42fd-b707-11e4c4b482c6_files:/galaxy/server/database/objects/9/3/4/dataset_93424f1c-2d3f-42fd-b707-11e4c4b482c6.dat']
galaxy.jobs.runners DEBUG 2025-01-10 13:16:37,318 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-2] (102) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/102/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/102/galaxy_102.ec; sh -c "exit $return_code"
galaxy.jobs.runners DEBUG 2025-01-10 13:16:45,643 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] executing external set_meta script for job 101: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:16:46,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] executing external set_meta script for job 102: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:16:50,938 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] execution of external set_meta for job 101 finished
galaxy.model.metadata DEBUG 2025-01-10 13:16:51,005 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] loading metadata from file for: HistoryDatasetAssociation 118
galaxy.jobs INFO 2025-01-10 13:16:51,051 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] Collecting metrics for Job 101 in /galaxy/server/database/jobs_directory/000/101
galaxy.jobs DEBUG 2025-01-10 13:16:51,114 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-3] job_wrapper.finish for job 101 executed (148.241 ms)
galaxy.jobs.runners DEBUG 2025-01-10 13:16:51,856 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] execution of external set_meta for job 102 finished
galaxy.model.metadata DEBUG 2025-01-10 13:16:51,922 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] loading metadata from file for: HistoryDatasetAssociation 119
galaxy.jobs INFO 2025-01-10 13:16:51,968 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] Collecting metrics for Job 102 in /galaxy/server/database/jobs_directory/000/102
galaxy.jobs DEBUG 2025-01-10 13:16:52,031 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-1] job_wrapper.finish for job 102 executed (141.824 ms)
galaxy.jobs.handler DEBUG 2025-01-10 13:16:52,525 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Grabbed Job(s): 103
tpv.core.entities DEBUG 2025-01-10 13:16:52,566 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Ranking destinations: [runner=k8s, dest_name=k8s, min_accepted_cores=None, min_accepted_mem=None, min_accepted_gpus=None, max_accepted_cores=100, max_accepted_mem=800, max_accepted_gpus=None, tpv_dest_tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=docker, type=TagType.ACCEPT>], handler_tags=None<class 'tpv.core.entities.Destination'> id=k8s, abstract=False, cores=None, mem=None, gpus=None, min_cores = None, min_mem = None, min_gpus = None, max_cores = 8, max_mem = 48, max_gpus = None, env=[], params={'tpv_cores': '{cores}', 'tpv_gpus': '{gpus}', 'tpv_mem': '{mem}', 'docker_enabled': 'true', 'limits_cpu': '{cores}', 'limits_memory': '{mem}Gi', 'requests_cpu': '{cores}', 'requests_memory': '{mem}Gi'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[], rank=, inherits=None, context={}, rules={}] for entity: <class 'tpv.core.entities.Tool'> id=toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/.*, abstract=False, cores=5, mem=cores * 3.8, gpus=0, min_cores = None, min_mem = None, min_gpus = None, max_cores = None, max_mem = None, max_gpus = None, env=[], params={'container_monitor': False, 'docker_default_container_id': 'quay.io/galaxyproject/galaxy-anvil:23.1', 'tmp_dir': 'true'}, resubmit={}, tags=<class 'tpv.core.entities.TagSetManager'> tags=[<Tag: name=scheduling, value=local, type=TagType.REJECT>, <Tag: name=scheduling, value=offline, type=TagType.REJECT>], rank=helpers.we, inherits=None, context={}, rules={} using custom function
galaxy.jobs.mapper DEBUG 2025-01-10 13:16:52,567 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Mapped job to destination id: k8s
galaxy.jobs.handler DEBUG 2025-01-10 13:16:52,571 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Dispatching to k8s runner
galaxy.jobs DEBUG 2025-01-10 13:16:52,588 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Persisting job destination (destination id: k8s)
galaxy.jobs DEBUG 2025-01-10 13:16:52,609 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Working directory for job is: /galaxy/server/database/jobs_directory/000/103
galaxy.jobs.runners DEBUG 2025-01-10 13:16:52,617 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] Job [103] queued (45.933 ms)
galaxy.jobs.handler INFO 2025-01-10 13:16:52,621 [pN:job_handler_0,p:8,tN:JobHandlerQueue.monitor_thread] (103) Job dispatched
galaxy.jobs.runners.kubernetes DEBUG 2025-01-10 13:16:52,626 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Starting queue_job for job 103
galaxy.jobs DEBUG 2025-01-10 13:16:52,734 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Job wrapper for Job [103] prepared (86.698 ms)
galaxy.tool_util.deps.containers INFO 2025-01-10 13:16:52,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:16:52,735 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/4.0.0rc1+galaxy3: mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003
galaxy.tool_util.deps.containers INFO 2025-01-10 13:16:52,757 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003-0,type=docker]]
galaxy.jobs.command_factory INFO 2025-01-10 13:16:52,781 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Built script [/galaxy/server/database/jobs_directory/000/103/tool_script.sh] for tool command [ln -s /galaxy/server/database/objects/6/a/5/dataset_6a5b56a7-a868-4ca7-b79c-b328d191fee2.dat reference.fa && ln -s /galaxy/server/database/objects/9/3/4/dataset_93424f1c-2d3f-42fd-b707-11e4c4b482c6.dat query.fa && nucmer  --sam-long=outsam.sam -b '200' -c '65' -D '5' -d '0.12'   -g '90' -l '20' -L '0'   --threads "${GALAXY_SLOTS:-1}" 'reference.fa' 'query.fa' && samtools dict reference.fa > outsamhead && tail -n +3 outsam.sam >> outsamhead && samtools sort  -@ ${GALAXY_SLOTS:-1} -T "${TMPDIR:-.}" outsamhead | samtools view -C --reference reference.fa -o outsam -]
galaxy.jobs.runners DEBUG 2025-01-10 13:16:52,797 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] (103) command is: mkdir -p working outputs configs
if [ -d _working ]; then
    rm -rf working/ outputs/ configs/; cp -R _working working; cp -R _outputs outputs; cp -R _configs configs
else
    cp -R working _working; cp -R outputs _outputs; cp -R configs _configs
fi
cd working; __out="${TMPDIR:-.}/out.$$" __err="${TMPDIR:-.}/err.$$"
mkfifo "$__out" "$__err"
trap 'rm -f "$__out" "$__err"' EXIT
tee -a '../outputs/tool_stdout' < "$__out" &
tee -a '../outputs/tool_stderr' < "$__err" >&2 & /bin/bash /galaxy/server/database/jobs_directory/000/103/tool_script.sh > "$__out" 2> "$__err"; return_code=$?; echo $return_code > /galaxy/server/database/jobs_directory/000/103/galaxy_103.ec; 
if [ -f "/galaxy/server/database/jobs_directory/000/103/working/outsam" ] ; then cp "/galaxy/server/database/jobs_directory/000/103/working/outsam" "/galaxy/server/database/objects/e/e/2/dataset_ee2877a9-9788-4f37-a663-45bc95277e28.dat" ; fi; sh -c "exit $return_code"
galaxy.tool_util.deps.containers INFO 2025-01-10 13:16:52,809 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [ExplicitContainerResolver[]] found description [None]
galaxy.tool_util.deps.container_resolvers.mulled DEBUG 2025-01-10 13:16:52,810 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Image name for tool toolshed.g2.bx.psu.edu/repos/iuc/mummer_nucmer/mummer_nucmer/4.0.0rc1+galaxy3: mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003
galaxy.tool_util.deps.containers INFO 2025-01-10 13:16:52,828 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Checking with container resolver [MulledDockerContainerResolver[namespace=biocontainers]] found description [ContainerDescription[identifier=quay.io/biocontainers/mulled-v2-373539cd51d908b0ef4294c9cff8015be7cf2072:242eaca4eb9c8ff4c9b6f90b393c876ccfa61003-0,type=docker]]
galaxy.jobs.runners DEBUG 2025-01-10 13:16:58,978 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] executing external set_meta script for job 103: GALAXY_LIB="/galaxy/server/lib"; if [ "$GALAXY_LIB" != "None" ]; then if [ -n "$PYTHONPATH" ]; then PYTHONPATH="$GALAXY_LIB:$PYTHONPATH"; else PYTHONPATH="$GALAXY_LIB"; fi; export PYTHONPATH; fi; GALAXY_VIRTUAL_ENV="None"; if [ "$GALAXY_VIRTUAL_ENV" != "None" -a -z "$VIRTUAL_ENV" -a -f "$GALAXY_VIRTUAL_ENV/bin/activate" ]; then . "$GALAXY_VIRTUAL_ENV/bin/activate"; fi; python metadata/set.py
galaxy.jobs.runners DEBUG 2025-01-10 13:17:04,120 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] execution of external set_meta for job 103 finished
galaxy.model.metadata DEBUG 2025-01-10 13:17:04,184 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] loading metadata from file for: HistoryDatasetAssociation 120
galaxy.jobs INFO 2025-01-10 13:17:04,264 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] Collecting metrics for Job 103 in /galaxy/server/database/jobs_directory/000/103
galaxy.jobs DEBUG 2025-01-10 13:17:04,323 [pN:job_handler_0,p:8,tN:KubernetesRunner.work_thread-0] job_wrapper.finish for job 103 executed (173.313 ms)
